{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzhKwrzmYVDB"
   },
   "source": [
    "# Multilingual Analysis\n",
    "\n",
    "This notebook contains a multilingual analysis for thesis \"narrative analysis with large language models\".\n",
    "\n",
    "15 Chinese excerpts and 15 English excerpts of 5 genres are analyzed respectively.\n",
    "\n",
    "Codes reference: https://colab.research.google.com/drive/1VTi6Z51X6_nAlfSYmIqU7ZnTRVJIjv4j?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUmCBPnQaSEv"
   },
   "source": [
    "## Import the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBP3NOXegc8S",
    "outputId": "7271b1a0-5b47-4f15-c7fd-8648d68cc2d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "mkdir: cannot create directory ‘/content/drive/My Drive/Brahe’: File exists\n",
      "/content/drive/My Drive/Brahe\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!mkdir \"/content/drive/My Drive/Brahe\"\n",
    "%cd \"/content/drive/My Drive/Brahe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "642_VP1NhOwO",
    "outputId": "1561f8ef-b185-42ab-f534-ccd2e23b0cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-14 13:09:11--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/config.json\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.92, 18.238.109.52, 18.238.109.102, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.92|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 801 [text/plain]\n",
      "Saving to: ‘config.json.10’\n",
      "\n",
      "\r",
      "config.json.10        0%[                    ]       0  --.-KB/s               \r",
      "config.json.10      100%[===================>]     801  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-14 13:09:11 (244 MB/s) - ‘config.json.10’ saved [801/801]\n",
      "\n",
      "--2024-05-14 13:09:11--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/generation_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.124, 18.172.134.88, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 132 [text/plain]\n",
      "Saving to: ‘generation_config.json.10’\n",
      "\n",
      "generation_config.j 100%[===================>]     132  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-14 13:09:11 (68.9 MB/s) - ‘generation_config.json.10’ saved [132/132]\n",
      "\n",
      "--2024-05-14 13:09:11--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.102, 18.238.109.52, 18.238.109.92, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/ef/ec/efec059a080e7700b8069113b9d2863c3e63ba00a034ebbbf1531bffbbe021ab/6a3af8239a9d01fa94c892610fefbd276e595de31f90f1cd705dcf618448b5a8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1715951351&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTk1MTM1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2VmL2VjL2VmZWMwNTlhMDgwZTc3MDBiODA2OTExM2I5ZDI4NjNjM2U2M2JhMDBhMDM0ZWJiYmYxNTMxYmZmYmJlMDIxYWIvNmEzYWY4MjM5YTlkMDFmYTk0Yzg5MjYxMGZlZmJkMjc2ZTU5NWRlMzFmOTBmMWNkNzA1ZGNmNjE4NDQ4YjVhOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=fjYoCnHuvpqE9nIGQzdzqh%7EAnCG7zmsl3OkZN6Le4B7CMbmfu3s0M53AUNLZwnoyFrghLSAdPJyMLpgnU7Si2GkImPokmMMp1PvRtG5FHal2HnF1JeR4mvcLiVS0ndJXqR0QVt8z6125%7EoeWAz4uhF%7E5BoNBK-ol6R%7EBSUIKdqFVKSev8EYo67qmZrghuJCGFPQzgorEjmAxSjzohH3gn2W-sXb1RWLEMVpZb1Zbxe45wMeDucIOWy9jdSHlz1LWfGFKiBCjk%7EtwibpB2cS8sLs3qILRAfrrtbQBiLaJNFx68aas8poYtT2foXe%7EFNfx37VwlOeSI2XpDVmd-9HYIQ__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-05-14 13:09:11--  https://cdn-lfs-us-1.huggingface.co/repos/ef/ec/efec059a080e7700b8069113b9d2863c3e63ba00a034ebbbf1531bffbbe021ab/6a3af8239a9d01fa94c892610fefbd276e595de31f90f1cd705dcf618448b5a8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1715951351&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTk1MTM1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2VmL2VjL2VmZWMwNTlhMDgwZTc3MDBiODA2OTExM2I5ZDI4NjNjM2U2M2JhMDBhMDM0ZWJiYmYxNTMxYmZmYmJlMDIxYWIvNmEzYWY4MjM5YTlkMDFmYTk0Yzg5MjYxMGZlZmJkMjc2ZTU5NWRlMzFmOTBmMWNkNzA1ZGNmNjE4NDQ4YjVhOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=fjYoCnHuvpqE9nIGQzdzqh%7EAnCG7zmsl3OkZN6Le4B7CMbmfu3s0M53AUNLZwnoyFrghLSAdPJyMLpgnU7Si2GkImPokmMMp1PvRtG5FHal2HnF1JeR4mvcLiVS0ndJXqR0QVt8z6125%7EoeWAz4uhF%7E5BoNBK-ol6R%7EBSUIKdqFVKSev8EYo67qmZrghuJCGFPQzgorEjmAxSjzohH3gn2W-sXb1RWLEMVpZb1Zbxe45wMeDucIOWy9jdSHlz1LWfGFKiBCjk%7EtwibpB2cS8sLs3qILRAfrrtbQBiLaJNFx68aas8poYtT2foXe%7EFNfx37VwlOeSI2XpDVmd-9HYIQ__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.155.173.44, 18.155.173.40, 18.155.173.56, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.155.173.44|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7248185990 (6.8G) [application/octet-stream]\n",
      "Saving to: ‘pytorch_model.bin.10’\n",
      "\n",
      "pytorch_model.bin.1 100%[===================>]   6.75G  78.1MB/s    in 84s     \n",
      "\n",
      "2024-05-14 13:10:35 (82.8 MB/s) - ‘pytorch_model.bin.10’ saved [7248185990/7248185990]\n",
      "\n",
      "--2024-05-14 13:10:36--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/quant_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.52, 18.238.109.102, 18.238.109.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 90 [text/plain]\n",
      "Saving to: ‘quant_config.json.10’\n",
      "\n",
      "quant_config.json.1 100%[===================>]      90  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-14 13:10:36 (24.2 MB/s) - ‘quant_config.json.10’ saved [90/90]\n",
      "\n",
      "--2024-05-14 13:10:36--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/model.safetensors\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.52, 18.238.109.102, 18.238.109.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2024-05-14 13:10:36 ERROR 404: Not Found.\n",
      "\n",
      "--2024-05-14 13:10:36--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/special_tokens_map.json\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.52, 18.238.109.102, 18.238.109.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 549 [text/plain]\n",
      "Saving to: ‘special_tokens_map.json.10’\n",
      "\n",
      "special_tokens_map. 100%[===================>]     549  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-14 13:10:37 (225 MB/s) - ‘special_tokens_map.json.10’ saved [549/549]\n",
      "\n",
      "--2024-05-14 13:10:37--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/tokenizer.model\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.52, 18.238.109.102, 18.238.109.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/ef/ec/efec059a080e7700b8069113b9d2863c3e63ba00a034ebbbf1531bffbbe021ab/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1715951437&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTk1MTQzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2VmL2VjL2VmZWMwNTlhMDgwZTc3MDBiODA2OTExM2I5ZDI4NjNjM2U2M2JhMDBhMDM0ZWJiYmYxNTMxYmZmYmJlMDIxYWIvOWU1NTZhZmQ0NDIxM2I2YmQxYmUyYjg1MGViYmJkOThmNTQ4MTQzN2E4MDIxYWZhZjU4ZWU3ZmIxODE4ZDM0Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JtKIO12xFvFwgpl%7Er55mrJEGpKrwYFVgsxGzjKkSfUtGFpASJxPH4CbIitYv-hn-2AfWoxpiwiGXfU9eJCOmQvWol2K2%7EN-zEbBfZc-62R2PGorcPGqyAlvGl-AhAWaWA23UTn%7EjnlVkm5BKVvmTW8bK%7EW7z5mUf%7EOlc3xEFJbkKTqPJjIfYXQDPmU5Hxvb%7EhYmD%7EMBwx1X4olPZ3ZDVhKnItTb1OEOvZ9l1oZt5N8cQ2S-2Rt%7EKxxVuv549hBRZr0qU0Il86-DmvIUQ1cs%7E9sJL9LL9poNlGM--JEXysYr5oNu%7E3n7jijT9vNP67NgkNakwM5hzz78VxudEGKiihA__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-05-14 13:10:37--  https://cdn-lfs-us-1.huggingface.co/repos/ef/ec/efec059a080e7700b8069113b9d2863c3e63ba00a034ebbbf1531bffbbe021ab/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tokenizer.model%3B+filename%3D%22tokenizer.model%22%3B&Expires=1715951437&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTk1MTQzN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2VmL2VjL2VmZWMwNTlhMDgwZTc3MDBiODA2OTExM2I5ZDI4NjNjM2U2M2JhMDBhMDM0ZWJiYmYxNTMxYmZmYmJlMDIxYWIvOWU1NTZhZmQ0NDIxM2I2YmQxYmUyYjg1MGViYmJkOThmNTQ4MTQzN2E4MDIxYWZhZjU4ZWU3ZmIxODE4ZDM0Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JtKIO12xFvFwgpl%7Er55mrJEGpKrwYFVgsxGzjKkSfUtGFpASJxPH4CbIitYv-hn-2AfWoxpiwiGXfU9eJCOmQvWol2K2%7EN-zEbBfZc-62R2PGorcPGqyAlvGl-AhAWaWA23UTn%7EjnlVkm5BKVvmTW8bK%7EW7z5mUf%7EOlc3xEFJbkKTqPJjIfYXQDPmU5Hxvb%7EhYmD%7EMBwx1X4olPZ3ZDVhKnItTb1OEOvZ9l1oZt5N8cQ2S-2Rt%7EKxxVuv549hBRZr0qU0Il86-DmvIUQ1cs%7E9sJL9LL9poNlGM--JEXysYr5oNu%7E3n7jijT9vNP67NgkNakwM5hzz78VxudEGKiihA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.154.206.42, 18.154.206.94, 18.154.206.88, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.154.206.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499723 (488K) [binary/octet-stream]\n",
      "Saving to: ‘tokenizer.model.10’\n",
      "\n",
      "tokenizer.model.10  100%[===================>] 488.01K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-05-14 13:10:37 (6.44 MB/s) - ‘tokenizer.model.10’ saved [499723/499723]\n",
      "\n",
      "--2024-05-14 13:10:37--  https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/tokenizer_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 18.238.109.52, 18.238.109.102, 18.238.109.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.238.109.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 933 [text/plain]\n",
      "Saving to: ‘tokenizer_config.json.10’\n",
      "\n",
      "tokenizer_config.js 100%[===================>]     933  --.-KB/s    in 0s      \n",
      "\n",
      "2024-05-14 13:10:37 (252 MB/s) - ‘tokenizer_config.json.10’ saved [933/933]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/config.json\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/generation_config.json\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/pytorch_model.bin\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/quant_config.json\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/model.safetensors\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/special_tokens_map.json\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/tokenizer.model\n",
    "!wget https://huggingface.co/Pclanglais/Brahe-AWQ/resolve/main/tokenizer_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYcb3u4Qh1Ro",
    "outputId": "f3362b22-b60b-4911-92c1-cbd7fa3e80a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "%cd \"..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvixRUSzbSC9"
   },
   "source": [
    "## Install the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EA7jcOnuia36",
    "outputId": "b1bbd15f-2942-4d88-becd-30c997e0aef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vllm\n",
      "  Downloading vllm-0.4.2-cp310-cp310-manylinux1_x86_64.whl (67.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cmake>=3.21 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.27.9)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.1.99)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vllm) (1.25.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vllm) (2.31.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.40.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.19.1)\n",
      "Collecting fastapi (from vllm)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai (from vllm)\n",
      "  Downloading openai-1.29.0-py3-none-any.whl (320 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn[standard] (from vllm)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.7.1)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting tiktoken==0.6.0 (from vllm)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lm-format-enforcer==0.9.8 (from vllm)\n",
      "  Downloading lm_format_enforcer-0.9.8-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting outlines==0.0.34 (from vllm)\n",
      "  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from vllm) (4.11.0)\n",
      "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.14.0)\n",
      "Collecting ray>=2.9 (from vllm)\n",
      "  Downloading ray-2.21.0-cp310-cp310-manylinux2014_x86_64.whl (65.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-ml-py (from vllm)\n",
      "  Downloading nvidia_ml_py-12.550.52-py3-none-any.whl (39 kB)\n",
      "Collecting vllm-nccl-cu12<2.19,>=2.18 (from vllm)\n",
      "  Downloading vllm_nccl_cu12-2.18.1.0.4.0.tar.gz (6.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch==2.3.0 (from vllm)\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xformers==0.0.26.post1 (from vllm)\n",
      "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting interegular>=0.3.2 (from lm-format-enforcer==0.9.8->vllm)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.9.8->vllm) (24.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.9.8->vllm) (6.0.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (3.1.4)\n",
      "Collecting lark (from outlines==0.0.34->vllm)\n",
      "  Downloading lark-1.1.9-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (2.2.1)\n",
      "Collecting diskcache (from outlines==0.0.34->vllm)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.11.4)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.58.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (1.4.2)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.0.34->vllm) (4.19.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0->vllm) (2023.12.25)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (3.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->vllm) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->vllm)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->vllm)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0->vllm)\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->vllm)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->vllm) (2.18.2)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.0.8)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (3.20.3)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vllm) (2024.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->vllm) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->vllm) (4.66.4)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->vllm)\n",
      "  Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi->vllm)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart>=0.0.7 (from fastapi->vllm)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->vllm)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi->vllm)\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->vllm)\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]->vllm)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]->vllm)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm)\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]->vllm)\n",
      "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]->vllm)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->vllm) (1.7.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->vllm) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->vllm) (1.2.1)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->vllm)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->vllm)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.* (from httpx>=0.23.0->fastapi->vllm)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.34->vllm) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm) (0.18.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.34->vllm) (0.41.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->vllm) (1.3.0)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->vllm) (0.1.2)\n",
      "Building wheels for collected packages: vllm-nccl-cu12\n",
      "  Building wheel for vllm-nccl-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for vllm-nccl-cu12: filename=vllm_nccl_cu12-2.18.1.0.4.0-py3-none-any.whl size=5418 sha256=d76e57a03c25d7d753474e5b3883db9a6f2e5047fb2de34f671b58405250521d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/28/b5/e99e6ea84b08c0bf19a218d408316e55e02ff725d3616fb79d\n",
      "Successfully built vllm-nccl-cu12\n",
      "Installing collected packages: vllm-nccl-cu12, nvidia-ml-py, ninja, websockets, uvloop, ujson, triton, shellingham, python-multipart, python-dotenv, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, httptools, h11, dnspython, diskcache, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, email_validator, typer, prometheus-fastapi-instrumentator, nvidia-cusolver-cu12, lm-format-enforcer, httpx, torch, ray, openai, xformers, outlines, fastapi-cli, fastapi, vllm\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.4\n",
      "    Uninstalling typer-0.9.4:\n",
      "      Successfully uninstalled typer-0.9.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1+cu121\n",
      "    Uninstalling torch-2.2.1+cu121:\n",
      "      Successfully uninstalled torch-2.2.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
      "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
      "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed diskcache-5.6.3 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 interegular-0.3.3 lark-1.1.9 lm-format-enforcer-0.9.8 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.550.52 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-1.29.0 orjson-3.10.3 outlines-0.0.34 prometheus-fastapi-instrumentator-7.0.0 python-dotenv-1.0.1 python-multipart-0.0.9 ray-2.21.0 shellingham-1.5.4 starlette-0.37.2 tiktoken-0.6.0 torch-2.3.0 triton-2.3.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.29.0 uvloop-0.19.0 vllm-0.4.2 vllm-nccl-cu12-2.18.1.0.4.0 watchfiles-0.21.0 websockets-12.0 xformers-0.0.26.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-xr0G-1MjdaU"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KWGk8Npbe5K"
   },
   "source": [
    "Initialize a function to generate the prompt with special keywords (needed for consistent analytic annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AusUOl6zjoUK"
   },
   "outputs": [],
   "source": [
    "def prepare_fabrique_prompt(text):\n",
    "  prompt = \"Text:\\n\" + str(text) +\"\\n\\n\\nAnalysis:\\n\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556,
     "referenced_widgets": [
      "20d26e46011f4d96ab12552dc3b429d2",
      "493bd0058cf148a4ba6e3370e71eebae",
      "01d2f2ef89c940c7a55d14ebb033b06b",
      "7ca00ab7e54d449b807585f321698592",
      "afd4da9d7cff4f0d946066fd4501c3c5",
      "70058513c62f47508c06e6d5158d4f94",
      "c4fe9fc1fd8e44f3a7860e340548457d",
      "1b6b1ec885a649ada802dabfc2495170",
      "1c3985927bbe4ba9b6dc1d9db019363d",
      "148ff16359d2491b92a92a34687ebce4",
      "ec92e6016573459ea1de88eb3525c0db",
      "f1d1ac31410e437b922455ecd555936e",
      "61cd6cffc6484dceb4b90e0139ea3d35",
      "4c4e3f77747a440cb9d79ffaf6b85c15",
      "a68ee7b19d9842da8bf0d6d905d4fe58",
      "459053230cfe48bfb524d3909914272d",
      "4d3caaf60a64433ca5acd7bf891e7ba3",
      "1ba8d204e78d454f88db334210721566",
      "cfbedf3425df40a6aedc926deba2ca97",
      "367376263a7d474ea4ef4a63b6a80454",
      "7a8bff2f8a014cf7a499389b4718f732",
      "275793a0cef14551976ef2f643d31cc0",
      "a72b77925f6d42508cb9cb360568ea48",
      "f4ed1890b93347e6b435dd1911885170",
      "b905af95cedb4996a14c7694a036c7bb",
      "6e1e9d79d0b249f096550c0fca1f76ca",
      "514795a9181947e5ab767d89e56d11a9",
      "c247445e26d44f61ac2c0ca907c31038",
      "e82b7047aae8441f994123a88a1b0965",
      "c9d0787b19d34d15a7ed4bf4eabc4bcc",
      "ce8f12c0e28e4b269487f1e6a0ef333c",
      "d2009653178c4be283d4cc3c37df6bc5",
      "a2fcb785f10d49f3bb06c2627596f360",
      "03529aeb8445436eb74b415534cb1401",
      "cea110e17f9e460c99fc69d4e54a2022",
      "93640d7c63f14100a612db9fd2b64b0a",
      "e77786870f2b446c9830bbf8777d2b7e",
      "269a8a1384044dbda76ac62267c51417",
      "7a93d100eaad4976a97f0ee4781cf955",
      "d4b007207038422587ee96e7fb460013",
      "20d6a9f68201444bbb3f0cea3f83e61b",
      "8c87aafb6d344c859cb165fe40e0a47f",
      "badc34400a2347c5b0a5227b63185a54",
      "3f0660201474476f9a9bd4965beff31f",
      "d9b2223b79b143ed828c3da933370550",
      "dd751cd8dc8e431b8e9ef6e2ee0a241a",
      "d2cf8641a2ce4e3298ce4c7fcfc97a78",
      "5b15ad68b44b4c89b065a2589f59e216",
      "a7311ce159094306aea90f57336a8c9a",
      "95f2ce0bb9cb4b68a932ac4aa02fc9e7",
      "5e9bacfc22224970b47351d388599b18",
      "3ba8af12b80a476590be557ff8783c03",
      "f3bbc8e2a0904c2d803c2ac59f92e1c8",
      "58e9bc59efd4428c800586d951341827",
      "20b3f1ac0b0744dcb7a7b2a290637288",
      "768d954c86154b1ba282bb9db666bbc6",
      "25956ab5162446d8a844d003fe09ef53",
      "da6641f5e44f4c278aa753f0434c54a3",
      "0f9890fa952246ee9c7414c6348c38cb",
      "fe5889afb2ea4aa38b67461b22ceb4e6",
      "8632926af4944d9389c9773607d98f39",
      "679da967f2bd4c58a867e739d90202a2",
      "f2746429ab0d49d9bcb556334f2ade4d",
      "842ed628cc334f7c977ceb0b4ad27ecc",
      "4227023eff9f4963a09fae4da20bbf4f",
      "349e3a3794a04b9499f9b69c8a67d3f6",
      "dac05e5586994e389083b31de9ae5aa2",
      "e88610e81abe42d3b1fc00f4fe6038f1",
      "c49908d9337643e382ce331c6141de60",
      "473d15ae81c6483391ce9c4a1a53de72",
      "235458f11a374e54ad442b82caec7d74",
      "cd7c9e6816a04a2fb2944d053b5e2b99",
      "3e96249695494fa58262dca662ab46db",
      "889f82c8b46e4d879da51a464790f1cd",
      "9b252255139c4ad1b9aa2ec28c5070b7",
      "47e9dd8eccb2462e82f5b5b989255a87",
      "08e636746dc64bd4a9732596630032ea"
     ]
    },
    "id": "v15FuhDbj9TH",
    "outputId": "87b5ca6e-4b27-492a-a909-c009731cedc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d26e46011f4d96ab12552dc3b429d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-14 13:15:25 config.py:205] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-14 13:15:25 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='Pclanglais/Brahe-AWQ', speculative_config=None, tokenizer='Pclanglais/Brahe-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Pclanglais/Brahe-AWQ)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d1ac31410e437b922455ecd555936e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/933 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72b77925f6d42508cb9cb360568ea48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03529aeb8445436eb74b415534cb1401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b2223b79b143ed828c3da933370550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/549 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768d954c86154b1ba282bb9db666bbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 13:15:27 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-14 13:15:27 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 05-14 13:15:27 selector.py:32] Using XFormers backend.\n",
      "INFO 05-14 13:15:30 weight_utils.py:199] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac05e5586994e389083b31de9ae5aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/7.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-14 13:16:36 model_runner.py:175] Loading model weights took 6.8084 GB\n",
      "INFO 05-14 13:16:39 gpu_executor.py:114] # GPU blocks: 977, # CPU blocks: 327\n",
      "INFO 05-14 13:16:42 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-14 13:16:42 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-14 13:16:53 model_runner.py:1017] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "output_merged_dir=\"Pclanglais/Brahe-AWQ\"\n",
    "llm = LLM(output_merged_dir, quantization = 'awq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwK_uopXbpgp"
   },
   "source": [
    "Define the parameters for text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eiBsH45T_MS2"
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=500, frequency_penalty = 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSAWq4INZPGQ"
   },
   "source": [
    "## Start analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRrsaPLLUT7x"
   },
   "source": [
    "### Detective Story in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jni5xkRp__WF",
    "outputId": "4055a181-676a-4f94-d1a3-d69882a9de7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text is a conversation between two characters, discussing the '\n",
      " 'death of someone named Red Aki and their plans to investigate.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious, tense\\n'\n",
      " 'Genre: Mystery/detective fiction\\n'\n",
      " 'Speech standard: Informal, colloquial\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "“赤井秀一死了。基尔杀的。由你这位前叛徒的好友去确认一下。你应该不会出问题吧，苏格兰？”\n",
    "“他选择的居然是基尔？我还以为会是你给他‘最后一吻’呢，琴酒。”\n",
    "对面重重啧了一声，挂断电话。黑发蓝眼的男人与旁边偷听的陌生人对上视线，耸耸肩笑了笑，没再去看对方尴尬的表情，摁灭屏幕，收起手机，抬头望向前方已经开始排队的登机口。\n",
    "\n",
    "“……由吉隆坡国际机场出发、前往日本东京的旅客请注意，您乘坐的JL414次航班现在开始登机。请带好您的随身物品，出示登机牌，由3号登机门登机。祝您旅途愉快。谢谢！”\n",
    "--\n",
    "<波本，调查赤井秀一的死亡真相。Time is money!>\n",
    "<收到。>\n",
    "“真是不让人消停啊，朗姆。”\n",
    "伦敦郊区的一处别墅中，金发深肤的男子放下手机，靠上靠背，伸了个懒腰。他稍微活动了一下脖颈和手指，随手点开旁边手提电脑的一个页面，定了七天后从希思罗机场飞往日本羽田机场的机票。头等舱。\n",
    "他给另一台电脑插上一个U盘，鼠标从中拖拽出文件，拉进一个软件开始解析。紫灰色的下垂眼中倒映着屏幕的白光与黑字，男人在除自己外无人的房间中低不可闻地喃喃自语。\n",
    "“把你解决掉后我就不得不去找某个死了也不让人安生的家伙了，你最好物超所值呀。”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rcm1YI6kRYIs",
    "outputId": "38c78fb0-c0df-44e1-96ef-3c3f704ae5ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: A young man named Yellow Child Ben is speaking with another '\n",
      " 'character, Zhang Xianqi, in a teahouse. They discuss the disappearance of '\n",
      " 'someone named Peng Xingxing.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious\\n'\n",
      " 'Genre: Historical novel\\n'\n",
      " 'Speech standard: Standard language with some formal elements\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "“请坐。”青年笑出满口白牙，乐呵呵的样子倒真像那些打着算盘的生意人。“我叫黄子弘凡，你叫我黄子便可。”\n",
    "\n",
    "曹恩齐把遮面的斗笠摘下来，随手放在桌上，略鞠一躬，“曹恩齐，请多指教。”\n",
    "\n",
    "“不必客气。”黄子弘凡神色微微一动，示意他坐下，大大咧咧地拍了拍他的肩膀，提手斟好一杯热茶，“小店粗茶，招待不周，还望谅解。”\n",
    "\n",
    "“黄掌柜才是客气了。”曹恩齐也不推让，一口饮尽，在略带惊讶和欣赏的目光中补充，“方才小厮既说掌柜不再加座，听完你的邀约又毫无疑义，我想这彩云楼的掌柜也不会有他人了。掌柜真是年轻有为。”\n",
    "\n",
    "“就不能是我作为食客盛情难却吗？”黄子弘凡笑得更开心了，“黄子便可。”\n",
    "\n",
    "“如果想低调些，下次可以换身行头。”曹恩齐扫了眼他白袍上细密精致的刺绣，忽地压低声音，“黄子弘凡，一位故人托我给彩云楼送个口信。”\n",
    "\n",
    "“故人？”他不动声色地反问，给自己也续上清茶，端起瓷杯抿了一口。\n",
    "\n",
    "“是名道长，名叫蒲熠星。他说，以防万一。”\n",
    "\n",
    "黄子弘凡的表情凝固了一瞬，眼里闪过悲伤之色，手竟是难以控制地颤抖起来。他垂下头，收起笑容后的面庞带了肃杀的意味，“阿蒲的消息还是太迟了。南波门，被灭了。”\n",
    "\n",
    "此言一出，曹恩齐浑身一冷。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWZPuMymSKJR",
    "outputId": "0dd59bba-042b-42bb-ce19-5beff17113a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " 'Summary: A woman is terrified by something she sees and asks a man for his '\n",
      " 'opinion. The man observes the body and determines the cause of death. '\n",
      " 'Another character, Rust, joins them and they discuss the crime scene. They '\n",
      " 'find a ticket from a hospital cafeteria, indicating that the victim may have '\n",
      " 'been an employee.\\n'\n",
      " 'Narrative arc: Investigative analysis of a crime scene\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Serious, investigative\\n'\n",
      " 'Genre: Crime fiction\\n'\n",
      " 'Intertextuality: Police report or investigation file\\n'\n",
      " 'Speech standard: Standard language with some technical terminology '\n",
      " '(medical/police jargon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "然而，这周五的早上注定是脱离常数的一天。女人伸个懒腰，露出赏心悦目的女性身体曲线，还有此刻的美景，男人享受这两件东西。突然，女人神色变了，下一秒她的牙齿开始打颤，别的人很难注意这样的细节，但枕边人可以，从眼神里他看出女人的眼神已经完全被恐惧占领。\n",
    "\n",
    "女人用一种男人从未听过的声音颤颤巍巍地说：“Johnny，你看，那边躺着的是不是个人啊？”\n",
    "\n",
    "越过呆呆瘫坐在草坪上的报案人，抬起黄色警戒线，Marty照例先去看了一眼受害人。除了个别特殊情况，比如赤身裸体跪姿戴鹿角的女人，大部分受害人相比之下还是相对平凡不起眼的。就像这个受害人，和他年轻相仿，伤在后脑，于是正面躺着的时候几乎看不出什么伤痕，只是他的眼睛圆睁着，似乎还在惊愕死亡就这么来了，而死亡又让那一瞬的惊愕凝固，最后在现场勘察人员的相机里结成一张纸。\n",
    "\n",
    "”死亡时间是什么时候？“\n",
    "\n",
    "“由肝脏温度推断，死亡时间为昨天晚上大约十点左右。”\n",
    "\n",
    "Marty在他的笔记本上写了点什么，这个笔记本不大也不小，写完他下意识往自己来时的方向看了一眼。\n",
    "\n",
    "“死因呢？”\n",
    "\n",
    "“初步观察是颅盖骨骨折造成的脑膜中动脉损伤，进而引起的硬膜外血肿。“ 钝物击打后脑致死，这样的死因老警察见过没有一百也有八十了，尽管如此，Marty还是记下来，写完他又往同样的方向看了一眼，然后他蹲下来，仔细观察唯一的伤口，看着并不大，估计是角度很刁钻，一下命中了动脉。\n",
    "\n",
    "好像有感应一样，他转过身，Rust就正好朝着他走过来了。他走的很正，Marty的心跳了一下，恍惚间他以为现在还是95年。好在他及时反应过来，伸手为Rust抬起警戒带，Rust和他配合得很好，稍稍一低身子就灵巧地钻了进来。\n",
    "\n",
    "Marty在他耳边尽职尽责地汇报刚刚记录下来的内容，Rust站在尸体旁边，定定对着尸体看，Marty盯着他的侧脸和眉骨，若有所思的样子，于是很相信他观察的结果：“钱包不在了，或许只是抢劫，劫匪下手也不重，只是正好击中了动脉。”\n",
    "\n",
    "“这个地方很适合亲近的人来不是吗？这么好的一片风景。”Rust说，背景里Johnny正拨开女友的头发专注地安慰着她，坚定地告诉她一切都会没事的。\n",
    "\n",
    "旁边穿着制服的普通警察向他们走来：“虽然没有钱包，但我们在死者口袋发现了一些小票，其中这张我觉得你们应该看看。”\n",
    "\n",
    "是一张普林斯顿教学医院食堂的点餐小票，时间是昨天，点了两份汉堡薯条。一般来说，医院内对待病人实行送餐制，这说明很有可能死者是医院职工。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNZYgPLAUHhv"
   },
   "source": [
    "### Love Story in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZlzHjYkULHL",
    "outputId": "b30e2164-55c7-497a-a965-95040332aeaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Dante takes a long time to get water for Cressida, and she feels '\n",
      " 'uncomfortable with his gaze. They discuss her symptoms and Dante asks if she '\n",
      " 'is pregnant.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious\\n'\n",
      " 'Genre: Fiction\\n'\n",
      " 'Speech standard: Standard language with some informal dialogue\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "但丁去取水花的时间比预想中长，他磨磨蹭蹭地拿着水走进来，眼神略带探究的意味，来回扫视着翠西的眼睛和小腹。翠西被他盯得不自在，疑惑地蹙起眉头，内心升腾起不祥的预感，与此同时也做好了心理建设，去接住但丁又粗手粗脚搞砸了某些小事的坦白。\n",
    "\n",
    "“怎么了？”\n",
    "\n",
    "“先喝水。”但丁把水杯塞进她的手里，堵住她的话。直到亲眼看着她喝下，从她手中接过水杯放好，才继续说下去：“你有没有觉得头晕乏力？”\n",
    "\n",
    "“嗯。”看他郑重其事的态度，翠西也认真了几分。\n",
    "\n",
    "“全身发热，恶心想吐，嗜睡多梦，这些都有吗？”\n",
    "\n",
    "“都有……”\n",
    "\n",
    "“那最近情绪起伏大吗？”\n",
    "\n",
    "“你想说什么呀？但丁医生？”翠西忍俊不禁地调侃道。\n",
    "\n",
    "但丁的脸色不太好看，似乎在斟酌着更合适的措辞，犹豫许久后还是放弃抵抗，一字一句吐出那个答案。\n",
    "\n",
    "“……你，会不会是怀孕了？”\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5tbAya2W6Qs",
    "outputId": "9baa28e8-4fb9-496a-a60d-a5a2f316a1c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Dionysus is restless and anxious in his carriage, constantly '\n",
      " 'adjusting his seating position. He is distracted by the distant city of '\n",
      " 'White Dragon and fidgets with the upholstery.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Anxious, restless\\n'\n",
      " 'Genre: Historical fiction\\n'\n",
      " 'Speech standard: Standard, formal language\\n'\n",
      " 'Literary form: Conversation/dialogue\\n'\n",
      " 'Active character: Dionysus, Tiren']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "桑布雷克的夏季温和少雨，坐在马车上的狄翁却显得有些躁动不安，他一次又一次地调整坐姿，时而倚在窗上盯着远处的白龙城，时而又靠在椅背上对着车顶发呆，双手不断地摩挲他柔软的天鹅绒座椅套，命令它们在湖蓝色和深蓝色之间改变颜色。\n",
    "“您刚刚有些失态，我的皇子。”\n",
    "狄翁伸出手摆了摆，说：“不必再说，何况在你面前也不是第一次。”\n",
    "“能拥有这份特权是我的荣幸，先生。”泰伦斯眨眨眼，手在空中划了一个小圈，向狄翁行礼。\n",
    "“你有时候真的很讨厌，比如说现在。”\n",
    "他稍稍前倾，脱下手套扣住了他的手，十指穿过他的指缝。泰伦斯顺从地配合他，让二人手上的茧子吻在一起。马车颠簸，狄翁离开座位顺势跨坐在泰伦斯身上。忠心的亲卫队长没被占领的左手按住狄翁的背，好让他的团长坐得安稳一点。泰伦斯听到狄翁满意的叹息声，头埋进泰伦斯的颈窝里，金色的发丝擦过他的鼻尖，棕发的骑士闻到了光的味道。\n",
    "“我爱你。”\n",
    "“您怎么突然说这个？”\n",
    "“因为我爱你，就像你爱我一样。”\n",
    "“那我爱的狄翁殿下能解释一下刚才发生了什么吗？”\n",
    "“你会知道的，但在此之前，你得先想一下我们在修道院学校的时候的事儿。”\n",
    "“为什么？”\n",
    "“桑布雷克的龙骑士需要打有准备的仗。”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGl72T6lXuKg",
    "outputId": "44c15ab7-4908-426a-efaf-c53b04bffd67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " \"Summary: Suo's family invites Ise to eat and discusses her marriage \"\n",
      " 'prospects.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious, tense\\n'\n",
      " 'Genre: Realistic fiction\\n'\n",
      " 'Speech standard: Standard, polite language\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "“早上好，苏我家主，苏我夫人。”大理石墙壁的光泽让她在闷热的夏日感到一丝寒意，苏我家主与夫人都在读报纸。“伊砂，你来啦？”苏我夫人放下报纸，“快坐下吧，要是饿了就先吃。”\n",
    "伊砂挤出一个笑：“谢谢夫人，我还不饿。”\n",
    "苏我夫人举起报纸，“伊砂你都十八岁了，再不嫁人可就晚了哦。女人的青春贬值可快了。”伊砂强忍恶心，做出一副害羞的表情：“夫……夫人说的是。”家主见到她的反应，洋洋自得地笑了笑：“女人就是麻烦，别到最后生不了孩子了还嫁不出去。”家主用刀叉时，脸上的赘肉总是会跟着手臂的动作抖动。他对待食物与钱就像对地位比自己低的人一样，迫不及待地想要享用。\n",
    "“家主说得没错。”伊砂觉得自己已经什么都吃不下了。\n",
    "“既然这样的话，”夫人听到这话，高兴地往伊砂的方向转了一点，“最近刚好有位大人在物色妻子，伊砂你要不要去？”\n",
    "伊砂睁大了眼睛：“现在吗？”\n",
    "“那位大人很有权势，你嫁给他可算是享福了。”苏我夫人在把烂摊子交给伊砂时，最爱装作嫉妒她的样子。她花了几十万保养自己的脸，但苹果肌与嘴唇两旁的肌肉都很僵硬，让人看不出来她是不是真的在笑。\n",
    "伊砂也笑了笑：“那就太感谢家主与夫人了，您们对我这么好，还愿意把我嫁给好人家，真是不知道该怎么报答。”\n",
    "“伊砂，你这么说我真高兴，”夫人歪着眉毛对她挤出一个用力的笑，“我们家里最近经济紧张，不过你的嫁妆，我们会尽力的。毕竟是我们伊砂的婚礼，总得像个贵族小姐吧？”\n",
    "没错，像个贵族小姐。父母的遗产早就被亲戚分走了，她在苏我家的价值与一幅画差不多。嫁人后，她又会有多少价值呢？\n",
    "“您们的恩情，我会用一辈子报答。”伊砂说着假话，抹着真心的眼泪。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhvKK4BQaSBj"
   },
   "source": [
    "### Mystery Fiction in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ykyrg3MaQ7-",
    "outputId": "08af12ac-a113-4aac-d188-29d709acc28a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " 'Summary: The text is about a team of archaeologists who discover ancient '\n",
      " \"artifacts in the mountains, but the surviving member's account is unclear \"\n",
      " 'and confusing.\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: First-person narrative\\n'\n",
      " 'Tone: Mysterious, curious\\n'\n",
      " 'Genre: Adventure/mystery fiction\\n'\n",
      " 'Intertextuality: Archaeological report or article on ancient civilizations\\n'\n",
      " 'Speech standard: Standard literary language with some technical '\n",
      " 'archaeological terms and descriptions of artifacts and locations']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "科考队深入静峪脑后音讯全无，失联两个星期后只有一男性幸存者被救yu涝河口附近村落，但神志不清。幸存者恢复知觉后，讲述了一则颇为怪诞的故事，他声称，科考队在秦岭深处发现了殷商遗址。后续在此人带领下再带队前往却只得破碎的青铜刻板，痕迹模糊，成于大概三千年前，但是否为商朝旧物仍有待考究。”\n",
    "\n",
    "奇怪，殷商的东西虽然名贵但并非不可探查，况且这些青铜板上的图案我从来不曾见过，听起来有些自负，但我毕竟是考古系研究生，主要方向恰好就是中古文明，所以我没见过，就说明这件事并不受重视，以至于继续研究的人都没有。\n",
    "\n",
    "好奇心前所未有的膨胀起来，我拆开了剩余三张信纸，墨迹十分清秀，让我真正大吃一惊的，却是上面的内容。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-r_4B8jcj23",
    "outputId": "e66ebf97-a73b-4285-c625-b57c22cd3ff5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: After the death of his mother, Emperor Cao Rui becomes a ghost in '\n",
      " \"the East Palace. The people around him are aware of his father's promiscuity \"\n",
      " 'and try to avoid him. One day, while being groomed by servants, he '\n",
      " 'accidentally breaks a comb and is unable to speak after that. He becomes '\n",
      " 'frustrated and leaves the palace, refusing to see anyone for the rest of the '\n",
      " 'day.\\n'\n",
      " 'Trope: Promiscuous royalty\\n'\n",
      " 'Narrative arc: Frustration\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Tragic\\n'\n",
      " 'Genre: Historical novel\\n'\n",
      " 'Intertextuality: Historical account or biography\\n'\n",
      " 'Speech standard: Formal/literary language\\n'\n",
      " 'Literary form: Description of events/conversations']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "母亲死后曹叡在东宫变成一个幽灵。大家惹不得他，毕竟是皇帝的儿子，但甄夫人的事人人皆知，陛下并不是那种不会迁怒于人的严谨性子，不如说曹氏一族都天然地有点滥情，在爱和恨上追求一种穷尽全力，像新修洛阳宫时上的漆，沾上就很难洗掉，洗掉也总有气味，为了往后余生而让人不得不避闲。东宫于是门前冷落，却也不完全是件坏事——曹叡那时候受一种奇怪病症困扰，说不了一整个长句子。他们是这样发现这回事的：有一天早上，大概就是甄夫人死后一个月的时候，婢女们服侍他洗漱，帮他篦头发，梳子用旧了，他头发又太长不方便，那天很不巧地折断了一个齿。曹叡闭着眼睛听到自己耳边极近极近那清脆的一声响，一根弦断了一样强烈地感到不吉利。他睁开眼睛，要他们赶快去换一个：“怎么断了？快去换——换……”他说到这，后面的字卡住一样吐不出来了。曹叡错愕之中捂住自己的嘴，那些原本纷纷拿着他头发的女子们诚惶诚恐地看着他，在意识到不对之后纷纷跪了下来。更多的梳子慌乱之中落到地上。她们以为是自己惹平原侯发怒，而曹叡坐在榻上想：你们这么紧张做什么？可这句话他也说不出，只剩看着她们，少女们低着头，头上发髻装饰的流苏在没有风的宫殿里颤动，玲玲地响，直到他忍无可忍地站起来绕开面前的人向外面走去，漆黑的头发和衣摆一起拖在地上。他走出门去，喊宫里的内侍。今天他不能见人了。今天……今天……曹叡张嘴，决定不显得自己是一个结巴，把一句话分成三个部分。“不舒服。”他指指自己，“让他们，”他又指指外面，“回去。”第二天宫里所有人都知道了这一回事。郭夫人派人过来慰问，曹叡对着那个女官很恭敬，知道自己名义上的母亲其实是替他血缘上的父亲来的，只想把她快点打发走。他向她解释自己没甚么大碍，话语无意间碎成好几块。女官了然，告退了，过几天郭夫人又送了更多东西来，都被封存入库。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpGjSJ4Edfhu",
    "outputId": "7eab23ee-302b-45b1-e843-c74543f644de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text is about a character named Ma Hanqing who is in a '\n",
      " 'dangerous situation and contemplates his options.\\n'\n",
      " 'Trope: Desperation, contemplation of death\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Tragic\\n'\n",
      " 'Genre: Historical novel\\n'\n",
      " 'Intertextuality: Political intrigue, conspiracy theories\\n'\n",
      " 'Speech standard: Formal, literary language']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "只差一点了。\n",
    "\n",
    "马汗青死死盯着着桌案上的鎏金锦盒，挑开直冲胸口的利箭，额角细密的汗水证明了他费尽千辛万苦才抛开府兵闯到这里，书房虽然无人把守却遍布机关，稍不留意就可能命丧黄泉。可目标已再眼前，只要再近一步，教他劈了这块害人的木头毁了那封伪造的密信，舞太师就没法拿出马将军通敌叛国的“证据”，十万马家军就能恢复清白，马将军也无需再受牢狱之灾。\n",
    "\n",
    "心口忽然传来一阵绞痛。\n",
    "\n",
    "糟了。马汗青心下一沉，那些人想必已经抓住了自己放出的幻像，再过不久就会追到这里，而自己还没能寻到这机关阵的破绽，与其坐以待毙，倒不如拼个玉石俱焚，比上马将军的清白和十万马家军的前程，死一个马汗青又算不得什么。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_sq5qcIm4so"
   },
   "source": [
    "### Epic Fantasy in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbvNwb6lfPZY",
    "outputId": "aabec432-238b-4b19-ab2b-6b99ac0bb9a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Summary: The text describes the protagonist's journey through a village \"\n",
      " 'during a war.\\n'\n",
      " 'Narrative arc: Descriptive\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious, somber\\n'\n",
      " 'Genre: Historical novel\\n'\n",
      " 'Intertextuality: Historical documentary\\n'\n",
      " 'Speech standard: Standard literary language\\n'\n",
      " 'Literary form: Description of a place and conversation between characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    " 在旅程几乎要过了一半时，二人才终于见到了沿途第一个有人烟的村镇。\n",
    "\n",
    "然而，本应是为旅人补充行装的地方，实际的情况却十分糟糕。阿蕾奇诺让哥伦比娅留在鞍垫上，自己则下了马，又走到左前方，牵着缰绳慢慢向前步行着。镇上四处游荡着衣衫褴褛的人，宛若一具具的空壳，连眼睛都几乎不曾转动一下。只有当看见衣着光鲜的来客时，那些瘦得如同骨架般的“人”，才会稍微张张嘴，表现出一点人的生气来。\n",
    "\n",
    "“他们为什么会变成这样？”哥伦比娅在马背上看着逐渐围过来乞讨的人群，问道。\n",
    "\n",
    "“战争。女皇正在向西征战。打仗所需要的粮草，财钱，人力，最终都会化为劳动者的重负。它为皇室带来声权，为贵族带来利益，而后又将隐藏在阴影中的税负，饥馑，压在每一个底层的人民身上。”阿蕾奇诺应着少女的话，从裘袍内侧掏出几枚银币，递给眼前的乞人。\n",
    "\n",
    "“他们为什么不去其他地方？”\n",
    "\n",
    "“他们无处可去。自出生起，农奴就被绑定在了土地上。他们属于耕田，属于庄园，属于领主——但唯独不属于自己。”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTKymauOj6KJ",
    "outputId": "7241e5fa-9f75-4b63-ec3d-95cfee097884"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " 'Summary: The text is about a young student who leaves his hometown to travel '\n",
      " 'to the new capital of Sanburek, accompanied by a bird. Along the way, he '\n",
      " 'encounters various landscapes and wonders about the mysteries of the past.\\n'\n",
      " 'Trope: Journey of self-discovery\\n'\n",
      " 'Narrative arc: Reflective and contemplative\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Reflective and contemplative\\n'\n",
      " 'Genre: Historical fiction or fantasy\\n'\n",
      " 'Intertextuality: Travelogue or diary entry\\n'\n",
      " 'Speech standard: Standard literary language with some poetic elements']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "新历初始，桑布雷克与周边两国一同修订了全新的律法。在战争中重新团结起来的三国同盟，如今正在努力恢复破碎的世界。就算是一百年之后，大路上流传着各种各样的旧历传说。很多事情是被忘却的事实，很多事情是虚掩着真相的线索。修道院的一位年轻学徒告别家乡，从偏远的龙巢边缘前往桑布雷克的新首都新奥列夫列姆的圣白龙城。他像一位苦行僧人，穿着简单潦草的布鞋沿着石子路向北而行。\n",
    "\n",
    "陆行鸟被他牵在身后，是他唯一的旅伴，他不忍让这托着重物的可怜小家伙再增添自己的重量。于是他慢慢悠悠地走出平原，穿过瀑布，直到他们望到了海边悬崖的瞭望塔。\n",
    "\n",
    "他翻开行囊中的书籍，那里记载着战争前世界地图的模样。他所处这片庄稼稀少的荒芜之地，一百年前仍有变异的怪物作乱，两百年前甚至被淹没在神秘的高浓度以太中难以前行。他反复在口中咀嚼以太这两个字，对这来自过去的未知充满了好奇与恐惧。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7eHU3KtlloQ",
    "outputId": "ab5425f8-ab96-4c7e-bf05-2edf22e8d969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text is a conversation between two characters, Odysseus and '\n",
      " \"Hermes, discussing the end of Odysseus's ten-year imprisonment.\\n\"\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Conversational\\n'\n",
      " 'Genre: Epic poetry\\n'\n",
      " 'Speech standard: Informal conversation\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "“老朋友，十年的监禁生涯结束啦，你感觉如何？重获新生？还是说，有点不舍？”\n",
    "这声音轻佻但有力，不像是凡间所拥有的。它向着倾听者颅内发出，有着不容抗拒的侵略意味，在耳朵的内腔里翻滚一番，引得整个人与之共鸣起来。\n",
    "“赫尔墨斯。”奥德修斯认出来者，有些意外地看向他，“你怎么会在这里？”\n",
    "“我来看看我们可怜的小狗，在这荒凉的小岛上过得好不好呀。”\n",
    "赫尔墨斯的语气微微上挑，如同他自身一样漫不经心。他蹬着飞鞋在空中愉悦地打着转，宛如一只羽翼丰满的小鸟。但奥德修斯知道，他和雅典娜不同，并不近似于猫头鹰，而是更加散漫和自由。赫尔墨斯乐意一天为一天买账，一刻为一刻过活，比起正派的智慧，更乐意耍小聪明。他看上去倒是个容易亲近的神明，但其实是另一种层面的难对付。但是，再怎么游手好闲，也不会无缘无故地来打趣自己。这其中一定是有原因、有预兆、有指引的，奥德修斯这么想到。\n",
    "聪慧如奥德修斯，很快领会了神使的意图。“是你劝说卡吕普索放我走的，那么，这是神的旨意吗？既然如此，我是否可以猜测，在不久的未来，我终于能够如愿，回到那伊萨卡吗？”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUocZYBam_gV"
   },
   "source": [
    "### Children's Fiction in Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjwdWdynnHfL",
    "outputId": "e1f5713a-1f35-4422-aec3-7a32cf4c3f0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The characters have been sitting in the monitoring room for an '\n",
      " 'hour, with Alana and Chirton constantly watching the monitors. Magg is '\n",
      " 'uninterested but brings food to Alana. Jack leaves after half an hour, '\n",
      " 'frustrated by the time spent observing a mute child and a woman painting. '\n",
      " 'Chirton gives up and leaves, leaving only Alana and Magg. They discuss '\n",
      " \"autism and wonder why they haven't heard the girl speak.\\n\"\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Serious\\n'\n",
      " 'Genre: Psychological fiction\\n'\n",
      " 'Intertextuality: Scientific article on autism\\n'\n",
      " 'Speech standard: Standard English\\n'\n",
      " 'Literary form: Conversation/dialogue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "他们已经在监控室坐了一个半小时。阿拉娜和奇尔顿从始至终盯着监控屏幕，奇尔顿手头还有一支录音笔，而玛格口头上说着不感兴趣，却在六点整提着些食物走进房间，还给阿拉娜带了杯饮料；杰克半小时前离开，他没有空闲时间看一个精神病人画画、一个不说话的小孩涂鸦，还有一个不幸用子宫让他们俩产生联系的倒霉蛋对着地板发呆。\n",
    "\n",
    "“这个世界上总有比观察自闭症儿童——和成年人更重要的事，”奇尔顿宣告投降，“诸位，回见。”\n",
    "\n",
    "他也匆匆离开，只剩阿拉娜和玛格面面相觑。\n",
    "\n",
    "“所以这是自闭症。”玛格转向屏幕，看到威尔起身，为女儿收拾画笔，“怪不得没见她说过话。”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLiHEkOao6nf",
    "outputId": "f51c8ed6-4c5b-48c3-c87a-2bfb91e2c7d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: A young demon hunter finds Vigil in an unfinished house and tries '\n",
      " 'to take him away.\\n'\n",
      " 'Trope: The mysterious stranger\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Dialogue between characters\\n'\n",
      " 'Tone: Mysterious, suspenseful\\n'\n",
      " 'Genre: Fantasy, adventure\\n'\n",
      " 'Intertextuality: Mythology, folklore\\n'\n",
      " 'Speech standard: Informal, colloquial language used by the characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "年轻的恶魔猎人在找到维吉尔的时候他正蜷缩在斯巴达老宅的一楼角落，还没有完全装修过的老宅内部空空荡荡，年幼的维吉尔就披着一件脏兮兮的窗帘，蜷缩在楼梯下的阴影里。尼禄挠了挠头，他蹲下身子才能勉强看清楚这个小小阴影里的全貌，年轻人一边腹诽着一定得把这里用水泥封好免得老鼠在这里做了窝，一边小心翼翼地试图把维吉尔抱起来。\n",
    "\n",
    "小孩睡得很浅，几乎是尼禄的手刚碰到他，他就陡然惊醒了。\n",
    "\n",
    "“嘿，嘿，嘿，我没有恶意，好吗？”尼禄举起双手，维吉尔艰难地拔出刀，阎魔刀的刀尖颤巍巍地指着尼禄的鼻子：“你不记得我了？老天，这都是什么事儿——你还记得你怎么来到这儿的吗？”\n",
    "\n",
    "“闭嘴！”年幼的维吉尔厉声呵斥他：“告诉我你的目的！”\n",
    "\n",
    "“我的目的就是把你带回去……不，不是那种，操，我的目的是把你带到一个安全的地方……”尼禄叹了口气：“听上去更可疑了……你想去见但丁吗？虽然他已经老得不像样了。”\n",
    "\n",
    "维吉尔紧盯着面前的男人，他知道自己身上大概出了点问题，穿越，或者是时间倒流。小孩醒在红墓市的边郊，这里和他记忆里的很不一样。他艰难跋涉过来的时候目之所及的一切已经告诉了他答案，红墓市没有这么多毁坏的建筑，街上的装饰和广告也和他见过的不同。\n",
    "\n",
    "“你是谁？”少年问：“我和你是什么关系？”\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLbSkiNSqCzh",
    "outputId": "36141732-258c-4cd3-f41d-4ecd84d19e91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " 'Summary: The text describes the scene at a World Cup event, with various '\n",
      " 'tents and activities taking place.\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Descriptive, enthusiastic\\n'\n",
      " 'Genre: Sports fiction\\n'\n",
      " 'Intertextuality: News reportage, travel guide\\n'\n",
      " 'Speech standard: Standard language with some informal expressions and '\n",
      " 'foreign words/phrases\\n'\n",
      " 'Literary form: Description of a place/event']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "清晨的薄雾渐渐散去，阿斯托利亚能看到四面八方布满了帐篷，看来这次英国承办的世界杯很受欢迎。尽管魔法部“尽由一些愚蠢的人发号施令”，但能聚焦这么多来自世界各地的人并保证他们的安全，阿斯托利亚还是很佩服的。\n",
    "\n",
    "在她们的周围，有几个插着星条旗的大帐篷，门口摆满了酒瓶，这使达芙妮提起了她的上唇；还有个圆筒状尖顶的白色建筑（其他人把它叫做“穹庐”），几个大汉在外面握着类似鞭子的东西，向烤架一挥动，肉就发出被烤熟的滋滋响声，阿斯托利亚不禁张大了嘴巴。\n",
    "\n",
    "她们继续向前走，路上发现了不同国家的更多帐篷，一个模样像是法国人的毛头小子还向阿斯托利亚笑着说了句外国话，但被达芙妮赶走了。阿斯托利亚被几个塞拉利昂女巫玩的一种不知名的纸牌吸引住了，她正要俯下身看个究竟，达芙妮拍了拍她的肩膀。\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooICxQTVsCt1"
   },
   "source": [
    "### Detective Story in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hr1vCTszsGO",
    "outputId": "2ef9f113-0fb9-42e9-e11c-36d7a7ff0aff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text is a news report about Dazai Osamu, a former executive of '\n",
      " 'the Port Mafia, who is on the run and possibly planting bombs in Yokohama '\n",
      " 'bay.\\n'\n",
      " 'Enunciation: News anchor\\n'\n",
      " 'Tone: Serious and urgent\\n'\n",
      " 'Genre: Mystery/thriller\\n'\n",
      " 'Intertextuality: News report\\n'\n",
      " 'Speech standard: Standard news language\\n'\n",
      " 'Literary form: News report/narrative description of events\\n'\n",
      " 'Active character: Dazai Osamu, Atsushi (mentioned but not active character']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "“Breaking news!” came the voice from the TV in the agency. “We got this just in. Apparently, Dazai Osamu, former executive of the notorious Port Mafia reigning over the harbor of Yokohama and its underground, was just seen walking around in the middle of the Yokohama bay. Eyewitness hadn’t been able to clearly see what the man was doing; they assume that he might have planted more of his bombs.”\n",
    "\n",
    "The stage is set and the curtain opens. Yokohama is in the middle of it all and Atsushi is nowhere to be found, Dazai is on the run and just about everything is going wrong. The Hunting Dogs are on the prowl and a lot of the mess doesn't make sense, if it ever did.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljIgEY3D0lbY",
    "outputId": "93716b0f-6d51-4e53-c0fd-6ef4d05e2aae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Hannibal reminisces about a past encounter with Will and fantasizes '\n",
      " 'about their potential relationship.\\n'\n",
      " 'Trope: The seductive and manipulative character\\n'\n",
      " 'Narrative arc: Desire and longing for Will\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Erotic, sensual\\n'\n",
      " 'Genre: Psychological thriller, erotica\\n'\n",
      " 'Speech standard: Poetic, elevated language\\n'\n",
      " 'Literary form: Stream of consciousness, internal monologue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Every once in a while, when he allowed himself to indulge, Hannibal would enter his mind palace, wander the halls until he found the leather bound book he had so closely associated with that day and he would remember. He would remember how Will had stood with hunched shoulders, in a suit that might have looked like it fit, to the trained eye was a nightmare of seams. How glasses had obscured big blue eyes at first and how unruly dark brown curls had been crudely attempted to be tamed.\n",
    "\n",
    "While in reality, it never went past that first look across the conference hall, sometimes, the doctor would allow himself the alternate path of events, where he crossed the floor to introduce himself. The thrill of integrating himself with the very people who were seeking to catch him. He imagined how he would slowly and seamlessly distance himself and Will from the younger man’s colleagues, how he would coax more wine down that scruff-covered throat.\n",
    "\n",
    "How they would end up in Hannibal’s hotel room by the end of the night.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ez4NQLSY1z3T",
    "outputId": "e6535109-99c4-422c-b41f-8ac76fd49853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Gem is caught in a net and pulled out of the water by humans on a '\n",
      " 'boat.\\n'\n",
      " 'Trope: Trapped hero\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Suspenseful, dramatic\\n'\n",
      " 'Genre: Adventure, fantasy\\n'\n",
      " 'Speech standard: Standard, everyday language with some cursing\\n'\n",
      " 'Literary form: Action scene/chase sequence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Gem stopped for a moment, catching her breath. She couldn't do anything. Now Gem is clever but in moments of high stress she isn't as much. Hence why she didn't notice that the water was a bit too clear, and she didn't hear the motor of a boat.\n",
    "\n",
    "She couldn't do anything as the net caught her tail, making her trapped. Then it started to pull up. She couldn't move or get out, and trust her she tried.\n",
    "\n",
    "As she was brought out of the water, hanging by the tail from a net, pain starting to form from the strain, her weapons had long fallen out of its halter. She was met with actual humans on the small boat before her, staring at her with evil grins, ‘they don't look like the nice humans’ she thought. Then Gem swore to herself that this was the last time she wasn't going to pay attention to her surroundings.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1DFfdh95Vl4"
   },
   "source": [
    "### Love Story in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tSGN_cL4qnj",
    "outputId": "8f1021d5-0a95-4f80-c132-d9a25bb8e87a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Wooyoung, a bartender, flirts with a customer at the bar.\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Flirtatious\\n'\n",
      " 'Genre: Fiction\\n'\n",
      " 'Speech standard: Informal\\n'\n",
      " 'Literary form: Conversation/dialogue\\n'\n",
      " 'Active character: Wooyoung, the customer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Someone approached the bar from the entrance, making Wooyoung snap out of his trance. He tried to put on his best smile, a meaningless flirtatious look painting his features. He had to do whatever was in his capabilities to earn some extra cash, and most people were already put under a trance by his charms anyways, so what was the harm of playing into it?\n",
    "\n",
    "\"Hey handsome, what can I get you?\" Wooyoung approached the man who took a seat amongst the many free barstools, most people too consumed with being on the dance floor to sit around at the bar.\n",
    "\n",
    "The comment obviously took the customer by surprise, as his eyes widened just slightly. Wooyoung merely waited for the man across from him to snap out of his thoughts, smiling slightly as he pulled back the sleeves to his already exposing shirt. Not only was the v-line cut extremely low, but it was nearly see through, which was hard to tell in the darkness of the club, but when the light hit just right basically everything was revealed.\n",
    "\n",
    "\"I-I'll just have a Coke.\" The man gave his order with a hesitant voice, just barely audible over the music that continued to fill the room.\n",
    "\n",
    "\"Ah, you the designated driver?\" Wooyoung tried to make conversation, quickly ducking below the bar to reach for the appropriate can. He resurfaced after grabbing a glass as well, popping the tab and pouring the carbonated drink into the glass, pushing it gently across to the customer with a smile.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "punPKeOn5NRo",
    "outputId": "6997791b-0b75-4807-a4c4-49766a07bee1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The narrator is excited about opening a hospital but also worried '\n",
      " 'about their relationship with Damon.\\n'\n",
      " 'Enunciation: First-person narrative\\n'\n",
      " 'Tone: Worried, anxious\\n'\n",
      " 'Genre: Drama\\n'\n",
      " 'Speech standard: Informal, everyday language\\n'\n",
      " 'Literary form: Stream of consciousness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "As I got up in the morning from a still-warm bed, after Charles had already left to do his morning chores I was both excited and a little worried. I was excited because it would be the opening of my hospital. My hospital. Well, it would be Mimi Salvatore's hospital. Now I had two other surnames, but it was easier this way. Charles had assured me that neither he nor Adam minded. Charles had taken care of everything that had to do with building and staffing the hospital. We had already been on a private tour, and today would be the Grand Opening.\n",
    "\n",
    "My life was far from dull. I sometimes hoped it would be a little less exciting or dangerous but I had no say in so many things that happened around me, so I just had to go with the flow so to speak, and take it all whatever life threw at me, and it did throw me a lot.\n",
    "\n",
    "The thing that always bothered me nowadays was Damon. His behavior had become increasingly difficult to predict in the last couple of years. The shed sessions had escalated and gotten rougher, and he was always finding reasons to take me to the shed. Or beat me up in the gym. It seemed that I had spent a considerable amount of the last five years, or so, recovering from his treatments. And still, I couldn't let go. Charles and Adam had now got me addicted to their pheromones, and that was another thing that was nagging at Damon.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFdq81kq6m8Q",
    "outputId": "5a94d91d-78b4-49bc-810f-15e95b81e7d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Summary: Calista enters Vargus' private quarters with a plan to kill him.\\n\"\n",
      " 'Trope: Femme fatale\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Dialogue\\n'\n",
      " 'Tone: Sinister, dangerous\\n'\n",
      " 'Genre: Thriller, crime fiction\\n'\n",
      " 'Speech standard: Formal, elevated language\\n'\n",
      " 'Literary form: Conversation/dialogue between characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "The small poison vial weighed heavy, hidden between her breasts as Calista walked through the gold-gilded corridor to her master’s private quarters. All of her sacrifice, planning, and conniving strategies were about to come to fruition- years in the making. She squeezed her eyes shut and released a shaky breath before she entered the grand room with a familiar feigned smile plastered on her face.\n",
    "“Ah, Calista.” Vargus purred from his comfortable seat directly across from the doors. Per usual he was dressed in his fine black silks and chunky bronze necklace, his bronze curls coiffed back and matching beard trimmed. A pair of mint green eyes lustfully gazed over her as she entered. He stood, his monumental frame dwarfed by the sheer enormity of the room. His private quarters were furnished with only the best that his many travels had to offer. All black lacquer furniture with a polished black and white checkered marble floor to match. Royal purple curtains hung from impressive-sized windows, cradling the shimmering night sky. A fire to the side casted a copper orange glow over Vargus’ face- sharpening his features and reminding her further that he was deadly.\n",
    "“Vargus.” She smiled sweetly, the same smile that unintentionally caught his eye over four decades ago, not that she looked a day over twenty-three.\n",
    "“Come sit.” He smiled as well, but his words were not a pleasantry they were a command.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcwTbwSIRjZm"
   },
   "source": [
    "### Mystery Fiction in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpPQ5itoRl-w",
    "outputId": "420166e9-91d5-42c9-8471-fc35dc95284f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Summary: Luule and Oleg's friendship turns into something more, and they \"\n",
      " 'continue to solve cases together.\\n'\n",
      " 'Trope: Friends-to-lovers\\n'\n",
      " 'Narrative arc: Investigation and resolution of a crime network\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Lighthearted, adventurous\\n'\n",
      " 'Genre: Mystery/Crime Fiction\\n'\n",
      " 'Intertextuality: Detective fiction, police procedural\\n'\n",
      " 'Speech standard: Conversational, informal\\n'\n",
      " 'Literary form: Action/adventure story with elements of mystery/crime '\n",
      " 'fiction\\n'\n",
      " 'Active character: Luule Tam, Oleg (the alien), other characters involved in '\n",
      " 'the crime network']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Twenty-something Luule Tam's life in the village of Kasmu in northern Estonia is fairly normal, even if magic and aliens exist in this world.\n",
    "\n",
    "After encountering Oleg, an alien who happens to be Head of the Red Guards, in a certain incident, the two of them would beome friends with Luule often helping Oleg solve cases. As time goes on, their friendship gradually blossoms into something much more.\n",
    "\n",
    "Recently, Luule has started dating Oleg. Despite the shift in status quo, solving cases is something that has remained unchanged. This time, Luule receives info from Oleg of a local crime network involved in smuggling. Together, the duo must find leads on the criminals' operations and stop them for good before it's too late.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJJEJWtWR2yN",
    "outputId": "d8307335-8179-4cc1-d642-3da0dba0db10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Summary: The text explores the theme of choosing between one's own life and \"\n",
      " 'the life of another, as well as a story about a warrior who defeats a '\n",
      " 'dragon.\\n'\n",
      " 'Trope: Heroic quest\\n'\n",
      " 'Narrative arc: Triumphant victory\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Epic, heroic\\n'\n",
      " 'Genre: Epic fantasy\\n'\n",
      " 'Intertextuality: Mythology, folklore\\n'\n",
      " 'Speech standard: Poetic, elevated language\\n'\n",
      " 'Literary form: Narrative with dialogue and action scenes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "What would you choose if you were to make a decision between your life and the life of another? Would yours weight heavy enough to overweight the other? Whichever you choose, you are wrong. But, someone else had something a lot more different than this to say. And that was none other than a mere wanderer who ran into a glorious empire on a very high mountain named Hei, with an emperor as imperial and grand yet selfish. However, there seemed to be a trouble within the empire, which was an enormous mighty dragon breathing fire and spreading chaos wherever it landed and up until that very day, it had managed to damage the empire and claimed lots of lives, causing terror upon the entire mountain for such a long time. Yet one day, this one gallant warrior showed up in the empire and was the only person present that was able to challenge the dragon. After a long wait for his mighty opponent, the dragon finally showed its face upon a fiery valley leading to the summit of the mountain, where a temple was built on, waiting for this next chosen warrior to dare a combat with. So, the warrior unsheathed his silver sword from its scabbard and beheaded the dragon at once before it could even make another move. Of course, the emperor and his people were truly overjoyed for this mysterious yet mighty warrior’s arrival in their empire and running to their rescue, so they threw a feast to celebrate the day of the warrior’s arrival and the very first victory they had ever had against the dragon.\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYUHZpbvSkpt",
    "outputId": "c67ae96e-e71a-4daf-d418-8819e8344f74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The narrator reflects on their new life as the Champion of Winter '\n",
      " 'and the changes it has brought.\\n'\n",
      " \"Trope: Hero's journey\\n\"\n",
      " 'Narrative arc: Reflective\\n'\n",
      " 'Enunciation: First-person narrative\\n'\n",
      " 'Tone: Reflective, confident\\n'\n",
      " 'Genre: Urban fantasy, coming-of-age story\\n'\n",
      " 'Intertextuality: Mythology, folklore\\n'\n",
      " 'Speech standard: Informal, colloquial\\n'\n",
      " 'Literary form: Stream of consciousness/internal monologue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "After a year of my new life full of magic, I was exhausted.\n",
    "Exhausted, but invigorated.\n",
    "Life had changed a lot since I had bombed my own home.\n",
    "For one, the magical government, MIASS, has declared me a passive threat, which basically meant I wasn’t kill-on-sight, but I was kill-on-first-sign-of-aggression. It wasn’t fun, but there wasn’t much I could do about that. Yet. If I hadn’t taken Mab’s offer, I would have been six feet under before I could so much as flick my gun up.\n",
    "Being the Champion of Winter had its perks, it seemed. Supernatural folk gave me more respect - no more was I just ‘a half-blood’ or ‘some wannabe wizard’. I was Fate Frost, the Claws of Winter. I had power, I had will, I had respect. It was getting to my head a bit, I think, but not enough to be worrisome.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XePjKyVcTNLU"
   },
   "source": [
    "### Epic Fantasy in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx0r2pyRTQNE",
    "outputId": "e1e10e6c-e43a-4923-e873-b6f649f08903"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The mind flayer is confused by the presence of a wizard and a '\n",
      " 'druid, as they trigger strange sensations and memories.\\n'\n",
      " 'Trope: The mysterious prisoner\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Mysterious, unsettling\\n'\n",
      " 'Genre: Fantasy\\n'\n",
      " 'Intertextuality: Horror fiction, science fiction\\n'\n",
      " 'Speech standard: Scholarly, philosophical\\n'\n",
      " 'Literary form: Stream of consciousness/internal monologue']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "That the wizard had come to speak with it, the illithid could attempt to rationalize. It knew Gale was voraciously curious – a commendable trait, in practice, as knowledge and the pursuit of the expanded mind was compatible with illithid philosophy – and the prospect of speaking to one of its kind with some mortal recollections to complement its present state was likely intriguing. It was the druid’s presence the mind flayer had not expected. It was the druid’s presence that had caused the strongest outburst of those strange sensations it should not be able to feel. His face had flickered in its memories in a thousand images, shattering and reforming as its brain attempted to reconstruct connections of meaning between the flashes of memory, and it had felt its digestive system churn. His voice had caused an unnatural itching in its auditory canals, summoning recollections of feeling the sound vibrate against a back of a different shape. The mind flayer knew that seeing it must have caused the elf pain; if it was able to feel as much as it did because of his presence in its prison, the elf surely felt much worse.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhM4EH_LTfCM",
    "outputId": "58d88bd7-5845-49a8-de31-2cc5fd1fe386"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text describes a dark kingdom ruled by Ahriman and his creation '\n",
      " 'of a special mirror.\\n'\n",
      " 'Trope: Evil ruler, magical mirror\\n'\n",
      " 'Narrative arc: Suspense\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Dark, ominous\\n'\n",
      " 'Genre: Fantasy, horror\\n'\n",
      " 'Intertextuality: Mythology, fairy tales\\n'\n",
      " 'Speech standard: Poetic, elevated language\\n'\n",
      " 'Literary form: Description of a place and dialogue between characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Once Upon a Time...\n",
    "\n",
    "\"Deep beneath ruins of a once great kingdom now covered by a wicked forest laid the Kingdom of Eternal Night. The ruler of this dark kingdom was Ahriman-Lord of Lies, Father of Darkness and Deceit, Embodiment of Destruction and Nothingness, God of Evil and Death, and Personification of Chaos!\n",
    "\n",
    "Within his personal workshop; Ahriman was creating something very special and his minions were curious on what it was. Their Master had created a great mirror but this was no ordinary mirror...it can show other worlds and dimensions, and the Dark Lord can communicate with anyone, travel through and bring anyone or anything through the mirror.\"\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNnr4-D1U0w-",
    "outputId": "c1865bb9-a3ef-48dd-bfd4-86ec8e49265b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Diwäia reassures Eilythia about her menstrual cycle, explaining '\n",
      " 'that it is a sign of womanhood.\\n'\n",
      " 'Trope: Menstruation as a symbol of womanhood\\n'\n",
      " 'Narrative arc: Reassurance and comfort\\n'\n",
      " 'Enunciation: Dialogue between Diwäia and Eilythia\\n'\n",
      " 'Tone: Reassuring, comforting\\n'\n",
      " 'Genre: Poetry, possibly coming-of-age or feminist literature\\n'\n",
      " \"Intertextuality: Medical textbook on menstruation or women's health magazine \"\n",
      " 'article']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Diwäia, maid of stars, could only smile,\n",
    "Upon seeing the source of malady\n",
    "That vexed divine and bright Eilythia so.\n",
    "Diwäia spoke in reassuring tones:\n",
    "‘My child, this bloody stain upon your thigh\n",
    "Is naught worthy to be concerned about!\n",
    "So calmly lay your weary mind to rest,\n",
    "Dispel all worry and relax, my child,\n",
    "For every moon a woman’s flower blooms.\n",
    "This lunar flow from deep within your nethers\n",
    "Is neither an imp’s curse nor dark enchantment\n",
    "Cast with malicious mind upon your form,\n",
    "But a blessing, my child, for come one’s blood\n",
    "One’s womanhood begins to bloom in full.\n",
    "Girlhood is but a fleeting thing, you see,\n",
    "Brief and ephemeral, naught but a passing\n",
    "Moment in time, soon swept away like chaff\n",
    "Against the ever-flowing gusts of wind.\n",
    "Worry not, little maid, the bloody flow\n",
    "Will soon abate, and having come to pass,\n",
    "You shall soon flourish into womanhood,\n",
    "Like a blossoming flower within its bloom.’\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNWVUlvNWwUK"
   },
   "source": [
    "### Children's Fiction in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVqr1UyPWvnE",
    "outputId": "dfb3bc8f-dc76-482d-f343-3e7c95bd0d95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Harold is searching for his purple crayon and uses a magic wand to '\n",
      " 'try and find it.\\n'\n",
      " 'Trope: Magic wand\\n'\n",
      " 'Narrative arc: Comic relief\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Playful, whimsical\\n'\n",
      " \"Genre: Children's literature, fantasy\\n\"\n",
      " 'Intertextuality: Spell casting, incantations\\n'\n",
      " 'Speech standard: Informal, childlike language\\n'\n",
      " 'Literary form: Dialogue/conversation between characters (Harold and the '\n",
      " 'magic wand']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Harold slid off the side of his bed, his fall to the ground cushioned by his pillow. He plastered his body against the ground and wriggled and wriggled until he could reach under his bed.\n",
    "\n",
    "He swept his arm this way and that and from out under the bed a telescope and a ball and a magic wand rolled out.\n",
    "\n",
    "But his purple crayon was not under his bed.\n",
    "\n",
    "Harold frowned at the magic wand.\n",
    "\n",
    "It wasn’t his purple crayon but maybe… just maybe…?\n",
    "\n",
    "Harold scrambled to pick up the magic wand and waved it to and fro like a conductor in front of a great hall.\n",
    "\n",
    "“Purple crayon, purple crayon, come to me, oh purple crayon!” he chanted.\n",
    "\n",
    "Harold waited in the silence, eyes and ears open and alert.\n",
    "\n",
    "But his purple crayon did not appear.\n",
    "\n",
    "But a clatter and a bang did.\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkTFTY5fW_2z",
    "outputId": "5bd49c2a-ce40-4da0-a5e3-183f8d2ebd22"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: Bluey Heeler and Chloe Dalmation are setting up a museum together.\\n'\n",
      " 'Trope: Friends working together on a project\\n'\n",
      " 'Narrative arc: Planning and organizing the museum exhibits\\n'\n",
      " 'Enunciation: Dialogue between Bluey Heeler and Chloe Dalmation\\n'\n",
      " 'Tone: Playful, enthusiastic\\n'\n",
      " \"Genre: Children's literature, friendship story\\n\"\n",
      " 'Intertextuality: Museum guide or tour script\\n'\n",
      " 'Speech standard: Informal, conversational language\\n'\n",
      " 'Literary form: Conversation/dialogue between characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Bluey Heeler ran after her classmate and best friend, Chloe Dalmation. “I found some more items for our museum!” she exclaimed, with a handful of pinecones. “These can be the things that people look at when they come into the museum.”\n",
    "“Ah yes! Chloe said. “I’ll put them on display over here.”\n",
    "“And then I can be the one who finds things for the museum and you can be the one who shows museum guests around!” Bluey added.\n",
    "“That sounds great. I’ll be the tour guide!” Chloe said, her tail started to wag.\n",
    "“Yeah, that’s it, the tour guide!” Bluey announced. She looked over at their collection of exhibits and thought for a moment. No one was lining up to take a tour. “Right now I can set up the stuff, and you can find people to come to our museum,” Bluey suggested.\n",
    "“That’s a great idea!” Chloe said. She ran off to look for their classmates so that people would join the game.\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ju_MoG0LXfNk",
    "outputId": "5f668fc1-98de-491a-9c5b-4f92e8fc2e19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: The text is about a young boy named Sunoo who realizes important '\n",
      " 'things about his life and the death of his mother.\\n'\n",
      " 'Trope: Death of a loved one\\n'\n",
      " 'Narrative arc: Reflective and introspective\\n'\n",
      " 'Enunciation: Third-person narrative\\n'\n",
      " 'Tone: Tragic\\n'\n",
      " 'Genre: Literary fiction or coming-of-age novel\\n'\n",
      " 'Intertextuality: Obituary or eulogy speeches, personal diaries or journals, '\n",
      " 'philosophical texts on death and existence\\n'\n",
      " 'Speech standard: Standard literary language with some poetic elements']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "At the age of 6, he found out that his birthday is in June. And when he was 7 years old, he realized that this is not important.\n",
    "\n",
    "Right at the moment when his mother died. In a dark ruin, under a spiral staircase. He understood many things in those minutes. His mother said that he is now the only 7-year-old child on earth. She said she was sorry that she gave birth to him. She said he named him \"Sunoo\". And when he asked what it meant, she answered: \"I don't know.\"\n",
    "\n",
    "His mother died of hunger. Malnutrition that lasted a lifetime. Whatever she found, gave it to Sunoo to eat. Sunoo was eating alone and she was watching.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "final_prompt = prepare_fabrique_prompt(text)\n",
    "\n",
    "prompts = [final_prompt]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "list_generated = []\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    list_generated.append(generated_text)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list_generated)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01d2f2ef89c940c7a55d14ebb033b06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b6b1ec885a649ada802dabfc2495170",
      "max": 801,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c3985927bbe4ba9b6dc1d9db019363d",
      "value": 801
     }
    },
    "03529aeb8445436eb74b415534cb1401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cea110e17f9e460c99fc69d4e54a2022",
       "IPY_MODEL_93640d7c63f14100a612db9fd2b64b0a",
       "IPY_MODEL_e77786870f2b446c9830bbf8777d2b7e"
      ],
      "layout": "IPY_MODEL_269a8a1384044dbda76ac62267c51417"
     }
    },
    "08e636746dc64bd4a9732596630032ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f9890fa952246ee9c7414c6348c38cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4227023eff9f4963a09fae4da20bbf4f",
      "placeholder": "​",
      "style": "IPY_MODEL_349e3a3794a04b9499f9b69c8a67d3f6",
      "value": " 132/132 [00:00&lt;00:00, 10.9kB/s]"
     }
    },
    "148ff16359d2491b92a92a34687ebce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b6b1ec885a649ada802dabfc2495170": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ba8d204e78d454f88db334210721566": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c3985927bbe4ba9b6dc1d9db019363d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20b3f1ac0b0744dcb7a7b2a290637288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d26e46011f4d96ab12552dc3b429d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_493bd0058cf148a4ba6e3370e71eebae",
       "IPY_MODEL_01d2f2ef89c940c7a55d14ebb033b06b",
       "IPY_MODEL_7ca00ab7e54d449b807585f321698592"
      ],
      "layout": "IPY_MODEL_afd4da9d7cff4f0d946066fd4501c3c5"
     }
    },
    "20d6a9f68201444bbb3f0cea3f83e61b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235458f11a374e54ad442b82caec7d74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25956ab5162446d8a844d003fe09ef53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8632926af4944d9389c9773607d98f39",
      "placeholder": "​",
      "style": "IPY_MODEL_679da967f2bd4c58a867e739d90202a2",
      "value": "generation_config.json: 100%"
     }
    },
    "269a8a1384044dbda76ac62267c51417": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "275793a0cef14551976ef2f643d31cc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "349e3a3794a04b9499f9b69c8a67d3f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "367376263a7d474ea4ef4a63b6a80454": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ba8af12b80a476590be557ff8783c03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e96249695494fa58262dca662ab46db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f0660201474476f9a9bd4965beff31f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4227023eff9f4963a09fae4da20bbf4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "459053230cfe48bfb524d3909914272d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "473d15ae81c6483391ce9c4a1a53de72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47e9dd8eccb2462e82f5b5b989255a87",
      "placeholder": "​",
      "style": "IPY_MODEL_08e636746dc64bd4a9732596630032ea",
      "value": " 7.25G/7.25G [00:59&lt;00:00, 142MB/s]"
     }
    },
    "47e9dd8eccb2462e82f5b5b989255a87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "493bd0058cf148a4ba6e3370e71eebae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70058513c62f47508c06e6d5158d4f94",
      "placeholder": "​",
      "style": "IPY_MODEL_c4fe9fc1fd8e44f3a7860e340548457d",
      "value": "config.json: 100%"
     }
    },
    "4c4e3f77747a440cb9d79ffaf6b85c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfbedf3425df40a6aedc926deba2ca97",
      "max": 933,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_367376263a7d474ea4ef4a63b6a80454",
      "value": 933
     }
    },
    "4d3caaf60a64433ca5acd7bf891e7ba3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514795a9181947e5ab767d89e56d11a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58e9bc59efd4428c800586d951341827": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b15ad68b44b4c89b065a2589f59e216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58e9bc59efd4428c800586d951341827",
      "placeholder": "​",
      "style": "IPY_MODEL_20b3f1ac0b0744dcb7a7b2a290637288",
      "value": " 549/549 [00:00&lt;00:00, 45.9kB/s]"
     }
    },
    "5e9bacfc22224970b47351d388599b18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61cd6cffc6484dceb4b90e0139ea3d35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d3caaf60a64433ca5acd7bf891e7ba3",
      "placeholder": "​",
      "style": "IPY_MODEL_1ba8d204e78d454f88db334210721566",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "679da967f2bd4c58a867e739d90202a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e1e9d79d0b249f096550c0fca1f76ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2009653178c4be283d4cc3c37df6bc5",
      "placeholder": "​",
      "style": "IPY_MODEL_a2fcb785f10d49f3bb06c2627596f360",
      "value": " 500k/500k [00:00&lt;00:00, 4.45MB/s]"
     }
    },
    "70058513c62f47508c06e6d5158d4f94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "768d954c86154b1ba282bb9db666bbc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_25956ab5162446d8a844d003fe09ef53",
       "IPY_MODEL_da6641f5e44f4c278aa753f0434c54a3",
       "IPY_MODEL_0f9890fa952246ee9c7414c6348c38cb"
      ],
      "layout": "IPY_MODEL_fe5889afb2ea4aa38b67461b22ceb4e6"
     }
    },
    "7a8bff2f8a014cf7a499389b4718f732": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a93d100eaad4976a97f0ee4781cf955": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ca00ab7e54d449b807585f321698592": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_148ff16359d2491b92a92a34687ebce4",
      "placeholder": "​",
      "style": "IPY_MODEL_ec92e6016573459ea1de88eb3525c0db",
      "value": " 801/801 [00:00&lt;00:00, 64.8kB/s]"
     }
    },
    "842ed628cc334f7c977ceb0b4ad27ecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8632926af4944d9389c9773607d98f39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "889f82c8b46e4d879da51a464790f1cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c87aafb6d344c859cb165fe40e0a47f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93640d7c63f14100a612db9fd2b64b0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6a9f68201444bbb3f0cea3f83e61b",
      "max": 1842764,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c87aafb6d344c859cb165fe40e0a47f",
      "value": 1842764
     }
    },
    "95f2ce0bb9cb4b68a932ac4aa02fc9e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b252255139c4ad1b9aa2ec28c5070b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2fcb785f10d49f3bb06c2627596f360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a68ee7b19d9842da8bf0d6d905d4fe58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a8bff2f8a014cf7a499389b4718f732",
      "placeholder": "​",
      "style": "IPY_MODEL_275793a0cef14551976ef2f643d31cc0",
      "value": " 933/933 [00:00&lt;00:00, 77.8kB/s]"
     }
    },
    "a72b77925f6d42508cb9cb360568ea48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4ed1890b93347e6b435dd1911885170",
       "IPY_MODEL_b905af95cedb4996a14c7694a036c7bb",
       "IPY_MODEL_6e1e9d79d0b249f096550c0fca1f76ca"
      ],
      "layout": "IPY_MODEL_514795a9181947e5ab767d89e56d11a9"
     }
    },
    "a7311ce159094306aea90f57336a8c9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afd4da9d7cff4f0d946066fd4501c3c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b905af95cedb4996a14c7694a036c7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9d0787b19d34d15a7ed4bf4eabc4bcc",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce8f12c0e28e4b269487f1e6a0ef333c",
      "value": 499723
     }
    },
    "badc34400a2347c5b0a5227b63185a54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c247445e26d44f61ac2c0ca907c31038": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c49908d9337643e382ce331c6141de60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_889f82c8b46e4d879da51a464790f1cd",
      "max": 7248185990,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b252255139c4ad1b9aa2ec28c5070b7",
      "value": 7248185990
     }
    },
    "c4fe9fc1fd8e44f3a7860e340548457d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c9d0787b19d34d15a7ed4bf4eabc4bcc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd7c9e6816a04a2fb2944d053b5e2b99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce8f12c0e28e4b269487f1e6a0ef333c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cea110e17f9e460c99fc69d4e54a2022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a93d100eaad4976a97f0ee4781cf955",
      "placeholder": "​",
      "style": "IPY_MODEL_d4b007207038422587ee96e7fb460013",
      "value": "tokenizer.json: 100%"
     }
    },
    "cfbedf3425df40a6aedc926deba2ca97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2009653178c4be283d4cc3c37df6bc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2cf8641a2ce4e3298ce4c7fcfc97a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ba8af12b80a476590be557ff8783c03",
      "max": 549,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3bbc8e2a0904c2d803c2ac59f92e1c8",
      "value": 549
     }
    },
    "d4b007207038422587ee96e7fb460013": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9b2223b79b143ed828c3da933370550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd751cd8dc8e431b8e9ef6e2ee0a241a",
       "IPY_MODEL_d2cf8641a2ce4e3298ce4c7fcfc97a78",
       "IPY_MODEL_5b15ad68b44b4c89b065a2589f59e216"
      ],
      "layout": "IPY_MODEL_a7311ce159094306aea90f57336a8c9a"
     }
    },
    "da6641f5e44f4c278aa753f0434c54a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2746429ab0d49d9bcb556334f2ade4d",
      "max": 132,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_842ed628cc334f7c977ceb0b4ad27ecc",
      "value": 132
     }
    },
    "dac05e5586994e389083b31de9ae5aa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e88610e81abe42d3b1fc00f4fe6038f1",
       "IPY_MODEL_c49908d9337643e382ce331c6141de60",
       "IPY_MODEL_473d15ae81c6483391ce9c4a1a53de72"
      ],
      "layout": "IPY_MODEL_235458f11a374e54ad442b82caec7d74"
     }
    },
    "dd751cd8dc8e431b8e9ef6e2ee0a241a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95f2ce0bb9cb4b68a932ac4aa02fc9e7",
      "placeholder": "​",
      "style": "IPY_MODEL_5e9bacfc22224970b47351d388599b18",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "e77786870f2b446c9830bbf8777d2b7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_badc34400a2347c5b0a5227b63185a54",
      "placeholder": "​",
      "style": "IPY_MODEL_3f0660201474476f9a9bd4965beff31f",
      "value": " 1.84M/1.84M [00:00&lt;00:00, 5.08MB/s]"
     }
    },
    "e82b7047aae8441f994123a88a1b0965": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e88610e81abe42d3b1fc00f4fe6038f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd7c9e6816a04a2fb2944d053b5e2b99",
      "placeholder": "​",
      "style": "IPY_MODEL_3e96249695494fa58262dca662ab46db",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "ec92e6016573459ea1de88eb3525c0db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1d1ac31410e437b922455ecd555936e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61cd6cffc6484dceb4b90e0139ea3d35",
       "IPY_MODEL_4c4e3f77747a440cb9d79ffaf6b85c15",
       "IPY_MODEL_a68ee7b19d9842da8bf0d6d905d4fe58"
      ],
      "layout": "IPY_MODEL_459053230cfe48bfb524d3909914272d"
     }
    },
    "f2746429ab0d49d9bcb556334f2ade4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3bbc8e2a0904c2d803c2ac59f92e1c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4ed1890b93347e6b435dd1911885170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c247445e26d44f61ac2c0ca907c31038",
      "placeholder": "​",
      "style": "IPY_MODEL_e82b7047aae8441f994123a88a1b0965",
      "value": "tokenizer.model: 100%"
     }
    },
    "fe5889afb2ea4aa38b67461b22ceb4e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
