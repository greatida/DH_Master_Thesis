{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9e357d",
   "metadata": {},
   "source": [
    "# Calculate BERTscore of 50 excerpts and their summaries\n",
    "\n",
    "This notebook is for descriptive analysis of thesis \"narrative analysis with large language models\".\n",
    "\n",
    "Bertscore helps to calculate the similarity score between two texts.\n",
    "\n",
    "Codes reference: https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964#:~:text=BertScore%20is%20a%20method%20used,gram%2Dbased%20metrics%20often%20encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9752222",
   "metadata": {},
   "source": [
    "## Install BERTscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3017e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                full_text instruction_id  \\\n",
      "count                                                8226           8226   \n",
      "unique                                               8226           8226   \n",
      "top     It's a complicated story, old man,\" began Matv...  1944_123775_1   \n",
      "freq                                                    1              1   \n",
      "\n",
      "                                                 analysis  \n",
      "count                                                8226  \n",
      "unique                                               8226  \n",
      "top     Summary: The story is about an old widow and h...  \n",
      "freq                                                    1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8226 entries, 0 to 8225\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   full_text       8226 non-null   object\n",
      " 1   instruction_id  8226 non-null   object\n",
      " 2   analysis        8226 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 192.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Data loading\n",
    "data = pd.read_csv('C:/Users/idaid/Desktop/Brahe_Novels.csv')\n",
    "\n",
    "# Descriptive stats\n",
    "print(data.describe())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448ed244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: bert-score in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (0.3.13)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from bert-score) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\anaconda3\\lib\\site-packages (from bert-score) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from bert-score) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from bert-score) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\anaconda3\\lib\\site-packages (from bert-score) (21.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from bert-score) (4.25.1)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from bert-score) (3.5.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from bert-score) (2.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging>=20.9->bert-score) (3.0.9)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: sympy in c:\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.0.0->bert-score) (4.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.0.0->bert-score) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert-score) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert-score) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from transformers>=3.0.0->bert-score) (0.21.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from transformers>=3.0.0->bert-score) (0.13.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->bert-score) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib->bert-score) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib->bert-score) (4.25.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->bert-score) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda3\\lib\\site-packages (from requests->bert-score) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->bert-score) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->bert-score) (2022.9.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99d5ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: http://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: transformers in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (4.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\idaid\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea482a8",
   "metadata": {},
   "source": [
    "The following is an example at a basic level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d7a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.6118, Recall: 0.3902, F1: 0.4765\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "# Example texts\n",
    "reference = \"It's a complicated story, old man,began Matvey Savitch, and if I were to tell you all just as it happened, it would take all night and more. Ten years ago in a little house in our street, next door to me, where now there's a tallow and oil factory, there was living an old widow, Marfa Semyonovna Kapluntsev, and she had two sons: one was a guard on the railway, but the other, Vasya, who was just my own age, lived at home with his mother. Old Kapluntsev had kept five pair of horses and sent carriers all over the town; his widow had not given up the business, but managed the carriers as well as her husband had done, so that some days they would bring in as much as five roubles from their rounds. The young fellow, too, made a trifle on his own account. He used to breed fancy pigeons and sell them to fanciers; at times he would stand for hours on the roof, waving a broom in the air and whistling; his pigeons were right up in the clouds, but it wasn't enough for him, and he'd want them to go higher yet. Siskins and starlings, too, he used to catch, and he made cages for sale. All trifles, but, mind you, he'd pick up some ten roubles a month over such trifles. Well, as time went on, the old lady lost the use of her legs and took to her bed.\"\n",
    "candidate = \"The story is about an old widow and her sons, particularly one son named Vasya who breeds pigeons and sells them for extra money..\"\n",
    "# BERTScore calculation\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "P, R, F1 = scorer.score([candidate], [reference])\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bcb43",
   "metadata": {},
   "source": [
    "Another example is a level intermediate. \n",
    "\n",
    "Note: The similarity scores of my thesis will be calculated in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423262fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7079\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the required libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 3: Define the two texts to compare\n",
    "#Passage E56\n",
    "text1= \"It was an age of valetudinarians, in many instances of imaginary ones ; but below its various crazes concerning health and disease, largely multiplied a few years after the time of which I am speaking by the miseries of a great pestilence, lay a valuable, because partly practicable, belief that all the maladies of the soit might be reached through the subtle gateways of the body. Salis salvation for the Romans, had come to mean bodily sanity ; and the religion of the god of VOL. I. D 34 MARIUS THE EPICUREAN. bodily health Salvator, as they called him, absolutely had a chance just then of becoming the one religion; that mild and philanthropic son of Apollo surviving, or absorbing, all other pagan godhead. The apparatus of the medical art, the salutary mineral or herb, diet or abstinence, and all the varieties of the bath, came to have a kind of sacramental character ; so deep was the feeling, in more serious minds, of a moral or spiritual profit in physical health, beyond the obvious bodily advantages one had of it ; the body becoming truly, in that case, but a quiet handmaid of the soul. The priesthood or family of Aesculapius, a vast college, believed to be in possession of certain precious medical secrets, came nearest perhaps, of all the institutions of the pagan world, to the Christian priesthood; the temples of the god, rich, in some instances, with the accumulated thank-offerings of centuries of a tasteful devotion, being really also a kind of hospitals for the sick, administered in a full conviction of the religiousness, the refined and sacred happiness, of a life spent in the relieving of pain. Elements of a really experimental and progressive knowledge there were doubtless amid this devout enthusiasm, bent so faithfully on the reception of health as a direct gift from God ; but for the most ]part his care was held to take effect through a machinery easily capable of misuse for purposes of religious fraud. It Alas above all through dreams, inspired by Aesculapius himself, that information as to the cause and cure of a malady was held to come Marius THE Epicurean. 35 to the sufferer, in a belief based on the truth that dreams do sometimes, for those who watch them carefully', give many hints concerning the conditions of the body those latent weak points at Which disease or death may most easily break into it. In the time of Marcus Aurelius these medical dreams had become more than ever a fashionable caprice. Aristeides, the Orator, a man of undoubted intellectual power, has devoted six discourses to their interpretation ; the really scientific Galen has recorded how beneficently they had intervened in his own case, at certain turning-points of life ; and a belief in them was one of the frailties of the wise emperor himself.\"\n",
    "text2= \"The text discusses the belief in the connection between physical health and spiritual well-being during a time of great pestilence.\"\n",
    "\n",
    "# Step 4: Prepare the texts for BERT\n",
    "inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Step 5: Feed the texts to the BERT model\n",
    "outputs1 = model(**inputs1)\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "# Step 6: Obtain the representation vectors\n",
    "embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Step 7: Calculate cosine similarity\n",
    "similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "\n",
    "# Step 8: Print the result\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021eeb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.5951\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the required libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#passage E164\n",
    "text3= \"Could I wander no farther than my legs could carry me; and were my rambles confined to so much as might be accomplished between sun and sun ? It was my own voluntary choice, that brought me back each successive evening, to the house in which I had resided ever since my escape from Ireland. I might emancipate myself from this restriction whenever I pleased. I might contrive the scheme of a secret elopement. But, if I desired to use my freedom with this additional enlargement, would elopement be the wisest way ot accomplishing that? Might I not form a project of departure and absence, to which it should not be difficult to obtain my uncle's consent ? When I thought of absenting myself for a time from this scene of my early years, the first suggestion that offered to me was that of paying a visit to my beloved sister. How much further the mandeville. 193 genius of romance, when I had put myself under his guidance, might conduct me, I could not tell. But, if I were enabled to execute any part of the project that now rose to my thoughts, I determined that the first stage in my journey kings, the first branch of the inheritance of pleasure I proposed to myself, should be, once again to embrace my dear Henrietta. 1 endeavoured in another way to anticipate the events of my future history. I was left to my own devices. No one of the houshold had the presumption to talk to me of my future destination ; and the silence and reserve of my own nature prevented me from inviting them to enter on the topic. But was my education ended ? I was not so ignorant of the rules of political society, as not to know that ten years more must elapse, before I should be acknowledged by the laws of my country as my own master. How was this period to be filled up ? Sh jitld I receive no VOL I. I 194 MANDEVILLE. more instruction in learning ? Would some reverend divine, hitherto a stranger, be introduced as the successor of Hilkiah, to superintend my studies, and keep alive the devotions of Mandeville House ? This was a very anxious question to me. If .the authority and the magisterial rebukes of Mr Bradford, familiarised as I had been to them from my earliest years, had proved an intolerable torment, vi'ith what patience could I think of being subjected upon the same terms to an entire stranger ?\"\n",
    "text4= \"The protagonist considers their options for their future and contemplates visiting their sister.\"\n",
    "# Step 4: Prepare the texts for BERT\n",
    "inputs3 = tokenizer(text3, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs4 = tokenizer(text4, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Step 5: Feed the texts to the BERT model\n",
    "outputs3 = model(**inputs3)\n",
    "outputs4 = model(**inputs4)\n",
    "\n",
    "# Step 6: Obtain the representation vectors\n",
    "embeddings3 = outputs3.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings4 = outputs4.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Step 7: Calculate cosine similarity\n",
    "similarity = np.dot(embeddings3, embeddings4.T) / (np.linalg.norm(embeddings3) * np.linalg.norm(embeddings4))\n",
    "\n",
    "# Step 8: Print the result\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b16e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6685\n"
     ]
    }
   ],
   "source": [
    "#passage E278\n",
    "text5= \"It is a passive feeling capable of being roused for any object, as the vine can grow on any tree; and the wisdom of the world recognises its strength when it urges a girl to marry the man who wants her with the assurance that love will follow. It is an emotion made up of the satisfaction in security, pride of property, the pleasure of being desired, the gratification of a household, and it is only by an amiable vanity that women ascribe to it spiritual value. It is an emotion which is defenceless against passion. I suspected that Blanche Stroeve's violent dislike of Strickland had in it from the beginning a vague element of sexual attraction. Who am I that I should seek to unravel the mysterious intricacies of sex? Perhaps Stroeve's passion excited without satisfying that part of her nature, and she hated Strickland because she felt in him the power to give her what she needed. I think she was quite sincere when she struggled against her husband's desire to bring him into the studio; I think she was frightened of him, though she knew not why; and I remembered how she had foreseen disaster. I think in some curious way the horror which she felt for him was a transference of the horror which she felt for herself because he so strangely troubled her. His appearance was wild and uncouth; there was aloofness in his eyes and sensuality in his mouth; he was big and strong; he gave the impression of untamed passion; and perhaps she felt in him, too, that sinister element which had made me think of those wild beings of the world's early history when matter, retaining its early connection with the earth, seemed to possess yet a spirit of its own. If he affected her at all, it was inevitable that she should love or hate him. She hated him. And then I fancy that the daily intimacy with the sick man moved her strangely. She raised his head to give him food, and it was heavy against her hand; when she had fed him she wiped his sensual mouth and his red beard. She washed his limbs; they were covered with thick hair; and when she dried his hands, even in his weakness they were strong and sinewy. His fingers were long; they were the capable, fashioning fingers of the artist; and I know not what troubling thoughts they excited in her. He slept very quietly, without a movement, so that he might have been dead, and he was like some wild creature of the woods, resting after a long chase; and she wondered what fancies passed through his dreams. Did he dream of the nymph flying through the woods of Greece with the satyr in hot pursuit?\"\n",
    "text6= \"The passage explores the emotions and attraction between characters in the context of love and desire.\"\n",
    "\n",
    "inputs5 = tokenizer(text5, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs6 = tokenizer(text6, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs5 = model(**inputs5)\n",
    "outputs6 = model(**inputs6)\n",
    "\n",
    "embeddings5 = outputs5.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings6 = outputs6.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings5, embeddings6.T) / (np.linalg.norm(embeddings5) * np.linalg.norm(embeddings6))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a551802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.8089\n"
     ]
    }
   ],
   "source": [
    "#passage E2\n",
    "text7 = \"It's a complicated story, old man,began Matvey Savitch, and if I were to tell you all just as it happened, it would take all night and more. Ten years ago in a little house in our street, next door to me, where now there's a tallow and oil factory, there was living an old widow, Marfa Semyonovna Kapluntsev, and she had two sons: one was a guard on the railway, but the other, Vasya, who was just my own age, lived at home with his mother. Old Kapluntsev had kept five pair of horses and sent carriers all over the town; his widow had not given up the business, but managed the carriers as well as her husband had done, so that some days they would bring in as much as five roubles from their rounds. The young fellow, too, made a trifle on his own account. He used to breed fancy pigeons and sell them to fanciers; at times he would stand for hours on the roof, waving a broom in the air and whistling; his pigeons were right up in the clouds, but it wasn't enough for him, and he'd want them to go higher yet. Siskins and starlings, too, he used to catch, and he made cages for sale. All trifles, but, mind you, he'd pick up some ten roubles a month over such trifles. Well, as time went on, the old lady lost the use of her legs and took to her bed.\"\n",
    "text8 = \"The story is about an old widow and her sons, particularly one son named Vasya who breeds pigeons and sells them for extra money..\"\n",
    "\n",
    "inputs7 = tokenizer(text7, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs8 = tokenizer(text8, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs7 = model(**inputs7)\n",
    "outputs8 = model(**inputs8)\n",
    "\n",
    "embeddings7 = outputs7.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings8 = outputs8.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings7, embeddings8.T) / (np.linalg.norm(embeddings7) * np.linalg.norm(embeddings8))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "849909bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.8052\n"
     ]
    }
   ],
   "source": [
    "#passage E7\n",
    "text9 = \"Of course he would come at his own odd hours, often just as one was dressing to go out and dine, and I can even remember finding him there when I returned, for I had long since given him a key of the flat. It was the inhospitable month of February, and I can recall more than one cosy evening when we discussed anything and everything but our own malpractices; indeed, there were none to discuss just then. Raffles, on the contrary, was showing himself with some industry in the most respectable society, and by his advice I used the club more than ever. There is nothing like it at this time of year,said he. In the summer I have my cricket to provide me with decent employment in the sight of men. Keep yourself before the public from morning to night, and they'll never think of you in the still small hours.\"\" Our behavior, in fine, had so long been irreproachable that I rose without misgiving on the morning of Lord Thornaby's dinner to the other Criminologists and guests. My chief anxiety was to arrive under the å¿™gis of my brilliant friend, and I had begged him to pick me up on his way; but at five minutes to the appointed hour there was no sign of Raffles or his cab. We were bidden at a quarter to eight for eight o'clock, so after all I had to hurry off alone.\"\n",
    "text10 = \"The narrator recalls a time when their friend Raffles would often visit their apartment at odd hours. They discuss various topics and Raffles advises the narrator to be seen in public to avoid suspicion. On the morning of a dinner, the narrator waits for Raffles, but he is late and the narrator ends up going alone.\"\n",
    "\n",
    "inputs9 = tokenizer(text9, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs10 = tokenizer(text10, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs9 = model(**inputs9)\n",
    "outputs10 = model(**inputs10)\n",
    "\n",
    "embeddings9 = outputs9.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings10 = outputs10.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings9, embeddings10.T) / (np.linalg.norm(embeddings9) * np.linalg.norm(embeddings10))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4de95b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7298\n"
     ]
    }
   ],
   "source": [
    "#passage E15\n",
    "text11 = \"Three plays have been accepted, and two more are commissioned. Oh! my dear, I am lost, all is darkness around me. I would set fire to the house in a moment if that would bring light. What does it all mean? Is he ashamed of taking money from me? He is too high-minded for so trumpery a matter to weigh with him. Besides, scruples of the kind could only be the outcome of some love affair. A man would take anything from his wife, but from the woman he has ceased to care for, or is thinking of deserting, it is different. If he needs such large sums, it must be to spend them on a woman. For himself, why should he hesitate to draw from my purse? Our savings amount to one hundred thousand francs! In short, my sweetheart, I have explored a whole continent of possibilities, and after carefully weighing all the evidence, am convinced I have a rival. I am deserted for whom? At all costs I must see the unknown. July 10th. Light has come, and it is all over with me. Yes, Renee, at the age of thirty, in the perfection of my beauty, with all the resources of a ready wit and the seductive charms of dress at my command, I am betrayed and for whom? A large-boned Englishwoman, with big feet and thick waist a regular British cow! There is no longer room for doubt. I will tell you the history of the last few days. Worn out with suspicions, which were fed by Gaston's guilty silence (for, if he had helped a friend, why keep it a secret from me?), his insatiable desire for money, and his frequent journeys to Paris; jealous too of the work from which he seemed unable to tear himself, I at last made up my mind to take certain steps, of such a degrading nature that I cannot tell you about them. Suffice it to say that three days ago I ascertained that Gaston, when in Paris, visits a house in the Rue de la Ville l'Eveque, where he guards his mistress with jealous mystery, unexampled in Paris. The porter was surly, and I could get little out of him, but that little was enough to put an end to any lingering hope, and with hope to life. On this point my mind was resolved, and I only waited to learn the whole truth first. With this object I went to Paris and took rooms in a house exactly opposite the one which Gaston visits. Thence I saw him with my own eyes enter the courtyard on horseback. Too soon a ghastly fact forced itself on me. This Englishwoman, who seems to me about thirty-six, is known as Mme. Gaston. This discovery was my deathblow. I saw him next walking to the Tuileries with a couple of children. Oh! my dear, two children, the living images of Gaston!\"\n",
    "text12 = \"The narrator suspects her partner of infidelity and sets out to gather evidence, eventually discovering that he has a wife and children with another woman.\"\n",
    "\n",
    "inputs11 = tokenizer(text11, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs12 = tokenizer(text12, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs11 = model(**inputs11)\n",
    "outputs12 = model(**inputs12)\n",
    "\n",
    "embeddings11 = outputs11.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings12 = outputs12.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings11, embeddings12.T) / (np.linalg.norm(embeddings11) * np.linalg.norm(embeddings12))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40e0cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7478\n"
     ]
    }
   ],
   "source": [
    "#passage E3650\n",
    "text13 = \"If you fellows take down what I say from time to time in note-books, as you ought to do, you'll remember that I offered to give anyone odds that Kay's would out us in the final. I always said that a really hot man like Fenn was more good to a side than half-a-dozen ordinary men. He can do all the bowling and all the batting. All the fielding, too, in the slips.\"\" Tea was just over at Blackburn's, and the bulk of the house had gone across to preparation in the school buildings. The prefects, as was their custom, lingered on to finish the meal at their leisure. These after-tea conversations were quite an institution at Blackburn's. The labours of the day were over, and the time for preparation for the morrow had not yet come. It would be time to be thinking of that in another hour. Meanwhile, a little relaxation might be enjoyed. Especially so as this was the last day but two of the summer term, and all necessity for working after tea had ceased with the arrival of the last lap of the examinations. Silver was head of the house, and captain of its cricket team, which was nearing the end of its last match, the final for the inter-house cup, and on paper getting decidedly the worst of it. After riding in triumph over the School House, Bedell's, and Mulholland's, Blackburn's had met its next door neighbour, Kay's, in the final, and, to the surprise of the great majority of the school, was showing up badly. The match was affording one more example of how a team of average merit all through may sometimes fall before a one-man side. Blackburn's had the three last men on the list of the first eleven, Silver, Kennedy, and Challis, and at least nine of its representatives had the reputation of being able to knock up a useful twenty or thirty at any time. Kay's, on the other hand, had one man, Fenn. After him the tail started. But Fenn was such an exceptional all-round man that, as Silver had said, he was as good as half-a-dozen of the Blackburn's team, equally formidable whether batting or bowling he headed the school averages at both. He was one of those batsmen who seem to know exactly what sort of ball you are going to bowl before it leaves your hand, and he could hit like another Jessop. As for his bowling, he bowled left hand always a puzzling eccentricity to an undeveloped batsman and could send them down very fast or very slow, as he thought best, and it was hard to see which particular brand he was going to serve up before it was actually in mid-air. But it is not necessary to enlarge on his abilities. The figures against his name in Wisden prove a good deal.\"\n",
    "text14 = \"The narrator discusses a cricket match between Blackburn's and Kay's, focusing on the exceptional skills of Fenn from Kay's.\"\n",
    "\n",
    "inputs13 = tokenizer(text13, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs14 = tokenizer(text14, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs13 = model(**inputs13)\n",
    "outputs14 = model(**inputs14)\n",
    "\n",
    "embeddings13 = outputs13.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings14 = outputs14.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings13, embeddings14.T) / (np.linalg.norm(embeddings13) * np.linalg.norm(embeddings14))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6534fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6866\n"
     ]
    }
   ],
   "source": [
    "#passage E151\n",
    "text15 = \"She played with great spirit, but appeared so diminutive on the stage, that a gentleman whimsically said,  he could hear her very well, but he could riot see lier without a glass. In her sixteenth year, she appeared at the same theatre, as Juliet, and met with so much applause, that she was immediately engaged, at a good salary, by the managers of Covent Garden, at which theatre she continued, with the exception of one winter passed in Liverpool, for the next twenty years. Besides sustaining a respectable line in tragedy, Mrs. Mattocks was the Rosetta, Polly, &c., of the theatre ;  but she has latterly,says a critic, writing in 1800,  devoted herself entirely to the comic muse, whose cause she supports with admirable spirit, and with a peculiarity of humour, which, though it may sometimes exceed the precise limitations of critical propriety, is richly comic, and perfectly original. She died on the 26th of June, 1826. VVEWITZER, (RALPH,) was born, about the year 1748, in London, where lie carried on, for some time, the business of a jeweller. His sister was a favourite actress and singer, and for her benefit, he made his first appearance on any stage, at Covent Garden, as Ralph, in The Maid of the Mill. The low comic humour which lie displayed in this part, induced the manager to engage him, and he soon established his reputation by his whimsical, but just, representation of Jews and Frenchmen. He subsequently performed at Dublin; and in 1789, undertook the management of the Royalty Theatre, on the failure of which concern, he appeared at Drury Lane. He also played, during the summer, for several seasons, at the Haymarket, and partly invented some new pantomimes. Wewitzer was the original Jew in The Young Quaker, and by his performance of it, contributed much to the success of the piece. During the latter part of his life, which terminated in 1824, he was a pensioner on the Theatrical Fund. He was the author of a jest book, entitled, The School of Wit, and was himself remarkable for many witty sayings. MURRAY, (CHARLES) was born at Cheshunt. in Hertfordshire, in 1754. His father, Sir John Murray, of Brough-ton, acted as secretary to the Pretender, and was arraigned for high treason for his share in the rebellion, but APPENDIX, wards received a pardon. The subject of our memoir was educated in France, and, on his return to London, was apprenticed to a surgeon ; but almost immediately quitted his profession for the stage, making his debut at York, in April, 1775, under the assumed name of Raymur. He subsequently performed at various provincial towns, and was a great favourite at Norwich and Bath. In 1796, he made his debut at Covent Garden, in the part of Shylock, and was well received, but never became a first-rate actor. During the latter part of his life, he was manager of the Edinburgh Theatre, and died in that city on the 8th of November, 1821.\"\n",
    "text16 = \"The text provides brief biographical information about three actors, Mrs. Mattocks, Ralph Wewitzer, and Charles Murray.\"\n",
    "\n",
    "inputs15 = tokenizer(text15, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs16 = tokenizer(text16, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs15 = model(**inputs15)\n",
    "outputs16 = model(**inputs16)\n",
    "\n",
    "embeddings15 = outputs15.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings16 = outputs16.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings15, embeddings16.T) / (np.linalg.norm(embeddings15) * np.linalg.norm(embeddings16))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2a156aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7350\n"
     ]
    }
   ],
   "source": [
    "#passage E316\n",
    "text17 = \"Count Varras was slain, together with three hundred of the Spanish infantry. Six hundred prisoners were taken, and thirty-eight colours fell into the victor's hands. The success was gained entirely by the eight hundred allied horse, the infantry never arriving upon the field. The brilliant little victory, which was one of the first gained by the allies in the open field, was the cause of great rejoicings. Not only were the Spaniards no longer invincible, but they had been routed by a force but one-sixth of their own number, and the battle showed how greatly the individual prowess of the two peoples had changed during the progress of the war. The Archduke Ernest had died in 1595, and had been succeeded by the Archduke Albert in the government of the Netherlands. He had with him no generals comparable with Parma, or even with Alva. His troops had lost their faith in themselves and their contempt for their foes. Holland was grown rich and prosperous, while the enormous expenses of carrying on the war both in the Netherlands and in France, together with the loss of the Armada, the destruction of the great fleet at Ferrol, and the capture of Cadiz and the ships there, had exhausted the resources of Spain, and Philip was driven to make advances for peace to France and England. Henry IV., knowing that peace with Spain meant an end of the civil war that had so long exhausted France, at once accepted the terms of Philip, and made a separate peace, in spite of the remonstrances of the ambassadors of England and Holland, to both of which countries he owed it in no small degree that he had been enabled to support himself against the faction of the Guises backed by the power of Spain. A fresh treaty was made between England and the Netherlands, Sir Francis Vere being sent out as special ambassador to negotiate. England was anxious for peace, but would not desert the Netherlands if they on their part would relieve her to some extent of the heavy expenses caused by the war. This the States consented to do, and the treaty was duly signed on both sides. A few days before its conclusion Lord Burleigh, who had been Queen Elizabeth's chief adviser for forty years, died, and within a month of its signature Philip of Spain, whose schemes he had so long opposed, followed him to the grave. On the 6th of the previous May Philip had formally ceded the Netherlands to his daughter Isabella, between whom and the Archduke Albert a marriage had been arranged. This took place on the 18th of April following, shortly after his death. It was celebrated at Valencia, and at the same time King Philip III. was united to Margaret of Austria.\"\n",
    "text18 = \"The text discusses a battle in which Count Varras is killed and the allies are victorious over the Spanish infantry.\"\n",
    "\n",
    "inputs17 = tokenizer(text17, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs18 = tokenizer(text18, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs17 = model(**inputs17)\n",
    "outputs18 = model(**inputs18)\n",
    "\n",
    "embeddings17 = outputs17.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings18 = outputs18.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings17, embeddings18.T) / (np.linalg.norm(embeddings17) * np.linalg.norm(embeddings18))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d8b148c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7213\n"
     ]
    }
   ],
   "source": [
    "#passage E592\n",
    "text19 = \"When I entered the place a deadly sickness seized me, weak as I was, brought on by the horrible sounds and smells, and the sights that I saw in the flare of the lanterns which my conductors carried, for the hold was shut off from light and air. But they dragged me along and presently I found myself chained in the midst of a line of black men and women, many feet resting in the bilge water. There the Spaniards left me with a jeer, saying that this was too good a bed for an Englishman to lie on. For a while I endured, then sleep or insensibility came to my succour, and I sank into oblivion, and so I must have remained for a day and a night. When I awoke it was to find the Spaniard to whom I had been sold or given, standing near me with a lantern and directing the removal of the fetters from a woman who was chained next to me. She was dead, and in the light of the lantern I could see that she had been carried off by some horrible disease that was new to me, but which I afterwards learned to know by the name of the Black Vomit. Nor was she the only one, for I counted twenty dead who were dragged out in succession, and I could see that many more were sick. Also I saw that the Spaniards were not a little frightened, for they could make nothing of this sickness, and strove to lessen it by cleansing the hold and letting air into it by the removal of some planks in the deck above. Had they not done this I believe that every soul of us must have perished, and I set down my own escape from the sickness to the fact that the largest opening in the deck was made directly above my head, so that by standing up, which my chains allowed me to do, I could breathe air that was almost pure. Having distributed water and meal cakes, the Spaniards went away. I drank greedily of the water, but the cakes I could not eat, for they were mouldy. The sights and sounds around me were so awful that I will not try to write of them. And all the while we sweltered in the terrible heat, for the sun pierced through the deck planking of the vessel, and I could feel by her lack of motion that we were becalmed and drifting. I stood up, and by resting my heels upon a rib of the ship and my back against her side, I found myself in a position whence I could see the feet of the passers-by on the deck above. Presently I saw that one of these wore a priest robe, and guessing that he must be my companion with whom I had escaped, I strove to attract his notice, and at length succeeded.\"\n",
    "text20= \"The narrator describes their experience being captured and chained on a slave ship, witnessing the sickness and death of the other captives.\"\n",
    "\n",
    "inputs19 = tokenizer(text19, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs20 = tokenizer(text20, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs19 = model(**inputs19)\n",
    "outputs20 = model(**inputs20)\n",
    "\n",
    "embeddings19 = outputs19.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings20 = outputs20.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings19, embeddings20.T) / (np.linalg.norm(embeddings19) * np.linalg.norm(embeddings20))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce51536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6553\n"
     ]
    }
   ],
   "source": [
    "#passage E733\n",
    "text21 = \"They went about the town all day and yet in such a quiet peculiar way that you couldn't have realized that they were working at all. They ate their dinner together at Smith's cafe and took an hour and a half over it to throw people off the scent. Then when they got them off it, they sat and talked with Josh Smith in the back bar to keep them off. Mr. Smith seemed to take to them right away. They were men of his own size, or near it, and anyway hotel men and detectives have a general affinity and share in the same impenetrable silence and in their confidential knowledge of the weaknesses of the public. Mr. Smith, too, was of great use to the detectives. Boys, he said, I wouldn't ask too close as to what folks was out late at night: in this town it don't do. When those two great brains finally left for the city on the five-thirty, it was hard to realize that behind each grand, impassible face a perfect vortex of clues was seething. But if the detectives were heroes, what was Pupkin? Imagine him with his bandage on his head standing in front of the bank and talking of the midnight robbery with that peculiar false modesty that only heroes are entitled to use. I don't know whether you have ever been a hero, but for sheer exhilaration there is nothing like it. And for Mr. Pupkin, who had gone through life thinking himself no good, to be suddenly exalted into the class of Napoleon Bonaparte and John Maynard and the Charge of the Light Brigade oh, it was wonderful. Because Pupkin was a brave man now and he knew it and acquired with it all the brave man's modesty. In fact, I believe he was heard to say that he had only done his duty, and that what he did was what any other man would have done: though when somebody else said: That's so, when you come to think of it, Pupkin turned on him that quiet look of the wounded hero, bitterer than words. And if Pupkin had known that all of the afternoon papers in the city reported him dead, he would have felt more luxurious still. That afternoon the Mariposa court sat in enquiry, technically it was summoned in inquest on the dead robber though they hadn't found the body and it was wonderful to see them lining up the witnesses and holding cross-examinations. There is something in the cross-examination of great criminal lawyers like Nivens, of Mariposa, and in the counter examinations of presiding judges like Pepperleigh that thrills you to the core with the astuteness of it. They had Henry Mullins, the manager, on the stand for an hour and a half, and the excitement was so breathless that you could have heard a pin drop. Nivens took him on first. What is your name? he said. Henry August Mullins. What position do you hold?\"\n",
    "text22 = \"Two detectives work undercover in a town, while a man named Pupkin becomes a hero after a bank robbery.\"\n",
    "\n",
    "inputs21 = tokenizer(text21, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs22 = tokenizer(text22, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs21 = model(**inputs21)\n",
    "outputs22 = model(**inputs22)\n",
    "\n",
    "embeddings21 = outputs21.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings22 = outputs22.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings21, embeddings22.T) / (np.linalg.norm(embeddings21) * np.linalg.norm(embeddings22))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8920dd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7004\n"
     ]
    }
   ],
   "source": [
    "#passage E772\n",
    "text23 = \"But never mind! So long as I'm not actually drowned, what does it matter? What shall we do next? asked the boy anxiously. Call someone to help you, was the reply. There is no one on the island but myself, said the boy;  excepting you, he added, as an afterthought. I'm not on it more's the pity! but in it, responded Rinkitink. Are the warriors all gone? Yes, said Inga, and they have taken my father and mother, and all our people, to be their slaves, he added, trying in vain to repress a sob. So so! said Rinkitink softly; and then he paused a moment, as if in thought. Finally he said: There are worse things than slavery, but I never imagined a well could be one of them. Tell me, Inga, could you let down some food to me? I'm nearly starved, and if you could manage to send me down some food I'd be well fed hoo, hoo, heek, keek, eek! well fed. Do you see the joke, Inga? Do not ask me to enjoy a joke just now, Your Majesty, begged Inga in a sad voice; but if you will be patient I will try to find something for you to eat. He ran back to the ruins of the palace and began searching for bits of food with which to satisfy the hunger of the King, when to his surprise he observed the goat, Bilbil, wandering among the marble blocks. What! cried Inga. Didn't the warriors get you, either? If they had, calmly replied Bilbil, I shouldn't be here. But how did you escape? asked the boy. Easily enough. I kept my mouth shut and stayed away from the rascals, said the goat. I knew that the soldiers would not care for a skinny old beast like me, for to the eye of a stranger I seem good for nothing. Had they known I could talk, and that my head contained more wisdom than a hundred of their own noddles, I might not have escaped so easily. Perhaps you are right, said the boy. I suppose they got the old man? carelessly remarked Bilbil. What old man? Rinkitink. Oh, no! His Majesty is at the bottom of the well, said Inga, and I don't know how to get him out again. Then let him stay there, suggested the goat. That would be cruel. I am sure, Bilbil, that you are fond of the good King, your master, and do not mean what you say. Together, let us find some way to save poor King Rinkitink. He is a very jolly companion, and has a heart exceedingly kind and gentle. Oh, well; the old boy isn't so bad, taken altogether, admitted Bilbil, speaking in a more friendly tone. But his bad jokes and fat laughter tire me dreadfully, at times. Prince Inga now ran back to the well, the goat following more leisurely. Here's Bilbil! shouted the boy to the King. The enemy didn't get him, it seems.\"\n",
    "text24 = \"A boy and a talking goat try to figure out how to rescue a king who is trapped at the bottom of a well.\"\n",
    "\n",
    "inputs23 = tokenizer(text23, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs24 = tokenizer(text24, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs23 = model(**inputs23)\n",
    "outputs24 = model(**inputs24)\n",
    "\n",
    "embeddings23 = outputs23.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings24 = outputs24.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings23, embeddings24.T) / (np.linalg.norm(embeddings23) * np.linalg.norm(embeddings24))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d87d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7260\n"
     ]
    }
   ],
   "source": [
    "# passage E799\n",
    "text25 = \"He wanted to walk round the box but his feet would not move and his new clean goloshes had grown to the stone floor, and he could neither lift them nor get his feet out of the goloshes. Then the taper-box was no longer a box but a bed, and suddenly Vasili Andreevich saw himself lying in his bed at home. He was lying in his bed and could not get up. Yet it was necessary for him to get up because Ivan Matveich, the police-officer, would soon call for him and he had to go with him either to bargain for the forest or to put Mukhorty's breeching straight. He asked his wife: 'Nikolaevna, hasn't he come yet?' 'No, he hasn't,' she replied. He heard someone drive up to the front steps. 'It must be him.' 'No, he's gone past.' 'Nikolaevna! I say, Nikolaevna, isn't he here yet?' 'No.' He was still lying on his bed and could not get up, but was always waiting. And this waiting was uncanny and yet joyful. Then suddenly his joy was completed. He whom he was expecting came; not Ivan Matveich the police-officer, but someone else yet it was he whom he had been waiting for. He came and called him; and it was he who had called him and told him to lie down on Nikita.\"\n",
    "text26 = \"The protagonist is trapped and unable to move, but is waiting for someone to arrive.\"\n",
    "\n",
    "inputs25 = tokenizer(text25, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs26 = tokenizer(text26, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs25 = model(**inputs25)\n",
    "outputs26 = model(**inputs26)\n",
    "\n",
    "embeddings25 = outputs25.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings26 = outputs26.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings25, embeddings26.T) / (np.linalg.norm(embeddings25) * np.linalg.norm(embeddings26))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ed941f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7339\n"
     ]
    }
   ],
   "source": [
    "#passage E966\n",
    "text27 = \"I believe, continued Laura steadily, we were all to be in the same party to-night with the Cresslers. But they don't seem to come, and we my sister and my aunt and I don't know what to do. She saw that he was embarrassed, convinced, and the knowledge that she controlled the little situation, that she could command him, restored her all her equanimity. My name is Miss Dearborn, she continued. I believe you know my sister Page. By some trick of manner she managed to convey to him the impression that if he did not know her sister Page, that if for one instant he should deem her to be bold, he would offer a mortal affront. She had not yet forgiven him that stare of suspicion when first their eyes had met; he should pay her for that yet. Miss Page, your sister, Miss Page Dearborn? Certainly I know her, he answered. And you have been waiting, too? What a pity! And he permitted himself the awkwardness of adding: I did not know that you were to be of our party. No, returned Laura upon the instant, I did not know you were to be one of us to-night until Page told me. She accented the pronouns a little, but it was enough for him to know that he had been rebuked. How, he could not just say; and for what it was impossible for him at the moment to determine; and she could see that he began to experience a certain distress, was beating a retreat, was ceding place to her. Who was she, then, this tall and pretty young woman, with the serious, unsmiling face, who was so perfectly at ease, and who hustled him about and made him feel as though he were to blame for the Cresslers' non-appearance; as though it was his fault that she must wait in the draughty vestibule. She had a great air with her; how had he offended her? If he had introduced himself to her, had forced himself upon her, she could not be more lofty, more reserved. I thought perhaps you might telephone, she observed. They haven't a telephone, unfortunately, he answered. Oh! This was quite the last slight, the Cresslers had not a telephone! He was to blame for that, too, it seemed. At his wits' end, he entertained for an instant the notion of dashing out into the street in a search for a messenger boy, who would take a note to Cressler and set him right again; and his agitation was not allayed when Laura, in frigid tones, declared: It seems to me that something might be done. I don't know, he replied helplessly. I guess there's nothing to be done but just wait. They are sure to be along. In the background, Page and Mrs. Wessels had watched the interview, and had guessed that Laura was none too gracious. Always anxious that her sister should make a good impression, the little girl was now in great distress.\"\n",
    "text28 = \"Laura and the person she is speaking to are waiting for the Cresslers to arrive, but they are late. Laura asserts her control over the situation and subtly rebukes the person she is speaking to.\"\n",
    "\n",
    "inputs27 = tokenizer(text27, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs28 = tokenizer(text28, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs27 = model(**inputs27)\n",
    "outputs28 = model(**inputs28)\n",
    "\n",
    "embeddings27 = outputs27.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings28 = outputs28.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings27, embeddings28.T) / (np.linalg.norm(embeddings27) * np.linalg.norm(embeddings28))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a172b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7839\n"
     ]
    }
   ],
   "source": [
    "#passage E1026\n",
    "text29 = \"For that reason, after his arrival, the fire was lit. What are you going to do now, Mr. Holmes? he asked. My friend smiled and laid his hand upon my arm. I think, Watson, that I shall resume that course of tobacco-poisoning which you have so often and so justly condemned, said he. With your permission, gentlemen, we will now return to our cottage, for I am not aware that any new factor is likely to come to our notice here. I will turn the facts over in my mind, Mr. Tregennis, and should anything occur to me I will certainly communicate with you and the vicar. In the meantime I wish you both good-morning. It was not until long after we were back in Poldhu Cottage that Holmes broke his complete and absorbed silence. He sat coiled in his armchair, his haggard and ascetic face hardly visible amid the blue swirl of his tobacco smoke, his black brows drawn down, his forehead contracted, his eyes vacant and far away. Finally he laid down his pipe and sprang to his feet. It won't do, Watson! said he with a laugh. Let us walk along the cliffs together and search for flint arrows. We are more likely to find them than clues to this problem. To let the brain work without sufficient material is like racing an engine. It racks itself to pieces. The sea air, sunshine, and patience, Watson all else will come. Now, let us calmly define our position, Watson, he continued as we skirted the cliffs together. Let us get a firm grip of the very little which we DO know, so that when fresh facts arise we may be ready to fit them into their places. I take it, in the first place, that neither of us is prepared to admit diabolical intrusions into the affairs of men. Let us begin by ruling that entirely out of our minds. Very good. There remain three persons who have been grievously stricken by some conscious or unconscious human agency. That is firm ground. Now, when did this occur? Evidently, assuming his narrative to be true, it was immediately after Mr. Mortimer Tregennis had left the room. That is a very important point. The presumption is that it was within a few minutes afterwards. The cards still lay upon the table. It was already past their usual hour for bed. Yet they had not changed their position or pushed back their chairs. I repeat, then, that the occurrence was immediately after his departure, and not later than eleven o'clock last night. Our next obvious step is to check, so far as we can, the movements of Mortimer Tregennis after he left the room. In this there is no difficulty, and they seem to be above suspicion. Knowing my methods as you do, you were, of course, conscious of the somewhat clumsy water-pot expedient by which I obtained a clearer impress of his foot than might otherwise have been possible.\"\n",
    "text30 = \"After arriving, Mr. Holmes plans to resume smoking, but then suggests they go for a walk to clear their minds. They discuss the case and narrow down the possibilities. They decide to investigate the movements of Mr. Tregennis after he left the room.\"\n",
    "\n",
    "inputs29 = tokenizer(text29, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs30 = tokenizer(text30, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs29 = model(**inputs29)\n",
    "outputs30 = model(**inputs30)\n",
    "\n",
    "embeddings29 = outputs29.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings30 = outputs30.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings29, embeddings30.T) / (np.linalg.norm(embeddings29) * np.linalg.norm(embeddings30))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28dba612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6374\n"
     ]
    }
   ],
   "source": [
    "#passage E1155\n",
    "text31 = \"The assertion that he had hounded Ferdinand Lopez to his death, that by his defence of himself he had brought the man's blood on his head, was made and repeated till those around him did not dare to mention the name of Lopez in his hearing. Even his wife was restrained and became fearful, and in her heart of hearts began almost to wish for that retirement to which he had occasionally alluded as a distant Elysium which he should never be allowed to reach. He was beginning to have the worn look of an old man. His scanty hair was turning grey, and his long thin cheeks longer and thinner. Of what he did when sitting alone in his chamber, either at home or at the Treasury Chamber, she knew less and less from day to day, and she began to think that much of his sorrow arose from the fact that among them they would allow him to do nothing. There was no special subject now which stirred him to eagerness and brought upon herself explanations which were tedious and unintelligible to her, but evidently delightful to him. There were no quints or semi-tenths now, no aspirations for decimal perfection, no delightfully fatiguing hours spent in the manipulation of the multiplication table. And she could not but observe that the old Duke now spoke to her much less frequently of her husband's political position than had been his habit. Through the first year and a half of the present ministerial arrangement he had been constant in his advice to her, and had always, even when things were difficult, been cheery and full of hope. He still came frequently to the house, but did not often see her. And when he did see her he seemed to avoid all allusion either to the political successes or the political reverses of the Coalition. And even her other special allies seemed to labour under unusual restraint with her. Barrington Erle seldom told her any news. Mr. Rattler never had a word for her. Warburton, who had ever been discreet, became almost petrified by discretion. And even Phineas Finn had grown to be solemn, silent, and uncommunicative. Have you heard who is the new Prime Minister? she said to Mrs. Finn one day. Has there been a change? I suppose so. Everything has become so quiet that I cannot imagine that Plantagenet is still in office. Do you know what anybody is doing? The world is going on very smoothly, I take it. I hate smoothness. It always means treachery and danger. I feel sure that there will be a great blow up before long. I smell it in the air. Don't you tremble for your husband? Why should I? He likes being in office because it gives him something to do; but he would never be an idle man. As long as he has a seat in Parliament, I shall be contented.\"\n",
    "text32 = \"The text is a collection of quotes and references to various works and authorities, discussing the Martin Mar-prelate Controversy and challenging Puritan beliefs.\"\n",
    "\n",
    "inputs31 = tokenizer(text31, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "inputs32 = tokenizer(text32, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs31 = model(**inputs31)\n",
    "outputs32 = model(**inputs32)\n",
    "\n",
    "embeddings31 = outputs31.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings32 = outputs32.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings31, embeddings32.T) / (np.linalg.norm(embeddings31) * np.linalg.norm(embeddings32))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf352bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6659\n"
     ]
    }
   ],
   "source": [
    "#passage E1194\n",
    "text33 = \"And there a lovely tracery Of branch and twig that naked tree Of shadows soft and dim has wove, And spread so gently, that above The pure whit^Jmow it seems to float Lighter than that celestial boat, The silver-beaked moon, on air, Lighter than feathery gossamer; As if its darkening touch, through fear, It held from thing so saintly clear. H4 FRAGMENT OF AN EPISTLE. Thus nature threw her beauties round me; Thus from the gloom in which she found me ; She won me by her simple graces, She wooed me with her happy faces. The day is closed; and I refrain From further talk. But if of pain It has beguiled a weary hour, If to my desert mind, like shower, That wets the parching earth, has come A cheerful thought, and made its home With me awhile, I d have you share, Who feel for me in ills I bear. THE PLEASURE BOAT. I. COME, hoist the sail, the fast let go! They re seated side by side; Wave chases wave in pleasant flow: The bay is fair and wide. II. The ripples lightly tap the boat. Loose! Give her to the wind! She shoots ahead: They re all afloat: The strand is far behind. III. No danger reach so fair a crew! Thou goddess of the foam, I 11 ever pay thee worship due, If thou wilt bring them home. IV. Fair ladies, fairer than the spray The prow is dashing wide, Soft breezes take you on your way, Soft flow the blessed tide! THE PLEASURE BOAT. V. O, might I like those breezes be, And touch that arching brow, I d toil for ever on the sea Where ye are floating now. VI. The boat goes tilting on the waves; The waves go tilting by; There dips the duck; her back she laves; O er head the sea-gulls fly. VII. Now, like the gulls that dart for prey, The little vessel stoops; Now rising, shoots along her way, Like them, in easy swoops. VIII. The sun-light falling on her sheet, It glitters like the drift Sparkling in scorn of summer s heat, High up some mountain rift. IX. The winds are fresh; she s driving fast Upon the bending tide, The wrinkling sail, and wrinkling mast, Go with her side by side. THE PLEASURE BOAT. 117 X. Why dies the breeze away so soon? Why hangs the pennant down ? The sea is glass; the sun at noon. Nay, lady, do not frown; XI. For, see, the winged fisher s plume Is painted on the sea: Below, a cheek of lovely bloom. Whose eyes look up at thee ? XII. She smiles; thou need st must smile on her. And, see, beside her face A rich, white cloud that doth not stir. What beauty, and what grace ! XIII. And pictured beach of yellow sand, And peaked rock, and hill, Change the smooth sea to fairy land. How lovely and how still! XIV.\"\n",
    "text34 = \"The text describes nature and the beauty of a boat sailing on the water.\"\n",
    "\n",
    "inputs33 = tokenizer(text33, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "inputs34 = tokenizer(text34, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs33 = model(**inputs33)\n",
    "outputs34 = model(**inputs34)\n",
    "\n",
    "embeddings33 = outputs33.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings34 = outputs34.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings33, embeddings34.T) / (np.linalg.norm(embeddings33) * np.linalg.norm(embeddings34))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76c76c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7165\n"
     ]
    }
   ],
   "source": [
    "#passage E1236\n",
    "text35 = \"Then up, straight up, the deviation of a fraction of an inch being a certain precursor of disaster, the snowshoe must be lifted till the surface is cleared; then forward, down, and the other foot is raised perpendicularly for the matter of half a yard. He who tries this for the first time, if haply he avoids bringing his shoes in dangerous propinquity and measures not his length on the treacherous footing, will give up exhausted at the end of a hundred yards; he who can keep out of the way of the dogs for a whole day may well crawl into his sleeping bag with a clear conscience and a pride which passeth all understanding; and he who travels twenty sleeps on the Long Trail is a man whom the gods may envy. The afternoon wore on, and with the awe, born of the White Silence, the voiceless travelers bent to their work. Nature has many tricks wherewith she convinces man of his finity the ceaseless flow of the tides, the fury of the storm, the shock of the earthquake, the long roll of heaven's artillery but the most tremendous, the most stupefying of all, is the passive phase of the White Silence. All movement ceases, the sky clears, the heavens are as brass; the slightest whisper seems sacrilege, and man becomes timid, affrighted at the sound of his own voice.\"\n",
    "text36 = \"The text describes the challenges and dangers of traveling in the snow, emphasizing the silence and awe of nature.\"\n",
    "\n",
    "inputs35 = tokenizer(text35, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "inputs36 = tokenizer(text36, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs35 = model(**inputs35)\n",
    "outputs36 = model(**inputs36)\n",
    "\n",
    "embeddings35 = outputs35.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings36 = outputs36.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings35, embeddings36.T) / (np.linalg.norm(embeddings35) * np.linalg.norm(embeddings36))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d2445fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7664\n"
     ]
    }
   ],
   "source": [
    "#passage E1302\n",
    "text37 = \"There confectioners, cooks, and printers of books, There stampers of linen, and weavers, repair ; There widows, and maids, and all sort of trades, Go join in the humours of Donnybrook fair. There tinkers and nailers, and beggars and tailors, And singers of ballads, and girls of the sieve ; With Barrack-street rangers, the known ones and strangers, And many that no one can tell how they live : There horsemen and walkers, and likewise fruit-hawkers, And swindlers, the devil himself that would dare, With pipers and fiddlers, and dandies and diddlers, All meet in the humours of Donnybrook fair. 198 THE POPULAR SONGS OP IRELAND. Tis there are dogs dancing, and wild beasts a prancing, With neat bits of painting in red, yellow, and gold ; Toss-players and scramblers, and showmen and gamblers, Pickpockets in plenty, both of young and of old. There are brewers, and bakers, and jolly shoemakers, With butchers, and porters, and men that cut hair; There are mountebanks grinning, while others are sinning, To keep up the humours of Donnybrook fair. Brisk lads and young lasses can there fill their glasses With whisky, and send a full bumper around; Jigg it off in a tent till their money s all spent, And spin like a top till they rest on the ground. Oh, Donnybrook capers, to sweet catgut-scrapers, They bother the vapours, and drive away care ; And what is more glorious there s naught more uproarious Huzza for the humours of Donnybrook fair ! GLASHEN-GLORA. This lyric originally appeared, with the signature W. , in the  Cork Constitution  newspaper of 4th June, 1824; and was introduced by the following note to the editor of that paper : Mr. EDITOR, Your politeness in inserting a few lines which I wrote on the death of Lord Byron (dated 18th May), induces me to request a place for the trifle I now send you in your poet s corner. A RAMBLER. GLASHEN-GLORA. 199 Glaslien-glora, adds the author,  is a mountain torrent, which finds its way into the Atlantic Ocean through Glengariff, in the west of this county (Cork). Glashen- glory, I have been informed, signifies the roaring torrent. Whether this is a literal or liberal translation, I will not venture to assert. The Editor may add that the name, literally translated, signifies  the noisy green water :  olaf, green ; ei), water ; noisy. Tis sweet, in midnight solitude, When the voice of man lies hush d, subdued, To hear thy mountain-voice so rude Break silence, Glashen-glora ! I love to see thy foaming stream Dash d sparkling in the bright moonbeam ; For then of happier days I dream, Spent near thee, Glashen-glora ! I see the holly and the yew Still shading thee, as then they grew ; But there s a form meets not my view, As once, near Glashen-glora ! Thou gaily, brightly, spark st on, Wreathing thy dimples round each stone ; But the bright eye that on thee shone Lies quench d, wild Glashen-glora I 200 THE POPULAR SONGS OF IRELAND.\"\n",
    "text38 = \"The text describes the lively atmosphere and various participants at the Donnybrook fair. It then transitions to a lyric about the mountain torrent, Glashen-glora.\"\n",
    "\n",
    "inputs37 = tokenizer(text37, return_tensors=\"pt\", padding=True, truncation=True) \n",
    "inputs38 = tokenizer(text38, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs37 = model(**inputs37)\n",
    "outputs38 = model(**inputs38)\n",
    "\n",
    "embeddings37 = outputs37.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings38 = outputs38.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings37, embeddings38.T) / (np.linalg.norm(embeddings37) * np.linalg.norm(embeddings38))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2231e6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7034\n"
     ]
    }
   ],
   "source": [
    "#passage E1311\n",
    "text39 = \"But this was insufficient, it was necessary to subdue our incredulity, as to the effects of their new maxims, by exhibiting those effects in detail, and winning our assent to their truth by engrossing the fancy and charming the affections. The journey that you have lately made, I merely regard as an excursion into their visionary world. I can trace the argument of the parts which you have unfolded, with those which are yet to come, and can pretty well conjecture of what hues, and lines, and figures, the remainder of time picture is intended to consist. '[hen, said I, the task that I enjoined on myself is superfluous. You are apprised of all that I mean to sav on the topic of marriage, and have already laid in an ample stock of disapprobation for my service. I frankly confess that I expect not to approve the matter of your narrative, however pleased I may be with the manner. Nevertheless I wish you to execute your first design, that I maybe able to unveil the fallacy of your opinions, and rescue one whom I have no reason to disrespect, from specious but fatal illusions. Your purpose is kind. It entitles you at least to my thanks. Yet to say truth, I did' not at first despair of your confidence with me in some of mv opinions. I imagined that some of the evils of marriage had not escaped you. I recollect that during our last conversation, you arraigned with great earnestness the injustice of condemning women to obey the will, and depend upon the bounty of father or husband. Come, come, interrupted the lady, with a severer aspect, if you mean to preserve my good opinion, you must tread on this ground with more caution. Remember the atrociousness of the charge you would insinuate. What! Because a just indignation at the iniquities that are hourly committed on one half of the human species rises in my heart, because I vindicate the plainest dictates of justice, and am willing to rescue so large a portion of human-kind, from so destructive a bondage: a bondage not only of the hands, but of the understanding ; which divests them of all those energies which distinguish men from the basest animals, destroys all perception of moral rectitude, and reduces its subjects to so calamitous a state, that they adore the tyranny that rears its crest over them, and kiss the hand that loads them with ignominy ! When I demand an equality of conditions among beings that equally partake of the same divine reason, would you rashly infer that I was an enemy to the institution of marriage itself? Where shall we look for human beings who surpass all others in depravity and wretchedness? Are they not to be found in the haunts of female licentiousness. If their vice admits of a darker hue, it would receive it from the circumstance of their being dissolute by theory ; of their modelling voluptuousness into a speculative system.\"\n",
    "text40 = \"The speaker expresses their disagreement with the new maxims and beliefs about marriage, but acknowledges the kindness of the narrator's intentions.\"\n",
    "\n",
    "inputs39 = tokenizer(text39, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs40 = tokenizer(text40, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs39 = model(**inputs39)\n",
    "outputs40 = model(**inputs40)\n",
    "\n",
    "embeddings39 = outputs39.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings40 = outputs40.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings39, embeddings40.T) / (np.linalg.norm(embeddings39) * np.linalg.norm(embeddings40))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4937e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.5866\n"
     ]
    }
   ],
   "source": [
    "#passage E13\n",
    "text41 = \"Yet it was an honorable purpose nursed by sweet dreams, and by hopes such as souls feed upon, strengthening themselves for trials of life ; I must carry it back with me, not for burial in my own breast, but for gossips to rend and tear, and make laughter of the wonder and amusement of an unfeeling city. How many modes of punishment God keeps in store for the chastening of those who love Him !  It is beggarly saying I sympathize - No, no wait! he cried, passionately. Now it breaks upon me. I may not offer thee a seat on my throne, or give a hand to help thee up to it; for the present I will not declare I love thee ; yet harm cannot come of telling thee what has been. Thou hadst my love at our first meeting. I loved thee then. As a man I loved thee, nor less as an Emperor because a man. Thou wast lovely with the loveliness of the angels. I saw thee in a light not of earth, and thou wert transparent as the light. I descended from the throne to thee thinking thou hadst collected all the radiance of the sun wasting in the void between stars, and clothed thyself in it. Oh, my Lord Not yet, not yet Blasphemy and madness!  Be it so! he answered, with greater intensity. This once I speak as a lover who was a lover making last memories of the holy passion, to be henceforth accounted dead. Dead ? Ah, yes ! tome dead to me! She timidly took the hand he dropped upon his knee at the close of a long sigh. It may rest my Lord to hear me, she said, tearfully.  I never doubted his fitness to be Emperor, or if ever I had such a doubt, it is no more. He has conquered himself! Indeed, indeed, it is sweet to hear him tell his love, for I am woman; and if I cannot give it back measure for measure, this much may be accepted by him I have never loved a man, and if the future holds such a condition in store forme, I will think of my Lord, and his strength and triumph, and in my humbler lot do as he has so nobly done. He has his Empire to engage him, and fill his hours with duties ; I have God to serve and obey with singleness. Out of the prison where my mother died, and in which my father grew old counting his years as they slowly wore away, a shadow issued, and is always at hand to ask me, Who art thou ? What right hast thou to happiness ? And if ever I fall into the thought so pleasant to woman, of loving and being loved, and of marriage, the shadow intervenes, and abides with me until I behold myself again bounden to religion, a servant vowed to my fellow creatures sick, suffering, or in sorrow.\"\n",
    "text42 = \"The text explores the themes of love, honor, and self-sacrifice through a conversation between two characters.\"\n",
    "\n",
    "inputs41 = tokenizer(text41, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs42 = tokenizer(text42, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs41 = model(**inputs41)\n",
    "outputs42 = model(**inputs42)\n",
    "\n",
    "embeddings41 = outputs41.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings42 = outputs42.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings41, embeddings42.T) / (np.linalg.norm(embeddings41) * np.linalg.norm(embeddings42))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61f9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6694\n"
     ]
    }
   ],
   "source": [
    "#passage E79\n",
    "text43 = \"But the fan-girl led the way with the lamp swinging in her hand, as one accustomed to the mazes. Here she doubled, there she turned, and here she stopped in the middle of a blank wall to push a stone, which swung to let us pass. And once she pressed at the corner of a flagstone on the floor, which reared up to the thrust of her foot, and showed us a stair steep and narrow. That we descended, coming to the foot of an inclined way which led us upward again; and so by degrees we came unto the chamber which had been given for my use. There is raiment in all these chests which stand by the walls, said the girl, and jewels and gauds in that bronze coffer. They are Phorenice's first presents, she bid me say, and but a small earnest of what is to come. My Lord Deucalion can drop his simplicity now, and fig himself out in finery to suit the fashion. Girl, I said sharply, be more decorous with your tongue, and spare me such small advice. If my Lord Deucalion thinks this a rudeness, he can give a word to Phorenice, and I shall be whipped.\"\n",
    "text44 = \"The protagonist follows a fan-girl through a maze-like structure to a chamber where they find clothes and jewels.\"\n",
    "\n",
    "inputs43 = tokenizer(text43, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs44 = tokenizer(text44, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs43 = model(**inputs43)\n",
    "outputs44 = model(**inputs44)\n",
    "\n",
    "embeddings43 = outputs43.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings44 = outputs44.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings43, embeddings44.T) / (np.linalg.norm(embeddings43) * np.linalg.norm(embeddings44))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a696547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6997\n"
     ]
    }
   ],
   "source": [
    "#passage E121\n",
    "text45 = \"She never ceased canvassing with herself how he had borne her desertion; whether he had sunk under it into a hopeless despondency, or called upon his pride to sustain him above any show of indignation. Reading it as the world must read it, there never was such ingratitude; but then the world could never know the provocation, nor ever know by what personal sacrifice she had avenged the slight passed upon her. My story, said she, can never be told; his, he may tell how it suite him. At moments, a sort of romantic exaltation and a sense of freedom would make her believe that she had done well to exchange the splendid bondage of the past for the untrammelled liberty of the present; and then, at other times, the terrible contrast would so overcome her, that she would sit and cry as if her heart was breaking. Would my 'old Gardy' pity or exult over me if he saw me now? What would he, who would not suffer me to tread on an uncarpeted step, say if he saw me alone, and poorly clad, clambering up these rugged cliffs to reach some point, where, for an instant, I may forget myself? Surely he would not triumph over my fall! Such a life as this is meant to expiate great crimes. Men are sent to wild and desolate islands in the ocean, to wear out days of hopeless misery, because they have warred against their fellows. But what have I done? whom have I injured? Others had friends to love and to guide them; I had none. The very worst that can be alleged against me is, that I was rash and headstrong too prone to resent; and what has it cost me! My uncle said, indeed, this need not be my prison if I could not endure its privations. But what did that mean what alternative did he point to? Was it that I was to go lower still, and fall back upon all the wretchedness I sprang from? That, never! The barren glory of calling myself a Luttrell may be a sorry price for forfeited luxury and splendour; but I have it, and I will hold it. I am a Luttrell now, and one day, perhaps, these dreary hills shall own me their mistress. In some such thoughts as these, crossed and recrossed by regrets and half-shadowed hopes, she was returning one night to the Abbey, when Molly met her. There was such evident anxiety and eagerness in the woman's face, that Kate quickly asked her: What is it? What has happened? Nothing, Miss, nothing at all. 'Tis only a man is come. He's down at the Holy Well, and wants to speak to you. Who is he? What is he? I never seen him before, Miss, but he comes from bryant there she motioned towards the main land of Ireland and says that you know him well. Have you told my uncle of him?\"\n",
    "text46 = \"The protagonist is reflecting on her past actions and the consequences they have had on her life. She wonders how the man she left behind has coped with her departure and contemplates her current situation.\"\n",
    "\n",
    "inputs45 = tokenizer(text45, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs46 = tokenizer(text46, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs45 = model(**inputs45)\n",
    "outputs46 = model(**inputs46)\n",
    "\n",
    "embeddings45 = outputs45.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings46 = outputs46.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings45, embeddings46.T) / (np.linalg.norm(embeddings45) * np.linalg.norm(embeddings46))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f818b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6991\n"
     ]
    }
   ],
   "source": [
    "#passage E6051\n",
    "text47 = \"Among his remaining works are, A Lecture on the Sulphur Water of Harrogate, and a singular publication entitled, China Famulatrix Medicine ; or, Receipts in Cookery, worthy the Notice of those Medical Practitioners who ride in their Chariots, &c. ANDERSON, (JAMES,) the son of a farmer, was born at Hermiston, near Edinburgh, in the year 1739. At an early age, he lost both his parents, and after having received an ordinary education, commenced the study of agriculture on his paternal farm. He subsequently removed to another of one thousand three hundred acres, in Aberdeenshire, where he made some experiments, of which he gave an account, in The Edinburgh Weekly Magazine, under the name of Agricola, It was succeeded by his Inquiry into the Nature of the Corn Laws; and Essays relating to Agriculture and Rural Affairs, in three volumes, octavo, which procured him much reputation, and reached a fifth edition in 1800. In 1779, appeared his Inquiry into the Causes that have retarded the Advancement of Agriculture in Europe; K U K A L AND DOMESTIC ECONOMISTS. and in the following year, the University of Aberdeen conferred upon him the degree of L. L. D. In 1783, he took up his residence near Edinburgh, and, about the same period, printed Proposals for establishing the Northern British Fisheries, which produced a request to him, from government, to survey the western coast of Scotland, with a view to obtain further information on the subject. His report of the survey, which he, in consequence, made, was presented to the Treasury, in 1785, but he does not appear to have received any other reward than that of their approbation. He now resumed his literary labours, and printed, in 1789, Observations on Slavery ; and, in 1791, commenced a weekly publication, called The Bee, which he carried on till 1794, making eighteen volumes, octavo. In this, he not only supplied the greater part of the anonymous papers, but all those signed Senex, Timothy Hairbrain, and Alcibiades. After publishing, among other works, Remarks on the Poor Laws in Scotland, and A Practical Treatise on Peat Moss, he removed to London, and commenced a periodical work, entitled, Recreations m Agriculture, which, having reached six octavo volumes, he discontinued in the month of March, 1802. These, and other publications too numerous to mention, procured Dr. Anderson considerable reputation as an agriculturist, and led to a correspondence between him and Washington, which appeared in 1800. His writings, which evince considerable learning, energy, and penetration, led the way to many improvements in agriculture ; and, in political as well as rural economy, tended to important and beneficial results. He died on the 15th of October, 1808, leaving a widow, who was his second wife, and six children, the survivors of a family of thirteen, by his first. He was a contributor to several periodicals besides those of which he was the establishes and wrote the articles Dictionary, Winds, Monsoons, &c., for the Encyclopaedia Britannica.\"\n",
    "text48 = \"The text provides biographical information about James Anderson, a writer and agriculturist, and mentions some of his works and accomplishments.\"\n",
    "\n",
    "inputs47 = tokenizer(text47, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs48 = tokenizer(text48, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs47 = model(**inputs47)\n",
    "outputs48 = model(**inputs48)\n",
    "\n",
    "embeddings47 = outputs47.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings48 = outputs48.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings47, embeddings48.T) / (np.linalg.norm(embeddings47) * np.linalg.norm(embeddings48))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb08352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.5996\n"
     ]
    }
   ],
   "source": [
    "#passsage E6255\n",
    "text49 = \"Sam made no answer, but after a minute laid his hand on Turk, who responded with a low growl. This made Caleb continue: Down on me, down on my Dog. Pogue says he kills Sheep 'an' every one is ready to believe it. I never knowed a Hound turn Sheep-killer, an' I never knowed a Sheep-killer kill at home, an' I never knowed a Sheep-killer content with one each night, an' I never knowed a Sheep-killer leave no tracks, an' Sheep was killed again and again when Turk was locked up in the shanty with me. Well, whose Dog is it does it? I don't know as it's any Dog, for part of the Sheep was eat each time, they say, though I never seen one o' them that was killed or I could tell. It's more likely a Fox or a Lynx than a Dog. There was a long silence, then outside again the hair-lifting screech to which the Dog paid no heed, although the Trapper and the boy were evidently startled and scared. They made up a blazing fire and turned in silently for the night. The rain came down steadily, and the wind swept by in gusts. It was the Banshee's hour, and two or three times, as they were dropping off, that fearful, quavering human wail, like a woman in distress, came from the woods to set their hearts a-jumping, not Caleb and Sam only, but all four. In the diary which Yan kept of those times each day was named after its event; there was Deer day, Skunk-and-Cat day, Blue Crane day, and this was noted down as the night of the Banshee's wailing. Caleb was up and had breakfast ready before the others were fully awake. They had carefully kept and cleaned the Coon meat, and Caleb made of it a prairie pie, in which bacon, potatoes, bread, one small onion and various scraps of food were made important. This, warmed up for breakfast and washed down with coffee, made a royal meal, and feasting they forgot the fears of the night. The rain was over, but the wind kept on. Great blockish clouds were tumbling across the upper sky Yan went out to look for tracks. He found none but those of raindrops. The day was spent chiefly about camp, making arrows and painting the teepee. Again Caleb was satisfied to sleep in the camp. The Banshee called once that night, and again Turk seemed not to hear, but half an hour later there was a different and much lower sound outside, a light, nasal wow. The boys scarcely heard it, but Turk sprang up with bristling hair, growling, and forcing his way out under the door, he ran, loudly barking, into the woods. He's after something now, all right, said his master; and now he's treed it, as the Dog began his high-pitched yelps. Good old Dog; he's treed the Banshee, and Yan rushed out into the darkness.\"\n",
    "text50 = \"The Bishop travels to various places in India, encountering religious conversions and building churches along the way.\"\n",
    "\n",
    "inputs49 = tokenizer(text49, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs50 = tokenizer(text50, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs49 = model(**inputs49)\n",
    "outputs50 = model(**inputs50)\n",
    "\n",
    "embeddings49 = outputs49.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings50 = outputs50.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings49, embeddings50.T) / (np.linalg.norm(embeddings49) * np.linalg.norm(embeddings50))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f91494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7044\n"
     ]
    }
   ],
   "source": [
    "#passage E6336\n",
    "text51 = \"Sometimes this alternation is merely a reversal of contrasts; as that, after red has been for some time on one side, and blue on the other, red shall pass to blue's side and blue to red's. This kind of alternation takes place simply in four-quartered shields; in more subtle pieces of treatment, a little bit only of each color is carried into the other, and they are as it were dovetailed together. One of the most curious facts which will impress itself upon you, when you have drawn some time carefully from Nature in light and shade, is the appearance of intentional artifice with which contrasts of this alternate kind are produced by her; the artist with which she will darken a tree trunk as long as it comes against light sky, and throw sunlight on it precisely at the spot where it comes against a dark hill, and similarly treat all her masses of shade and color, is so great, that if you only follow her closely, every one who looks at your drawing with attention will think that you have been inventing the most artificially and unnaturally delightful interchanges of shadow that could possibly be devised by human wit. 229. You will find this law of interchange insisted upon at length by Prout in his Lessons on Light and Shade: it seems of all his principles of composition to be the one he is most conscious of; many others he obeys by instinct, but this he formally accepts and forcibly declares. The typical purpose of the law of interchange is, of course, to teach us how opposite natures may be helped and strengthened by receiving each, as far as they can, some impress or reflection, or imparted power, from the other. 8. THE LAW OF CONSISTENCY. 230. It is to be remembered, in the next place, that while contrast exhibits the characters of things, it very often neutralizes or paralyzes their power . A number of white things may be shown to be clearly white by opposition of a black thing, but if we want the full power of their gathered light, the black thing may be seriously in our way. Thus, while contrast displays things, it is unity and sympathy which employ them, concentrating the power of several into a mass. And, not in art merely, but in all the affairs of life, the wisdom of man is continually called upon to reconcile these opposite methods of exhibiting, or using, the materials in his power. By change he gives them pleasantness, and by consistency value; by change he is refreshed, and by perseverance strengthened. 231.\"\n",
    "text52 = \"The text discusses the alternation of contrasts and the use of unity and sympathy in art and life.\"\n",
    "\n",
    "inputs51 = tokenizer(text51, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs52 = tokenizer(text52, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs51 = model(**inputs51)\n",
    "outputs52 = model(**inputs52)\n",
    "\n",
    "embeddings51 = outputs51.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings52 = outputs52.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings51, embeddings52.T) / (np.linalg.norm(embeddings51) * np.linalg.norm(embeddings52))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6a2af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6216\n"
     ]
    }
   ],
   "source": [
    "#passage E3090\n",
    "text53 = \"And ears to hear you even in his dreams.With that he turn'd and look'd as keenly at her As cared robins eye the delver's toil ; And that within her, which a wanton fool, Or hasty judger would have call'd her guilt. Made her cheek burn and either eyelid fall. And Geraint look'd and was not satisfied. ENID. 69 Then forward by a way which, beaten broad, Led from the territory of false Lion's To the waste earldom of another earl, Doormm, whom his shaking vassals call'd the Bull, Went Enid with her sullen follower on. Once she look'd back, and when she saw him ride More near by many a rood than yester-morn. It wellnigh made her cheerful ; till Geraint Waving an angry hand as who should say You watch me,' sadden'd all her heart ao-ain. But while the sun yet beat a dewy blade, The sound of many a heavily-galloping hoof Smote on her ear, and turning round she saw Dust, and the points of lances bicker in it. Then not to disobey her lord's behest. And yet to give him warning, for he rode As if he heard not, moving back she held Her finger up, and pointed to the dust. At which the warrior in his obstinacy. Because she kept the letter of his word 70 ENID. Was in a manner pleased, and turning, stood. And in the moment after, wild Limours, Borne on a black horse, like a thunder-cloud Whose skirts are loosen'd by the breaking storm, Half ridden off with by the thing he rode, And all in passion uttering a diy shriek, Dash'd on Geraint, who closed with him, and bore DoYvai by the length of lance and arm beyond The crupper, and so left him stunn'd or dead, And overthrew the next that follow' d him, And blindly rush'd on all the rout behind. But at the flash and motion of the man They vanish' d panic-stricken, like a shoal Of darting fish, that on a summer morn Adown the crystal dykes at Camelot Come slipping o'er their shadows on the sand, But if a man who stands upon the brink But lift a shining hand against the sun, There is not left the twinkle of a fin Betwixt the cressy islets white in flower ; ENID. So, scared but at the motion of the man, Fled all the boon companions of the Earl, And left him lying in the public way ; So vanish friendships only made in wine. Then like a stormy sunlight smiled Geraint, Who saw the chargers of the two that fell Start from their fallen lords, and wildly fiy, Mixt with the flyers. ' Horse and man,' he said, All of one mind and all right-honest friends !\"\n",
    "text54 = \"A pilgrim sets out on a journey and meets a young man along the way. They arrive at a palace where they are treated lavishly, but the young man steals a golden goblet from the host.\"\n",
    "\n",
    "inputs53 = tokenizer(text53, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs54 = tokenizer(text54, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs53 = model(**inputs53)\n",
    "outputs54 = model(**inputs54)\n",
    "\n",
    "embeddings53 = outputs53.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings54 = outputs54.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings53, embeddings54.T) / (np.linalg.norm(embeddings53) * np.linalg.norm(embeddings54))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243e2b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7746\n"
     ]
    }
   ],
   "source": [
    "#passage E3153\n",
    "text55 = \"He seemed so bright and happy, in spite of his lameness, which kept him from running about like other young men. He looked a little older than Miss Laura, and one day, a week or two later, when they were sitting on the veranda, I heard him tell her that he was just nineteen. He told her, too, that his lameness made him love animals. They never laughed at him, or slighted him, or got impatient, because he could not walk quickly. They were always good to him, and he said he loved all animals while he liked very few people. On this day as he was limping along, he said to Mrs. Wood: I am getting more absent-minded every day. Have you heard of my latest escapade? No, she said. I am glad, he replied. I was afraid that it would be all over the village by this time. I went to church last Sunday with my poor guinea pig in my pocket. He hasn't been well, and I was attending to him before church, and put him in there to get warm, and forgot about him. Unfortunately I was late, and the back seats were all full, so I had to sit farther up than I usually do. During the first hymn I happened to strike Piggy against the side of the seat. Such an ear-splitting squeal as he set up. It sounded as if I was murdering him.\"\n",
    "text56 = \"A young man with a lameness tells Miss Laura about his love for animals and his recent mishap at church.\"\n",
    "\n",
    "inputs55 = tokenizer(text55, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs56 = tokenizer(text56, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs55 = model(**inputs55)\n",
    "outputs56 = model(**inputs56)\n",
    "\n",
    "embeddings55 = outputs55.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings56 = outputs56.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings55, embeddings56.T) / (np.linalg.norm(embeddings55) * np.linalg.norm(embeddings56))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a66147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7335\n"
     ]
    }
   ],
   "source": [
    "#passage E3396\n",
    "text57 = \"Mother didn't like the idea of my going to a strange boarding-house, so Miss Mills kindly made a place for me. You know she lets her rooms without board, but she is going to give me my dinners, and I'm to get my own breakfast and tea, quite independently. I like that way, and it 's very little trouble, my habits are so simple; a bowl of bread and milk night and morning, with baked apples or something of that sort, is all I want, and I can have it when I like. Is your room comfortably furnished? Can't we lend you anything, my dear? An easy-chair now, or a little couch, so necessary when one comes in tired, said Mrs. Shaw, taking unusual interest in the affair. Thank you, but I don't need anything, for I brought all sorts of home comforts with me. Oh, Fan, you ought to have seen my triumphal entry into the city, sitting among my goods and chattels, in a farmer's cart. Polly's laugh was so infectious that every one smiled and forgot to be shocked at her performance. Yes, she added, I kept wishing I could meet you, just to see your horrified face when you saw me sitting on my little sofa, with boxes and bundles all round me, a bird-cage on one side, a fishing basket, with a kitten's head popping in and out of the hole, on the other side, and jolly old Mr. Brown, in his blue frock, perched on a keg of apples in front. It was a lovely bright day, and I enjoyed the ride immensely, for we had all sorts of adventures. Oh, tell about it, begged Maud, when the general laugh at Polly's picture had subsided. Well, in the first place, we forgot my ivy, and Kitty came running after me, with it. Then we started again, but were soon stopped by a great shouting, and there was Will racing down the hill, waving a pillow in one hand and a squash pie in the other. How we did laugh when he came up and explained that our neighbor, old Mrs. Dodd, had sent in a hop-pillow for me, in case of headache, and a pie to begin housekeeping with. She seemed so disappointed at being too late that Will promised to get them to me, if he ran all the way to town. The pillow was easily disposed of, but that pie! I do believe it was stowed in every part of the wagon, and never staid anywhere. I found it in my lap, then on the floor, next, upside down among the books, then just on the point of coasting off a trunk into the road, and at last it landed in my rocking-chair. Such a remarkable pie as it was, too, for in spite of all its wanderings, it never got spilt or broken, and we finally ate it for lunch, in order to be left in peace.\"\n",
    "text58 = \"The protagonist, Polly, is moving into a boarding house and describes her simple habits and the adventures she had on her way to the city.\"\n",
    "\n",
    "inputs57 = tokenizer(text57, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs58 = tokenizer(text58, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs57 = model(**inputs57)\n",
    "outputs58 = model(**inputs58)\n",
    "\n",
    "embeddings57 = outputs57.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings58 = outputs58.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings57, embeddings58.T) / (np.linalg.norm(embeddings57) * np.linalg.norm(embeddings58))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c117c3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6739\n"
     ]
    }
   ],
   "source": [
    "#passage E3405\n",
    "text59 = \"It is often a matter of thought and reflection to me, when friends have left my room whom I have kept in a roar of laughter, how little do they know of the miserableness of one who appeared to be in such spirits. Then comes the self-inquiry, Am I indeed a hypocrite? of all characters to me the most detestable. I think not. A man is under no more obligation to expose his griefs than to exhibit his bruises and sores. These should be shown to only the trusted few who have access to the inner shrine of his heart. To this shrine, with me, but one living being upon earth was ever admitted, and that one is yourself. If I had not one at least with whom I thus could communicate, it appears to me that life would become intolerable. Do you ask, then, why I am thus miserable ? It is because I meet with little sympathy from the world. Even the praise of those who approve, from whatever motive given, is often, indeed most frequently offered, in a manner which is gall and wormwood to me. My life has been a warfare from the beginning. My strife has been with fate. The contest began in the cradle and will end only in the grave. Weak and sickly, I was sent into the world with a constitution barely able to sustain the vital functions. Health I have never known and do not expect to know. But this I could bear : pain I can endure ; I am used to it. Physical sufferings are not the worst ills I am heir to. I find no unison of feelings, tastes, and sentiments with the world. . I feel myself to be 286 LIFE OF ALEXANDER H. STEPHENS. alone; and feel that my habitation should be in solitude. But do not think that I cower before fate. No ; to my destiny I bow, submissively bow to that which is beyond my control. I yield to nothing else. And even in solitude I feel that spirit within me which would enable me, so far from sinking into despair, to drink to the very dregs the bitterest cup that time can measure out, and looking up, ask for more. Other letters refer to the Know-Nothing party, then just coming into notice. Not being informed of their policy, he suspends his judgment about them, except that he is opposed to all secret organizations in a Republic,  where, he says,  every man ought to have his principles written on his forehead. December 31st. A letter in the usual style for this season. He digresses, however, into politics a little. Public sentiment in this country is in a transition state, so far as the principle of party organization is concerned. Old parties, old names, old issues, and old organizations are passing away. A day of new things, new issues, new leaders, and new organizations is at hand. The men now in power, holding their places by the foulest coalition known in our history, seem not to foresee that doom which evidently awaits them.\"\n",
    "text60 = \"The narrator reflects on their inner misery and lack of sympathy from the world.\"\n",
    "\n",
    "inputs59 = tokenizer(text59, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs60 = tokenizer(text60, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs59 = model(**inputs59)\n",
    "outputs60 = model(**inputs60)\n",
    "\n",
    "embeddings59 = outputs59.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings60 = outputs60.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings59, embeddings60.T) / (np.linalg.norm(embeddings59) * np.linalg.norm(embeddings60))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc22fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6705\n"
     ]
    }
   ],
   "source": [
    "#passage E85\n",
    "text61 = \"Before even she could see it she heard the hollow bumping of a large boat against its rotten posts, and heard also the murmur of whispered conversation in that boat whose white paint and great dimensions, faintly visible on nearer approach, made her rightly guess that it belonged to the brig just anchored. Stopping her course by a rapid motion of her paddle, with another swift stroke she sent it whirling away from the wharf and steered for a little rivulet which gave access to the back courtyard of the house. She landed at the muddy head of the creek and made her way towards the house over the trodden grass of the courtyard. To the left, from the cooking shed, shone a red glare through the banana plantation she skirted, and the noise of feminine laughter reached her from there in the silent evening. She rightly judged her mother was not near, laughter and Mrs. Almayer not being close neighbours. She must be in the house, thought Nina, as she ran lightly up the inclined plane of shaky planks leading to the back door of the narrow passage dividing the house in two. Outside the doorway, in the black shadow, stood the faithful Ali. Who is there?\"\n",
    "text62 = \"A woman named Nina arrives at a house by boat, looking for her mother.\"\n",
    "\n",
    "inputs61 = tokenizer(text61, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs62 = tokenizer(text62, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs61 = model(**inputs61)\n",
    "outputs62 = model(**inputs62)\n",
    "\n",
    "embeddings61 = outputs61.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings62 = outputs62.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings61, embeddings62.T) / (np.linalg.norm(embeddings61) * np.linalg.norm(embeddings62))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0952a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7261\n"
     ]
    }
   ],
   "source": [
    "#passage E102\n",
    "text63 = \"At any rate, seeing that the Jews were more specifically than any other nation educated into a sense of their supreme moral value, the chief matter of surprise is that any other nation is found to rival them in this form of self-confidence. More exceptional éˆ¥?less like the course of our own history éˆ¥?has been their dispersion and their subsistence as a separate people through ages in which for the most part they were regarded and treated very much as beasts hunted for the sake of their skins, or of a valuable secretion peculiar to their species. The Jews showed a talent for accumulating what was an object of more immediate desire to Christians than animal oils or well-furred skins, and their cupidity and avarice were found at once particularly hateful and particularly useful : hateful when seen as a reason for punishing them 330 THEOPHRASTUS SUCH. by mulcting or robbery, useful when this retributive process could be successfully carried forward. Kings and emperors naturally were more alive to the usefulness of subjects who could gather and yield money; but edicts issued to protect  the King's Jews equally with the King's game from being harassed and hunted by the commonalty were only slight mitigations to the deplorable lot of a race held to be under the divine curse, and had little force after the Crusades began. As the slave-holders in the United States counted the curse on Ham a justification of negro slavery, so the curse on the Jews was counted a justification for hindering them from pursuing agriculture and handicrafts ; for marking them out as execrable figures by a peculiar dress ; for torturing them to make them part with their gains, or for more gratuitously spitting at them and pelting them ; for taking it as certain that they killed and ate babies, poisoned the wells, and took pains to spread the plague ; for putting it to them whether they would be baptised or burned, and not failing to burn and massacre them when they were obstinate ; but also for suspecting them THE MODERN HEP ! HEP ! HEP ! 331 of disliking the baptism when they had got it, and then burning them in punishment of their insincerity ; finally, for hounding them by tens on tens of thousands from the homes where they had found shelter for centuries, and inflicting on them the horrors of a new exile and a new dispersion. All this to avenge the Saviour of mankind, or else to compel these stiff-necked people to acknowledge a Master whose servants showed such beneficent effects of His teaching.\"\n",
    "text64 = \"The text discusses the historical mistreatment of Jews and the justifications used for their persecution.\"\n",
    "\n",
    "inputs63 = tokenizer(text63, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs64 = tokenizer(text64, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs63 = model(**inputs63)\n",
    "outputs64 = model(**inputs64)\n",
    "\n",
    "embeddings63 = outputs63.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings64 = outputs64.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings63, embeddings64.T) / (np.linalg.norm(embeddings63) * np.linalg.norm(embeddings64))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f881787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6767\n"
     ]
    }
   ],
   "source": [
    "#passage E259\n",
    "text65 = \"He was banished in 1782, and his property was confiscated. DENTON. In 1775 Joseph Denton, of Brook-haven, New York, assisted Major Benjamin Floyd in procuring signatures 256 BIOGRAPHICAL SKETCHES to a paper expressive of a determination to support the royal authority. In 1776 Thomas, Amos junior, Joseph, Samuel, Isaac, and Amos Denton, of Queen s County, professed themselves to Lord Richard and General William Howe, loyal and well affected subjects. In 1780, James Denton of that County was in arms against the Whigs. The name of Joseph Denton is found among the Addressers of Lieutenant Colonel Sterling. DEONEZZAU, ADAM. In 1776 he embarked at Boston for Halifax with the British army. DE Pester, ABRAHAM. Of New York. He entered the king s service, and was a captain in the New York Volunteers. He was second in command at the battle of King s Mountain, in 1780, and after the fall of Ferguson, hoisted a flag as a signal of surrender. The firing immediately ceased, and the royal troops laying down their arms, the most of which were loaded, submitted to the conquerors at discretion. It seems not to be generally understood, that nearly the whole of Ferguson s force was composed of Loyalists; but such is the fact. He went into action with eleven hundred and twenty-five men, of whom only one hundred and sixty-two were regulars. Of the Loyalists, no less than two hundred and six were killed, one hundred and twenty-eight wounded, and six hundred and twenty-nine taken prisoners. The loss of regulars, was eighteen slain, and one hundred and three wounded and captured. Captain De Pester was paid off the morning of the battle. Among the coin which he received was a doubloon, which he put in a pocket of his vest. While on the field, a bullet struck the gold and stopped, and his life was thus saved. He went to St. John, New Brunswick, at the peace, and was one of the grantees of that city. He received half-pay. He Alas treasurer of New Brunswick, and a colonel in the militia. He died in that Colony previous to 1799, as in that year leave was given to sell a part of his estate in the hands of his administrator. DE Pester, FREDERICK. Of New York. He was a captain in the New York Volunteers in 1782. In 1784 he was at St. John, New Brunswick, and received the grant of a city OF AMERICAN LOYALISTS. 257 lot. In 1792 he was a magistrate in the County of York. He returned to the United States. A gentleman of this name was a student of Peter Van Shaack in early life, was much esteemed by him, and one of his principal correspondents in his old age. This Mr. De Pester and possibly the same was living in New York in 1S28. Dickson, CAPTAIN JACOB. Of Brandywine, Delaware.\"\n",
    "text66 = \"The text provides brief biographical sketches of various individuals and their involvement in the American Revolution.\"\n",
    "\n",
    "inputs65 = tokenizer(text65, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs66 = tokenizer(text66, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs65 = model(**inputs65)\n",
    "outputs66 = model(**inputs66)\n",
    "\n",
    "embeddings65 = outputs65.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings66 = outputs66.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings65, embeddings66.T) / (np.linalg.norm(embeddings65) * np.linalg.norm(embeddings66))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14e2000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7776\n"
     ]
    }
   ],
   "source": [
    "#passgae E301\n",
    "text67 = \"Supplehouse was to make one of the Chaldicotes party. Now Mr. Supplehouse was a worse companion for a gentlemanlike, young, High Church, conservative county parson than even Harold Smith. He also was in Parliament, and had been extolled during the early days of that Russian war by some portion of the metropolitan daily press, as the only man who could save the country. Let him be in the ministry, the Jupiter had said, and there would be some hope of reform, some chance that England's ancient glory would not be allowed in these perilous times to go headlong to oblivion. And upon this the ministry, not anticipating much salvation from Mr. Supplehouse, but willing, as they usually are, to have the Jupiter at their back, did send for that gentleman, and gave him some footing among them. But how can a man born to save a nation, and to lead a people, be content to fill the chair of an under-secretary? Supplehouse was not content, and soon gave it to be understood that his place was much higher than any yet tendered to him. The seals of high office, or war to the knife, was the alternative which he offered to a much-belaboured Head of Affairs nothing doubting that the Head of Affairs would recognize the claimant's value, and would have before his eyes a wholesome fear of the Jupiter. But the Head of Affairs, much belaboured as he was, knew that he might pay too high even for Mr. Supplehouse and the Jupiter; and the saviour of the nation was told that he might swing his tomahawk. Since that time he had been swinging his tomahawk, but not with so much effect as had been anticipated. He also was very intimate with Mr. Sowerby, and was decidedly one of the Chaldicotes set. And there were many others included in the stigma whose sins were political or religious rather than moral. But they were gall and wormwood to Lady Lufton, who regarded them as children of the Lost One, and who grieved with a mother's grief when she knew that her son was among them, and felt all a patron's anger when she heard that her clerical protege was about to seek such society. Mrs. Robarts might well say that Lady Lufton would be annoyed. You won't call at the house before you go, will you? the wife asked on the following morning. He was to start after lunch on that day, driving himself in his own gig, so as to reach Chaldicotes, some twenty-four miles distant, before dinner. No, I think not. What good should I do? Well, I can't explain; but I think I should call: partly, perhaps, to show her that as I had determined to go, I was not afraid of telling her so. Afraid! That's nonsense, Fanny. I'm not afraid of her. But I don't see why I should bring down upon myself the disagreeable things she will say. Besides, I have not time.\"\n",
    "text68 = \"Mr. Supplehouse, a politician, is not satisfied with his position and desires higher office. He is associated with a group of individuals that Lady Lufton disapproves of, including her own son.\"\n",
    "\n",
    "inputs67 = tokenizer(text67, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs68 = tokenizer(text68, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs67 = model(**inputs67)\n",
    "outputs68 = model(**inputs68)\n",
    "\n",
    "embeddings67 = outputs67.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings68 = outputs68.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings67, embeddings68.T) / (np.linalg.norm(embeddings67) * np.linalg.norm(embeddings68))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a0a8f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7067\n"
     ]
    }
   ],
   "source": [
    "#passage E358\n",
    "text69 = \"There were, moreover, no resident Grizzlies, no signs even of passing travelers, and the Blackbears that were in possession did not count. Wahb was well pleased. He rolled his vast bulk in an old Buffalo-wallow, and rearing up against a tree where the Piney Canon quits the Graybull Canon, he left on it his mark fully eight feet from the ground. In the days that followed he wandered farther and farther up among the rugged spurs of the Shoshones, and took possession as he went. He found the signboards of several Blackbears, and if they were small dead trees he sent them crashing to earth with a drive of his giant paw. If they were green, he put his own mark over the other mark, and made it clearer by slashing the bark with the great pickaxes that grew on his toes. The Upper Piney had so long been a Blackbear range that the Squirrels had ceased storing their harvest in hollow trees, and were now using the spaces under flat rocks, where the Blackbears could not get at them; so Wahb found this a land of plenty: every fourth or fifth rock in the pine woods was the roof of a Squirrel or Chipmunk granary, and when he turned it over, if the little owner were there, Wahb did not scruple to flatten him with his paw and devour him as an agreeable relish to his own provisions. And wherever Wahb went he put up his sign-board: Trespassers beware! It was written on the trees as high up as he could reach, and every one that came by understood that the scent of it and the hair in it were those of the great Grizzly Wahb. If his Mother had lived to train him, Wahb would have known that a good range in spring may be a bad one in summer. Wahb found out by years of experience that a total change with the seasons is best. In the early spring the Cattle and Elk ranges, with their winter-killed carcasses, offer a bountiful feast. In early summer the best forage is on the warm hill-sides where the quamash and the Indian turnip grow. In late summer the berry-bushes along the river-flat are laden with fruit, and in autumn the pine woods gave good chances to fatten for the winter. So he added to his range each year. He not only cleared out the Blackbears from the Piney and the Meteetsee, but he went over the Divide and killed that old fellow that had once chased him out of the Warhouse Valley. And, more than that, he held what he had won, for he broke up a camp of tenderfeet that were looking for a ranch location on the Middle Meteetsee; he stampeded their horses, and made general smash of the camp.\"\n",
    "text70 = \"A grizzly bear named Wahb claims his territory and hunts for food in different areas throughout the seasons.\"\n",
    "\n",
    "inputs69 = tokenizer(text69, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs70 = tokenizer(text70, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs69 = model(**inputs69)\n",
    "outputs70 = model(**inputs70)\n",
    "\n",
    "embeddings69 = outputs69.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings70 = outputs70.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings69, embeddings70.T) / (np.linalg.norm(embeddings69) * np.linalg.norm(embeddings70))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a81e079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7418\n"
     ]
    }
   ],
   "source": [
    "#passage E364\n",
    "text71 = \"Captain Amyas Preston and Captain Sommers, the colonist of the Bermudas, or Sommers' Islands, will land, with a force tiny enough, though larger far than Leigh's, where Leigh dare not land; and taking the fort of Guayra, will find, as Leigh found, that their coming has been expected, and that the Pass of the Venta, three thousand feet above, has been fortified with huge barricadoes, abattis, and cannon, making the capital, amid its ring of mountain-walls, impregnable to all but Englishmen or Zouaves. For up that seven thousand feet of precipice, which rises stair on stair behind the town, those fierce adventurers will climb hand over hand, through rain and fog, while men lie down, and beg their officers to kill them, for no farther can they go. Yet farther they will go, hewing a path with their swords through woods of wild plantain, and rhododendron thickets, over (so it seems, however incredible) the very saddle of the Silla,* down upon the astonished Mantuanos of St. Jago, driving all before them; and having burnt the city in default of ransom, will return triumphant by the right road, and pass along the coast, the masters of the deep. * Humboldt says that there is a path from Caravellada to St. Jago, between the peaks, used by smugglers.\"\n",
    "text72 = \"Captain Amyas Preston and Captain Sommers plan to land in the Bermudas, take the fort of Guayra, and conquer the capital.\"\n",
    "\n",
    "inputs71 = tokenizer(text71, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs72 = tokenizer(text72, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs71 = model(**inputs71)\n",
    "outputs72 = model(**inputs72)\n",
    "\n",
    "embeddings71 = outputs71.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings72 = outputs72.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings71, embeddings72.T) / (np.linalg.norm(embeddings71) * np.linalg.norm(embeddings72))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455ac35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6907\n"
     ]
    }
   ],
   "source": [
    "#passage E387\n",
    "text73 = \"She spoke with an accent of wild joy: I knew you would come back in time! You are safe now. I have done it! I would never, never have let him  Her voice died out, while her eyes shone at him as when the sun breaks through a mist. Never get it back. Oh, my beloved! He bowed his head gravely, and said in his polite. Heystian tone: No doubt you acted from instinct. Women have been provided with their own weapon. I was a disarmed man, I have been a disarmed man all my life as I see it now. You may glory in your resourcefulness and your profound knowledge of yourself; but I may say that the other attitude, suggestive of shame, had its charm. For you are full of charm! The exultation vanished from her face. You mustn't make fun of me now. I know no shame. I was thanking God with all my sinful heart for having been able to do it for giving you to me in that way oh, my beloved all my own at last! He stared as if mad. Timidly she tried to excuse herself for disobeying his directions for her safety. Every modulation of her enchanting voice cut deep into his very breast, so that he could hardly understand the words for the sheer pain of it. He turned his back on her; but a sudden drop, an extraordinary faltering of her tone, made him spin round. On her white neck her pale head dropped as in a cruel drought a withered flower droops on its stalk. He caught his breath, looked at her closely, and seemed to read some awful intelligence in her eyes. At the moment when her eyelids fell as if smitten from above by an the gleam of old silver familiar to him from boyhood, the very invisible power, he snatched her up bodily out of the chair, and disregarding an unexpected metallic clatter on the floor, carried her off into the other room. The limpness of her body frightened him. Laying her down on the bed, he ran out again, seized a four-branched candlestick on the table, and ran back, tearing down with a furious jerk the curtain that swung stupidly in his way, but after putting the candlestick on the table by the bed, he remained absolutely idle. There did not seem anything more for him to do. Holding his chin in his hand he looked down intently at her still face. Has she been stabbed with this thing? asked Davidson, whom suddenly he saw standing by his side and holding up Ricardo's dagger to his sight. Heyst uttered no word of recognition or surprise. He gave Davidson only a dumb look of unutterable awe, then, as if possessed with a sudden fury, started tearing open the front of the girls dress.\"\n",
    "text74 = \"A woman expresses joy at someone's return, but the tone shifts to a darker one. The man becomes frantic and discovers the woman is dead.\"\n",
    "\n",
    "inputs73 = tokenizer(text73, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs74 = tokenizer(text74, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs73 = model(**inputs73)\n",
    "outputs74 = model(**inputs74)\n",
    "\n",
    "embeddings73 = outputs73.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings74 = outputs74.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings73, embeddings74.T) / (np.linalg.norm(embeddings73) * np.linalg.norm(embeddings74))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e60f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7908\n"
     ]
    }
   ],
   "source": [
    "#passage E392\n",
    "text75 = \"He's going back to Washington and it suited him to have him. I don't quarrel with him for that. I wish I were married to him and back in the States. You do? I do. You have given it all up about Lord Rufford then? No; that's just where it is. I haven't given it up, and I still see trouble upon trouble before me. But I know how it will be. He doesn't mean anything. He's only amusing himself. If he'd once say the word he couldn't get back again. The Duke would interfere then. What would he care for the Duke? The Duke is no more than anybody else nowadays. I shall just fall to the ground between two stools. I know it as well as if it were done already. And then I shall have to begin again! If it comes to that I shall do something terrible. I know I shall. Then they turned in at Lord Rufford's gates; and as they were driven up beneath the oaks, through the gloom, both mother and daughter thought how charming it would be to be the mistress of such a park. CHAPTER XXI. THE FIRST EVENING AT RUFFORD HALL. The phaeton arrived the first, the driver having been especially told by Arabella that he need not delay on the road for the other carriage. She had calculated that she might make her entrance with better effect alone with her mother than in company with Morton and the Senator. It would have been worth the while of any one who had witnessed her troubles on that morning to watch the bland serenity and happy ease with which she entered the room. Her mother was fond of a prominent place but was quite contented on this occasion to play a second fiddle for her daughter. She had seen at a glance that Rufford Hall was a delightful house. Oh, if it might become the home of her child and her grandchildren, and possibly a retreat for herself! Arabella was certainly very handsome at this moment. Never did she look better than when got up with care for travelling, especially as seen by an evening light. Her slow motions were adapted to heavy wraps, and however she might procure her large sealskin jacket she graced it well when she had it. Lord Rufford came to the door to meet them and immediately introduced them to his sister. There were six or seven people in the room, mostly ladies, and tea was offered to the new-comers. Lady Penwether was largely made, like her brother; but was a languidly lovely woman, not altogether unlike Arabella herself in her figure and movements, but with a more expressive face, with less colour, and much more positive assurance of high breeding.\"\n",
    "text76 = \"A woman and her daughter discuss a man they both are interested in while driving to Lord Rufford's estate. They arrive at the estate and are introduced to Lord Rufford's sister.\"\n",
    "\n",
    "inputs75 = tokenizer(text75, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs76 = tokenizer(text76, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs75 = model(**inputs75)\n",
    "outputs76 = model(**inputs76)\n",
    "\n",
    "embeddings75 = outputs75.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings76 = outputs76.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings75, embeddings76.T) / (np.linalg.norm(embeddings75) * np.linalg.norm(embeddings76))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bba68ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7772\n"
     ]
    }
   ],
   "source": [
    "#passage E410\n",
    "text77 = \"You! she repeated. He said, with a bow: I beg your pardon, madame, but I did not receive your letter until very late. Is it possible? Is it possible that this is you ... that you were able to ...? He seemed greatly surprised: Did I not promise to come in answer to your call? Yes ... but ... Well, here I am, he said, with a smile. He examined the strips of canvas from which Yvonne had succeeded in freeing herself and nodded his head, while continuing his inspection: So those are the means employed? The Comte d'Origny, I presume?... I also saw that he locked you in.... But then the pneumatic letter?... Ah, through the window!... How careless of you not to close it! He pushed both sides to. Yvonne took fright: Suppose they hear! There is no one in the house. I have been over it. Still ... Your husband went out ten minutes ago. Where is he? With his mother, the Comtesse d'Origny. How do you know? Oh, it's very simple! He was rung up by telephone and I awaited the result at the corner of this street and the boulevard. As I expected, the count came out hurriedly, followed by his man. I at once entered, with the aid of special keys. He told this in the most natural way, just as one tells a meaningless anecdote in a drawing-room. But Yvonne, suddenly seized with fresh alarm, asked: Then it's not true?... His mother is not ill?... In that case, my husband will be coming back.... Certainly, the count will see that a trick has been played on him and in three quarters of an hour at the latest.... Let us go.... I don't want him to find me here.... I must go to my son.... One moment.... One moment!... But don't you know that they have taken him from me?... That they are hurting him, perhaps?... With set face and feverish gestures, she tried to push Velmont back. He, with great gentleness, compelled her to sit down and, leaning over her in a respectful attitude, said, in a serious voice: Listen, madame, and let us not waste time, when every minute is valuable. First of all, remember this: we met four times, six years ago.... And, on the fourth occasion, when I was speaking to you, in the drawing-room of this house, with too much what shall I say? with too much feeling, you gave me to understand that my visits were no longer welcome. Since that day I have not seen you. And, nevertheless, in spite of all, your faith in me was such that you kept the card which I put between the pages of that book and, six years later, you send for me and none other. That faith in me I ask you to continue. You must obey me blindly. Just as I surmounted every obstacle to come to you, so I will save you, whatever the position may be.\"\n",
    "text78 = \"A woman is surprised to see a man who received her letter and came to her house. They discuss her husband's whereabouts and the man offers to save her from a dangerous situation.\"\n",
    "\n",
    "inputs77 = tokenizer(text77, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs78 = tokenizer(text78, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs77 = model(**inputs77)\n",
    "outputs78 = model(**inputs78)\n",
    "\n",
    "embeddings77 = outputs77.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings78 = outputs78.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings77, embeddings78.T) / (np.linalg.norm(embeddings77) * np.linalg.norm(embeddings78))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb23b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7138\n"
     ]
    }
   ],
   "source": [
    "#passage E415\n",
    "text79 = \"She has always been glad enough to get our kings to make war on England whenever she wanted a diversion made, but she has never put herself out of the way to return the favour. It has been a one sided alliance all along. Scotland has for centuries been sending some of her best blood to fight as soldiers in France, but with a few exceptions no Frenchman has ever drawn his sword for Scotland. No, I am inclined to think you are right, Ronald, and especially after what we saw at Fontenoy I have no wish ever to draw sword again against the English, and am willing to be the best friends in the world with them if they will but let us Scots have our own king and go away peacefully. I don't want to force Prince Charles upon them if they will but let us have him for ourselves. If they won't, you know, it is they who are responsible for the quarrel, not us. That is one way of putting it, certainly, Ronald laughed. I am afraid after having been one kingdom since King James went to London, they won't let us go our own way without making an effort to keep us; but here is a crossroad, we will strike off here and make for the west. They avoided the towns on their routes, for although they felt certain that they were ahead of any messengers who might be sent out with orders for their arrest, they knew that they might be detained for some little time at Nantes, and were therefore anxious to leave no clue of their passage in that direction. On the evening of the third day after starting they approached their destination. On the first morning after leaving Versailles they had halted in wood a short distance from Chartres, and Malcolm had ridden in alone and had purchased a suit of citizen's clothes for Ronald, as the latter's uniform as an officer of the Scotch Dragoons would at once have attracted notice. Henceforward, whenever they stopped, Malcolm had taken an opportunity to mention to the stable boy that he was accompanying his master, the son of an advocate of Paris, on a visit to some relatives in La Vendee. This story he repeated at the inn where they put up at Nantes. The next morning Malcolm went round to all the inns in the town, but could hear nothing of the Duke of Athole, so he returned at noon with the news of his want of success. They may have hired a private lodging to avoid observation, Ronald said, or, not improbably, may have taken another name. The best thing we can do is to go down to the river side, inquire what vessels are likely to leave port soon, and then, if we see anyone going off to them, to accost them. We may hear of them in that way. Accordingly they made their way down to the river.\"\n",
    "text80 = \"Two characters discuss their alliance with France and their desire for Scotland to have its own king.\"\n",
    "\n",
    "inputs79 = tokenizer(text79, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs80 = tokenizer(text80, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs79 = model(**inputs79)\n",
    "outputs80 = model(**inputs80)\n",
    "\n",
    "embeddings79 = outputs79.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings80 = outputs80.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings79, embeddings80.T) / (np.linalg.norm(embeddings79) * np.linalg.norm(embeddings80))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69705860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7778\n"
     ]
    }
   ],
   "source": [
    "#passage E419\n",
    "text81 = \"Mrs. Baxter managed to obtain his attention. If you'll go and brush your hair I'll send Genesis and Clematis away for the rest of the afternoon. And then if you 'll sit down quietly and try to keep cool until your friends get here, I'll  'Quietly'! he echoed, shaking his head over this mystery. I'm the only one that IS quiet around here. Things 'd be in a fine condition to receive guests if I didn't keep pretty cool, I guess! There, there, she said, soothingly. Go and brush your hair. And change your collar, Willie; it's all wilted. I'll send Genesis away. His wandering eye failed to meet hers with any intelligence. Collar, he muttered, as if in soliloquy. Collar. Change it! said Mrs. Baxter, raising her voice. It's WILTED. He departed in a dazed manner. Passing through the hall, he paused abruptly, his eye having fallen with sudden disapproval upon a large, heavily framed, glass-covered engraving, The Battle of Gettysburg, which hung upon the wall, near the front door. Undeniably, it was a picture feeble in decorative quality; no doubt, too, William was right in thinking it as unworthy of Miss Pratt, as were Jane and Genesis and Clematis. He felt that she must never see it, especially as the frame had been chipped and had a corner broken, but it was more pleasantly effective where he found it than where (in his nervousness) he left it. A few hasty jerks snapped the elderly green cords by which it was suspended; then he laid the picture upon the floor and with his handkerchief made a curious labyrinth of avenues in the large oblong area of fine dust which this removal disclosed upon the wall. Pausing to wipe his hot brow with the same implement, he remembered that some one had made allusions to his collar and hair, whereupon he sprang to the stairs, mounted two at a time, rushed into his own room, and confronted his streaked image in the mirror. XIII AT HOME TO HIS FRIENDS After ablutions, he found his wet hair plastic, and easily obtained the long, even sweep backward from the brow, lacking which no male person, unless bald, fulfilled his definition of a man of the world. But there ensued a period of vehemence and activity caused by a bent collar-button, which went on strike with a desperation that was downright savage. The day was warm and William was warmer; moisture bedewed him afresh. Belated victory no sooner arrived than he perceived a fatal dimpling of the new collar, and was forced to begin the operation of exchanging it for a successor. Another exchange, however, he unfortunately forgot to make: the handkerchief with which he had wiped the wall remained in his pocket.\"\n",
    "text82 = \"Mrs. Baxter asks William to brush his hair and change his wilted collar before their friends arrive. William becomes distracted by a picture on the wall and ends up removing it and creating a mess on the wall. He then goes to his room to fix his appearance but encounters difficulties with his collar and forgets to remove the handkerchief from his pocket.\"\n",
    "\n",
    "inputs81 = tokenizer(text81, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs82 = tokenizer(text82, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs81 = model(**inputs81)\n",
    "outputs82 = model(**inputs82)\n",
    "\n",
    "embeddings81 = outputs81.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings82 = outputs82.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings81, embeddings82.T) / (np.linalg.norm(embeddings81) * np.linalg.norm(embeddings82))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a34f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.8015\n"
     ]
    }
   ],
   "source": [
    "#passage E438\n",
    "text83 = \"At the moment he looked to me convincingly tired, gone slack all over, like a man who has passed through some sort of crisis. I have had a lot of urgent writing to do, he added. I rose from my chair at once, and he followed my example, without haste, a little heavily. I must apologize for detaining you so long, I said. Why apologize? One can't very well go to bed before night. And you did not detain me. I could have left you at any time. I had not stayed with him to be offended. I am glad you have been sufficiently interested, I said calmly. No merit of mine, though the commonest sort of regard for the mother of your friend was enough.... As to Miss Haldin herself, she at one time was disposed to think that her brother had been betrayed to the police in some way. To my great surprise Mr. Razumov sat down again suddenly. I stared at him, and I must say that he returned my stare without winking for quite a considerable time. In some way, he mumbled, as if he had not understood or could not believe his ears. Some unforeseen event, a sheer accident might have done that, I went on. Or, as she characteristically put it to me, the folly or weakness of some unhappy fellow-revolutionist. Folly or weakness, he repeated bitterly. She is a very generous creature, I observed after a time. The man admired by Victor Haldin fixed his eyes on the ground. I turned away and moved off, apparently unnoticed by him. I nourished no resentment of the moody brusqueness with which he had treated me. The sentiment I was carrying away from that conversation was that of hopelessness. Before I had got fairly clear of the raft of chairs and tables he had rejoined me. H'm, yes! I heard him at my elbow again. But what do you think? I did not look round even. I think that you people are under a curse. He made no sound. It was only on the pavement outside the gate that I heard him again. I should like to walk with you a little. After all, I preferred this enigmatical young man to his celebrated compatriot, the great Peter Ivanovitch. But I saw no reason for being particularly gracious. I am going now to the railway station, by the shortest way from here, to meet a friend from England, I said, for all answer to his unexpected proposal. I hoped that something informing could come of it. As we stood on the curbstone waiting for a tramcar to pass, he remarked gloomily  I like what you said just now. Do you? We stepped off the pavement together. The great problem, he went on, is to understand thoroughly the nature of the curse. That's not very difficult, I think.\"\n",
    "text84 = \"The narrator is having a conversation with Mr. Razumov, who appears tired and has been doing urgent writing. They discuss Miss Haldin's belief that her brother was betrayed by someone in the revolution, and the narrator expresses hopelessness and believes that Razumov and his people are cursed.\"\n",
    "\n",
    "inputs83 = tokenizer(text83, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs84 = tokenizer(text84, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs83 = model(**inputs83)\n",
    "outputs84 = model(**inputs84)\n",
    "\n",
    "embeddings83 = outputs83.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings84 = outputs84.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings83, embeddings84.T) / (np.linalg.norm(embeddings83) * np.linalg.norm(embeddings84))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b592ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6335\n"
     ]
    }
   ],
   "source": [
    "#passage E447\n",
    "text85 = \"Only he never reached the Captain. In the very act of falling Jellico had pulled his legs under him so that he was not supine but crouched, and his net swept but at ground level, clipping the I-S man about the shins, entangling his feet so that he crashed heavily to the sod and lay still. The whip that Lalox whip trick! Wilcox's voice rose triumphantly above the babble of the crowd. Using his net as if it had been a thong, Jellico had brought down the Eysie with a move the other had not foreseen. Breathing hard, sweat running down his shoulders and making tracks through the powdery red dust which streaked him, Jellico got to his feet and walked over to the I-S champion who had not moved or made a sound since his fall. The Captain went down on one knee to examine him. Kill! Kill! That was the Salariki, all their instinctive savagery aroused. But Jellico spoke to Groft. By our customs we do not kill the conquered. Let his friends bear him hence. He took the claw knife the Eysie still clutched in his hand and thrust it into his own belt. Then he faced the I-S party and Kallee. Take your man and get out! The rein he had kept on his temper these past days was growing very thin. You've made your last play here. Kallee's thick lips drew back in something close to a Salarik snarl. But neither he nor his men made any reply. They bundled up their unconscious fighter and disappeared. Of their own return to the sanctuary of the Queen Dane had only the dimmest of memories afterwards. He had made the privacy of the forest road before he yielded to the demands of his outraged interior. And after that he had stumbled along with Van Rycke's hand under his arm, knowing from other miserable sounds that he was not alone in his torment. It was some time later, months he thought when he first roused, that he found himself lying in his bunk, feeling very weak and empty as if a large section of his middle had been removed, but also at peace with his world. As he levered himself up the cabin had a nasty tendency to move slowly to the right as if he were a pivot on which it swung, and he had all the sensations of being in free fall though the Queen was still firmly planeted. But that was only a minor discomfort compared to the disturbance he remembered. Fed the semi-liquid diet prescribed by Tau and served up by Mura to him and his fellow sufferers, he speedily got back his strength. But it had been a close call, he did not need Tau's explanation to underline that. Weeks had suffered the least of the four, he the most though none of them had had an easy time. And they had been out of circulation three days.\"\n",
    "text86 = \"Jellico defeats the I-S champion in a fight and spares his life. He later recovers from injuries with the help of his friends.\"\n",
    "\n",
    "inputs85 = tokenizer(text85, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs86 = tokenizer(text86, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs85 = model(**inputs85)\n",
    "outputs86 = model(**inputs86)\n",
    "\n",
    "embeddings85 = outputs85.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings86 = outputs86.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings85, embeddings86.T) / (np.linalg.norm(embeddings85) * np.linalg.norm(embeddings86))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0d7f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7322\n"
     ]
    }
   ],
   "source": [
    "#passage E469\n",
    "text87 = \"But it is highly desirable, that a practice so eminently useful, should be universally adopted by classical teachers, both public and private, as an indispensable part of education. At the close of the year 1808, I met with a most affecting event, in the death of my youngest sister, the wife of Gilbert Golden Willet. She had been, for nearly a year, much indisposed ; and the disorder made a gradual progress, till it put a period to her mortal existence, in the forty-fifth year of her age. From the letters of my relations, which mention her decease, and the circumstances attending it, I have derived great consolation. She was so patient, so fully resigned to the will of God, and so well prepared to leave the world, and enter into a state of blessedness ; that we have no cause to mourn on her account. She has, doubtless, commenced that life, which is free from temptation and sorrow ; and in which she will be unspeakably happy for ever. I rejoice that 1 have had such a sister; and I trust that the recollection of her pious and bright example, will prove, through life, a source of thankfulness, and an additional incentive to virtue. As I feel so deeply interested in this event, I think that an account of some of her expressions, and her deportment, at and near the closing scene, will not be deemed unsuitable to v a narrative which relates the chief occurrences of my life. For several weeks before her death, she was at times affected with exquisite bodily pain, and was often nearly suffocated by the disorder of her lungs. But her own distresses, which were borne with great patience, did not prevent her from attending to the feelings and situation of her husband and children, her relations and friends. She was solicitous to diminish their care, and to relieve their anxiety about her, as much as possible. Though her hope and trust in the mercy of God, through Jesus Christ, were strong and unshaken, yet she was very humble, and thought but little of her own attainments. To a person w r ho expressed a desire to take pattern after her, she meekly replied :  I desire to take pattern after the Lord Jesus Christ. In one of her intervals of relief from great pain, a person in the room calling her blessed, she answered ;  Not yet blessed. And to her husband who said, he was sure she was going to be happy, she replied : Not sure, my love ; we can not be sure : but I trust in the mercy of the Almighty. At an other time, her husband speaking of her goodness, as the ground of a lively hope, she put her hand on his lips, as if to silence him on the subject of herself.\"\n",
    "text88 = \"The narrator recounts the death of their youngest sister and reflects on her pious and humble demeanor during her illness.\"\n",
    "\n",
    "inputs87 = tokenizer(text87, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs88 = tokenizer(text88, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs87 = model(**inputs87)\n",
    "outputs88 = model(**inputs88)\n",
    "\n",
    "embeddings87 = outputs87.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings88 = outputs88.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings87, embeddings88.T) / (np.linalg.norm(embeddings87) * np.linalg.norm(embeddings88))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b39678ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7738\n"
     ]
    }
   ],
   "source": [
    "#passage E476\n",
    "text89 = \"She has no discrimination, she takes to all of them, she thinks they are all treasures, every new one is welcome. When the mighty brontosaurus came striding into camp, she regarded it as an acquisition, I considered it a calamity; that is a good sample of the lack of harmony that prevails in our views of things. She wanted to domesticate it, I wanted to make it a present of the homestead and move out. She believed it could be tamed by kind treatment and would be a good pet; I said a pet twenty-one feet high and eighty-four feet long would be no proper thing to have about the place, because, even with the best intentions and without meaning any harm, it could sit down on the house and mash it, for any one could see by the look of its eye that it was absent-minded. Still, her heart was set upon having that monster, and she couldn't give it up. She thought we could start a dairy with it, and wanted me to help milk it; but I wouldn't; it was too risky. The sex wasn't right, and we hadn't any ladder anyway. Then she wanted to ride it, and look at the scenery. Thirty or forty feet of its tail was lying on the ground, like a fallen tree, and she thought she could climb it, but she was mistaken; when she got to the steep place it was too slick and down she came, and would have hurt herself but for me. Was she satisfied now? No. Nothing ever satisfies her but demonstration; untested theories are not in her line, and she won't have them. It is the right spirit, I concede it; it attracts me; I feel the influence of it; if I were with her more I think I should take it up myself. Well, she had one theory remaining about this colossus: she thought that if we could tame it and make him friendly we could stand in the river and use him for a bridge. It turned out that he was already plenty tame enough at least as far as she was concerned so she tried her theory, but it failed:  every time she got him properly placed in the river and went ashore to cross over him, he came out and followed her around like a pet mountain. Like the other animals. They all do that. Tuesday Wednesday Thursday and today:  all without seeing him. It is a long time to be alone; still, it is better to be alone than unwelcome. FRIDAY I HAD to have company I was made for it, I think so I made friends with the animals. They are just charming, and they have the kindest disposition and the politest ways; they never look sour, they never let you feel that you are intruding, they smile at you and wag their tail, if they've got one, and they are always ready for a romp or an excursion or anything you want to propose.\"\n",
    "text90 = \"The narrator's wife wants to domesticate a brontosaurus, but the narrator thinks it's a bad idea. The wife tries to use the brontosaurus as a bridge but fails. The narrator finds solace in befriending the animals.\"\n",
    "\n",
    "inputs89 = tokenizer(text89, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs90 = tokenizer(text90, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs89 = model(**inputs89)\n",
    "outputs90 = model(**inputs90)\n",
    "\n",
    "embeddings89 = outputs89.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings90 = outputs90.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings89, embeddings90.T) / (np.linalg.norm(embeddings89) * np.linalg.norm(embeddings90))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f98040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6918\n"
     ]
    }
   ],
   "source": [
    "#passage E502\n",
    "text91 = \"The poor girl did her best, and I am sure that when she accepted Mr. Leavenworth she thought of you. She thought of the pleasure her marriage would give me. Ay, pleasure indeed! She is a thoroughly good girl, but she has her little grain of feminine spite, like the rest. Well, he 's richer than you, and she will have what she wants; but before I forgive you I must wait and see this new arrival what do you call her? Miss Garland. If I like her, I will forgive you; if I don't, I shall always bear you a grudge. Rowland answered that he was sorry to forfeit any advantage she might offer him, but that his exculpatory passion for Miss Garland was a figment of her fancy. Miss Garland was engaged to another man, and he himself had no claims. Well, then, said Madame Grandoni, if I like her, we 'll have it that you ought to be in love with her. If you fail in this, it will be a double misdemeanor. The man she 's engaged to does n't care a straw for her. Leave me alone and I 'll tell her what I think of you. As to Christina Light's marriage, Madame Grandoni could make no definite statement. The young girl, of late, had made her several flying visits, in the intervals of the usual pre-matrimonial shopping and dress-fitting; she had spoken of the event with a toss of her head, as a matter which, with a wise old friend who viewed things in their essence, she need not pretend to treat as a solemnity. It was for Prince Casamassima to do that. It is what they call a marriage of reason, she once said. That means, you know, a marriage of madness! What have you said in the way of advice? Rowland asked. Very little, but that little has favored the prince. I know nothing of the mysteries of the young lady's heart. It may be a gold-mine, but at any rate it 's a mine, and it 's a long journey down into it. But the marriage in itself is an excellent marriage. It 's not only brilliant, but it 's safe. I think Christina is quite capable of making it a means of misery; but there is no position that would be sacred to her. Casamassima is an irreproachable young man; there is nothing against him but that he is a prince. It is not often, I fancy, that a prince has been put through his paces at this rate. No one knows the wedding-day; the cards of invitation have been printed half a dozen times over, with a different date; each time Christina has destroyed them.\"\n",
    "text92 = \"The text discusses the thoughts and feelings of various characters regarding a marriage and the potential arrival of a new person.\"\n",
    "\n",
    "inputs91 = tokenizer(text91, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs92 = tokenizer(text92, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs91 = model(**inputs91)\n",
    "outputs92 = model(**inputs92)\n",
    "\n",
    "embeddings91 = outputs91.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings92 = outputs92.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings91, embeddings92.T) / (np.linalg.norm(embeddings91) * np.linalg.norm(embeddings92))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8c9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7205\n"
     ]
    }
   ],
   "source": [
    "#passage E572\n",
    "text93 = \"Except for slaves, and for sacrifice to the gods, there is nothing to be gained by their conquest. And you all worship the same gods? Roger asked. Assuredly, he said, although some are thought more highly of in one kingdom, some in another. Mexitli or as he is generally called, Huitzilopotchli is of course the greatest everywhere; but he is worshiped most of all by the Aztecs. Quetzalcoatl is also greatly worshiped. As he spoke, the merchant glanced furtively up at Roger. The lad saw that this was a favorable opportunity for creating an impression. He smiled quietly. It is right that he should be, he said, since he taught you all the good things you know; and was, like myself, white. This proof of the great knowledge possessed by the being before him vastly impressed the Mexican. How could this strange being know the Mexican tongue, and be acquainted with its gods, unless he were one of them? It had pleased him to assume ignorance of other matters, but doubtless he was well aware of everything that had passed in the country since he left it. Henceforth the respect which he and his companions paid to Roger was redoubled. As soon as they had reached the borders of Mexico, a swift runner had been dispatched to the nearest post with a message, to be sent forward to the King of Tezcuco, with the tidings of the arrival of a strange white being in the land; and asking for instructions as to what was to be done with him. In the meantime, the merchants told Roger that they wished him to abstain from going out into the various villages and towns at which they stopped. Until we know what are the king's wishes concerning you, it were better that you were not seen. In the first place, all this country by the coast is under the Aztec rule, and as soon as you were seen, messages would be sent forward to Mexico, and the Emperor might desire that so great a wonder should be sent direct to him; whereas, if our own King sends first for you, you would be his property as it were, and even Montezuma would not interfere. It will not be long before an answer arrives, for along all the roads there are post houses, two leagues apart from each other. At each of these couriers are stationed, men trained to run at great speed, and these carry the dispatches from post to post, at the rate of eight or nine miles an hour. But the messages must get changed, where they have to be given so often? Not at all, he said. The couriers know nothing of the dispatches they carry. Oh, they are written dispatches? Roger said. Then you possess the art of writing? Writing, what is writing? the merchant asked.\"\n",
    "text94 = \"A conversation between Roger and a Mexican merchant about gods and the customs of Mexico.\"\n",
    "\n",
    "inputs93 = tokenizer(text93, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs94 = tokenizer(text94, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs93 = model(**inputs93)\n",
    "outputs94 = model(**inputs94)\n",
    "\n",
    "embeddings93 = outputs93.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings94 = outputs94.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings93, embeddings94.T) / (np.linalg.norm(embeddings93) * np.linalg.norm(embeddings94))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58bc66d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7406\n"
     ]
    }
   ],
   "source": [
    "#passage E590\n",
    "text95 = \"What's to become of poah Miss 'Lizabeth if that's the case, is moah'n I know. We mustn't cross the bridge till we come to it, Sistah Po'tah, he suggested. I know that; but a lookin'-glass broke yeste'day mawnin' when nobody had put fingah on it. An' his picture fell down off the wall while I was sweepin' the pa'lah. Pete said his dawg done howl all night last night, an' I've dremp three times hand runnin' 'bout muddy watah. Mom Beck felt a little hand clutch her skirts, and turned to see a frightened little face looking anxiously up at her. Now, what's the mattah with you, honey? she asked. I'm only a-tellin' Mistah Fostah about some silly old signs my mammy used to believe in. But they don't mean nothin' at all. Lloyd couldn't have told why she was unhappy. She had not understood all that Mom Beck had said, but her sensitive little mind was shadowed by a foreboding of trouble. The shadow deepened as the days passed. Papa Jack got worse instead of better. There were times when he did not recognize any one, and talked wildly of things that had happened out at the mines. All the long, beautiful October went by, and still he lay in the darkened room. Lloyd wandered listlessly from place to place, trying to keep out of the way, and to make as little trouble as possible. I'm a real little woman now, she repeated, proudly, whenever she was allowed to pound ice or carry fresh water. I'm papa's little comfort. One cold, frosty evening she was standing in the hall, when the doctor came out of the room and began to put on his overcoat. Her mother followed him to take his directions for the night. He was an old friend of the family's. Elizabeth had climbed on his knees many a time when she was a child. She loved this faithful, white-haired old doctor almost as dearly as she had her father. My daughter, he said, kindly, laying his hand on her shoulder, you are wearing yourself out, and will be down yourself if you are not careful. You must have a professional nurse. No telling how long this is going to last. As soon as Jack is able to travel you must have a change of climate. Her lips trembled. We can't afford it, doctor, she said. Jack has been too sick from the very first to talk about business. He always said a woman should not be worried with such matters, anyway. I don't know what arrangements he has made out West. For all I know, the little I have in my purse now may be all that stands between us and the poorhouse. The doctor drew on his gloves. Why don't you tell your father how matters are? he asked. Then he saw he had ventured a step too far.\"\n",
    "text96 = \"A young girl named Lloyd worries about her sick father and the family's financial situation, while her mother considers getting a nurse and a change of climate for her husband's recovery.\"\n",
    "\n",
    "inputs95 = tokenizer(text95, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs96 = tokenizer(text96, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs95 = model(**inputs95)\n",
    "outputs96 = model(**inputs96)\n",
    "\n",
    "embeddings95 = outputs95.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings96 = outputs96.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings95, embeddings96.T) / (np.linalg.norm(embeddings95) * np.linalg.norm(embeddings96))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe9f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.7367\n"
     ]
    }
   ],
   "source": [
    "#passage E4157\n",
    "text97 = \"I'm pretty good at climbing trees, myself, so I think I'll capture the Monkey and eat him for my breakfast. Woot the Monkey, hearing this speech from his perch on the tree, became much frightened, for he knew the nature of jaguars and realized they could climb trees and leap from limb to limb with the agility of cats. So he at once began to scamper through the forest as fast as he could go, catching at a branch with his long monkey arms and swinging his green body through space to grasp another branch in a neighboring tree, and so on, while the Jaguar followed him from below, his eyes fixed steadfastly on his prey. But presently Woot got his feet tangled in the Lace Apron, which he was still wearing, and that tripped him in his flight and made him fall to the ground, where the Jaguar placed one huge paw upon him and said grimly: I've got you, now! The fact that the Apron had tripped him made Woot remember its magic powers, and in his terror he cried out: Open! without stopping to consider how this command might save him. But, at the word, the earth opened at the exact spot where he lay under the Jaguar's paw, and his body sank downward, the earth closing over it again. The last thing Woot the Monkey saw, as he glanced upward, was the Jaguar peering into the hole in astonishment. He's gone! cried the beast, with a long-drawn sigh of disappointment; he's gone, and now I shall have no breakfast. The clatter of the Tin Owl's wings sounded above him, and the little Brown Bear came trotting up and asked: Where is the monkey? Have you eaten him so quickly? No, indeed, answered the Jaguar. He disappeared into the earth before I could take one bite of him! And now the Canary perched upon a stump, a little way from the forest beast, and said: I am glad our friend has escaped you; but, as it is natural for a hungry beast to wish his breakfast, I will try to give you one. Thank you, replied the Jaguar. You're rather small for a full meal, but it's kind of you to sacrifice yourself to my appetite. Oh, I don't intend to be eaten, I assure you, said the Canary, but as I am a fairy I know something of magic, and though I am now transformed into a bird's shape, I am sure I can conjure up a breakfast that will satisfy you. If you can work magic, why don't you break the enchantment you are under and return to your proper form? inquired the beast doubtingly. I haven't the power to do that, answered the Canary, for Mrs. Yoop, the Giantess who transformed me, used a peculiar form of yookoohoo magic that is unknown to me.\"\n",
    "text98 = \"Woot the Monkey is being chased by a Jaguar, but escapes by using the magic powers of his Lace Apron.\"\n",
    "\n",
    "inputs97 = tokenizer(text97, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs98 = tokenizer(text98, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs97 = model(**inputs97)\n",
    "outputs98 = model(**inputs98)\n",
    "\n",
    "embeddings97 = outputs97.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings98 = outputs98.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings97, embeddings98.T) / (np.linalg.norm(embeddings97) * np.linalg.norm(embeddings98))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4696ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the texts: 0.6581\n"
     ]
    }
   ],
   "source": [
    "#passage E4175\n",
    "text99 = \"The nomes are our friends, and they possess magic powers that will enable them to protect the prisoners from discovery. If we can manage to get the King and Queen of Pingaree to the Nome Kingdom before the boy knows what we are doing, I am sure our plot will succeed. Gos gave the plan considerable thought in the next five minutes, and the more he thought about it the more clever and reasonable it seemed. So he agreed to do as Queen Cor suggested and at once hurried away to the mines, where he arrived before Prince Inga did. The next morning he carried King Kitticut back to Regos. While Gos was gone, Queen Cor busied herself in preparing a large and swift boat for the journey. She placed in it several bags of gold and jewels with which to bribe the nomes, and selected forty of the strongest oarsmen in Regos to row the boat. The instant King Gos returned with his royal prisoner all was ready for departure. They quickly entered the boat with their two important captives and without a word of explanation to any of their people they commanded the oarsmen to start, and were soon out of sight upon the broad expanse of the Nonestic Ocean. Inga arrived at the city some hours later and was much distressed when he learned that his father and mother had been spirited away from the islands. I shall follow them, of course, said the boy to Rinkitink, and if I cannot overtake them on the ocean I will search the world over until I find them. But before I leave here I must arrange to send our people back to Pingaree. Chapter Sixteen Nikobob Refuses a Crown Almost the first persons that Zella saw when she landed from the silver-lined boat at Regos were her father and mother. Nikobob and his wife had been greatly worried when their little daughter failed to return from Coregos, so they had set out to discover what had become of her. When they reached the City of Regos, that very morning, they were astonished to hear news of all the strange events that had taken place; still, they found comfort when told that Zella had been seen in the boat of Prince Inga, which had gone to the north. Then, while they wondered what this could mean, the silver-lined boat appeared again, with their daughter in it, and they ran down to the shore to give her a welcome and many joyful kisses. Inga invited the good people to the palace of King Gos, where he conferred with them, as well as with Rinkitink and Bilbil. Now that the King and Queen of Regos and Coregos have run away, he said, there is no one to rule these islands. So it is my duty to appoint a new ruler, and as Nikobob, Zella's father, is an honest and worthy man, I shall make him the King of the Twin Islands.\"\n",
    "text100 = \"The text discusses a plan to protect prisoners and the protagonist's determination to find his parents.\"\n",
    "\n",
    "inputs99 = tokenizer(text99, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs100 = tokenizer(text100, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "outputs99 = model(**inputs99)\n",
    "outputs100 = model(**inputs100)\n",
    "\n",
    "embeddings99 = outputs99.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "embeddings100 = outputs100.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "similarity = np.dot(embeddings99, embeddings100.T) / (np.linalg.norm(embeddings99) * np.linalg.norm(embeddings100))\n",
    "\n",
    "print(\"Similarity between the texts: {:.4f}\".format(similarity[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f59e13",
   "metadata": {},
   "source": [
    "## Similarity Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f27919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7120\n",
      "Median: 0.7151\n",
      "Variance: 0.0030\n",
      "Standard Deviation: 0.0544\n",
      "Maximum Similarity Score: 0.8089\n",
      "Minimum Similarity Score: 0.5866\n"
     ]
    }
   ],
   "source": [
    "# Similarity score data\n",
    "scores = np.array([0.8089, 0.8052, 0.7298, 0.7079, 0.5951, 0.6685, 0.7478, 0.6866, 0.7350, 0.7213, \n",
    "                   0.6553, 0.7004, 0.7260,0.7399, 0.7839, 0.6374, 0.6659, 0.7165,0.7664, 0.7034, \n",
    "                   0.5866, 0.6694, 0.6997,0.6991,0.5996,0.7044,0.6216,0.7746,0.7335,0.6739,\n",
    "                   0.6705,0.7261,0.6767,0.7776,0.7067,0.7418,0.6907,0.7908,0.7772,0.7138,\n",
    "                   0.7778, 0.8015, 0.6335, 0.7322, 0.7738, 0.6918, 0.7205, 0.7406, 0.7367, 0.6581])\n",
    "\n",
    "# Clean data: Remove NaN and infinite values\n",
    "clean_scores = scores[np.isfinite(scores)]\n",
    "\n",
    "# Compute basic statistics\n",
    "mean_score = np.mean(clean_scores)\n",
    "median_score = np.median(clean_scores)\n",
    "variance_score = np.var(clean_scores)\n",
    "std_deviation = np.std(clean_scores)\n",
    "\n",
    "# Calculate maximum and minimum similarity scores\n",
    "max_similarity = np.max(df['Similarity'])\n",
    "min_similarity = np.min(df['Similarity'])\n",
    "\n",
    "# Output statistical results\n",
    "print(f\"Mean: {mean_score:.4f}\")\n",
    "print(f\"Median: {median_score:.4f}\")\n",
    "print(f\"Variance: {variance_score:.4f}\")\n",
    "print(f\"Standard Deviation: {std_deviation:.4f}\")\n",
    "print(f\"Maximum Similarity Score: {max_similarity}\")\n",
    "print(f\"Minimum Similarity Score: {min_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6640fbd",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f5a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfca61f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = {\n",
    "    \"Excerpt\": [\"E2\", \"E7\", \"E15\", \"E56\", \"E164\", \"E278\", \"E3650\", \"E151\", \"E316\", \"E592\",\n",
    "                \"E733\", \"E772\",\"E799\", \"E966\", \"E1026\", \"E1155\", \"E1194\", \"E1236\", \"E1302\", \"E1311\",\n",
    "                 \"E13\", \"E79\", \"E121\",\"E6051\",\"E6255\",\"E6336\",\"E3090\",\"E3153\",\"E3396\",\"E3405\",\n",
    "                 \"E85\", \"E102\", \"E259\",\"E301\",\"E358\",\"E364\",\"E387\",\"E392\",\"E410\",\"E415\",\n",
    "                \"E419\", \"E438\",\"E447\",\"E469\",\"E476\",\"E502\",\"E572\",\"E590\",\"E4157\",\"E4175\"],\n",
    "    \"Similarity\": [0.8089, 0.8052, 0.7298, 0.7079, 0.5951, 0.6685, 0.7478, 0.6866, 0.7350, 0.7213, \n",
    "                0.6553, 0.7004, 0.7260, 0.7399, 0.7839, 0.6374, 0.6659, 0.7165, 0.7664, 0.7034,\n",
    "                0.5866, 0.6694, 0.6997,0.6991, 0.5996,0.7044,0.6216,0.7746,0.7335,0.6739,\n",
    "                0.6705, 0.7261, 0.6767, 0.7776, 0.7067, 0.7418, 0.6907, 0.7908, 0.7772, 0.7138,\n",
    "                0.7778, 0.8015, 0.6335, 0.7322, 0.7738, 0.6918, 0.7205, 0.7406, 0.7367, 0.6581]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_value = df['Similarity'].mean()\n",
    "median_value = df['Similarity'].median()\n",
    "variance_value = df['Similarity'].var()\n",
    "\n",
    "# Plot bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar = sns.barplot(x=\"Excerpt\", y=\"Similarity\", data=df, color=\"darkblue\")\n",
    "plt.xlabel(\"Excerpt\")\n",
    "plt.ylabel(\"Similarity Score\")\n",
    "plt.axhline(mean_value, color='r', linestyle='--', label=f'Mean: {mean_value:.4f}')\n",
    "plt.axhline(median_value, color='g', linestyle='-', label=f'Median: {median_value:.4f}')\n",
    "\n",
    "# Add variance in the chart's title or annotation\n",
    "plt.annotate(f'Variance: {variance_value:.4f}', xy=(0.5, 0.95), xycoords='axes fraction', ha='center', color='blue')\n",
    "\n",
    "# Display legend\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6533f",
   "metadata": {},
   "source": [
    "Get the number of articles above or below mean and median vaules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6691575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles above both mean and median values: 25\n",
      "Articles above both mean and median values: ['E2', 'E7', 'E15', 'E3650', 'E316', 'E592', 'E799', 'E966', 'E1026', 'E1236', 'E1302', 'E3153', 'E3396', 'E102', 'E301', 'E364', 'E392', 'E410', 'E419', 'E438', 'E469', 'E476', 'E572', 'E590', 'E4157']\n",
      "Number of articles not above both mean and median values: 25\n",
      "Articles not above both mean and median values: ['E56', 'E164', 'E278', 'E151', 'E733', 'E772', 'E1155', 'E1194', 'E1311', 'E13', 'E79', 'E121', 'E6051', 'E6255', 'E6336', 'E3090', 'E3405', 'E85', 'E259', 'E358', 'E387', 'E415', 'E447', 'E502', 'E4175']\n"
     ]
    }
   ],
   "source": [
    "# Filter articles that are above both the mean and median values\n",
    "above_mean_median = df[(df['Similarity'] > mean_value) & (df['Similarity'] > median_value)]\n",
    "\n",
    "# Filter articles that are not above both the mean and median values\n",
    "below_mean_median = df[~((df['Similarity'] > mean_value) & (df['Similarity'] > median_value))]\n",
    "\n",
    "# Count the number of articles\n",
    "above_count = above_mean_median.shape[0]\n",
    "below_count = below_mean_median.shape[0]\n",
    "\n",
    "# Output results\n",
    "above_articles = above_mean_median['Excerpt'].tolist()\n",
    "below_articles = below_mean_median['Excerpt'].tolist()\n",
    "\n",
    "print(\"Number of articles above both mean and median values:\", above_count)\n",
    "print(\"Articles above both mean and median values:\", above_articles)\n",
    "print(\"Number of articles not above both mean and median values:\", below_count)\n",
    "print(\"Articles not above both mean and median values:\", below_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
